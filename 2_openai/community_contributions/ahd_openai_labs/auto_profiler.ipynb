{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648a63db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from openai import AsyncOpenAI\n",
    "from agents import Agent, Runner, trace, function_tool, OpenAIChatCompletionsModel, input_guardrail, GuardrailFunctionOutput, WebSearchTool\n",
    "from openai.types.responses import ResponseTextDeltaEvent\n",
    "from typing import Dict, List, Optional\n",
    "from pydantic import BaseModel, HttpUrl\n",
    "import sendgrid\n",
    "import os\n",
    "from sendgrid.helpers.mail import Mail, Email, To, Content\n",
    "import asyncio\n",
    "from scraper import fetch_website_contents, fetch_website_links\n",
    "from IPython.core.display import Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463022bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e18b163",
   "metadata": {},
   "outputs": [],
   "source": [
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "GEMINI_BASE_URL = \"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    "gemini_client = AsyncOpenAI(base_url=GEMINI_BASE_URL, api_key=google_api_key)\n",
    "gemini_2_5_pro_model = OpenAIChatCompletionsModel(model=\"gemini-2.5-pro\", openai_client=gemini_client)\n",
    "gemini_2_5_flash_model = OpenAIChatCompletionsModel(model=\"gemini-2.5-flash\", openai_client=gemini_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5fa9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPANY_URL = \"https://cbtw.tech/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5ea77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PageSummarizer agent\n",
    "\n",
    "page_summarizer_instruction =\"\"\"\n",
    "You are a customer service content analyzer. Your task is to create concise summaries of content to support customer service teams.\n",
    "\n",
    "INPUT: Website content (HTML or text) from a company website\n",
    "\n",
    "OUTPUT: A brief, structured markdown summary (max 500 words)\n",
    "\"\"\"\n",
    "page_summarizer_agent = Agent(\n",
    "        name=\"PageSummarizer Agent\",\n",
    "        instructions=page_summarizer_instruction,\n",
    "        model=gemini_2_5_flash_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c687d4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def summarize(url):\n",
    "    web_content = fetch_website_contents(url)\n",
    "    return web_content\n",
    "    #print(f\"AGENT: PageSummarizer is running {url}\")\n",
    "    #with trace(\"PageSummarizer Agent\"):\n",
    "        #result = await Runner.run(page_summarizer_agent, web_content)\n",
    "        #return result.final_output\n",
    "\n",
    "# web_content = await summarize(CBTW_URL)\n",
    "# print(web_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39512565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get relevant links Agent\n",
    "class Link(BaseModel):\n",
    "    type: str\n",
    "    url: HttpUrl\n",
    "\n",
    "class RelevantLinks(BaseModel):\n",
    "    links: List[Link]\n",
    "\n",
    "get_relevant_links_system_prompt = \"\"\"\n",
    "You are an AI customer service content analyzer.\n",
    "\n",
    "**Goal:** From a provided list of webpage links, identify and extract the links that are most relevant for inclusion in customer-facing content about the company.\n",
    "\n",
    "**Relevant links typically include:**\n",
    "\n",
    "* About or Company information pages\n",
    "* Careers, Jobs, or Recruitment pages\n",
    "* Contact or Support pages (if applicable)\n",
    "* Corporate responsibility or leadership information (optional if found)\n",
    "* All products this company supported\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Review the provided list of links.\n",
    "2. Select only those that directly relate to the company's identity, values, or opportunities.\n",
    "3. Exclude irrelevant links such as facebook, email, instagram, marketing campaigns, or unrelated resources ...\n",
    "4. Return your answer strictly in JSON format following this structure:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"links\": [\n",
    "        {\"type\": \"about page\", \"url\": \"https://full.url/goes/here/about\"},\n",
    "        {\"type\": \"careers page\", \"url\": \"https://another.full.url/careers\"}\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "**Notes:**\n",
    "\n",
    "* The `type` field should clearly describe the purpose of the page (e.g., `about page`, `careers page`, `contact page`).\n",
    "* The `url` field must contain the full URL as provided.\n",
    "* If no relevant links are found, return an empty list: `{ \"links\": [] }`.\n",
    "\"\"\"\n",
    "\n",
    "def generate_relevant_filter_user_prompt(base_url): \n",
    "   user_prompt = f\"\"\"\n",
    "   Here is the list of links on the website {base_url} -\n",
    "   Please decide which of these are relevant web links for relevant for inclusion in customer-facing content about the company, \n",
    "   respond with the full https URL in JSON format.\n",
    "   Do not include Terms of Service, Privacy, email links, facebook, instagram, youtube, twitter\n",
    "\n",
    "   Links (some might be relative links):\n",
    "   \"\"\"\n",
    "   links = fetch_website_links(base_url)\n",
    "   user_prompt += \"\\n\".join(links)\n",
    "   return user_prompt\n",
    "\n",
    "get_relevant_links_agent = Agent(\n",
    "        name=\"Agent: get relevant links\",\n",
    "        instructions=get_relevant_links_system_prompt,\n",
    "        output_type=RelevantLinks,\n",
    "        model=gemini_2_5_flash_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7974306",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_relevant_filter_user_prompt(COMPANY_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b71c356",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_relevant_links(url):\n",
    "    print(f\"AGENT: Link filtering agent is running {url}\")\n",
    "    user_prompt= generate_relevant_filter_user_prompt(url)\n",
    "    with trace(\"Link filtering agent\"):\n",
    "        result = await Runner.run(get_relevant_links_agent, user_prompt)\n",
    "        return result.final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0459c8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_link = await get_relevant_links(COMPANY_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fc16b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_link.links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3762e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_markdown(content: str, filename: str = \"output.md\"):\n",
    "    \"\"\"\n",
    "    Write markdown content to a local file.\n",
    "    Overrides file if it exists, creates new file if it doesn't.\n",
    "    \n",
    "    Args:\n",
    "        content: Markdown text to write\n",
    "        filename: Output filename (default: output.md)\n",
    "    \"\"\"\n",
    "\n",
    "    with open(f\"profiles/{filename}\", 'w', encoding='utf-8') as f:\n",
    "        f.write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ce4b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_to_file(filename, text):\n",
    "    \"\"\"\n",
    "    Appends text to a local file. Creates the file if it doesn't exist.\n",
    "    \n",
    "    Args:\n",
    "        filename: Path to the file\n",
    "        text: Text content to append\n",
    "    \n",
    "    Returns:\n",
    "        True if successful, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create directory if it doesn't exist\n",
    "        directory = os.path.dirname(filename)\n",
    "        if directory and not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "        \n",
    "        # Append to file (creates file if it doesn't exist)\n",
    "        with open(f\"profiles/{filename}\", 'a', encoding='utf-8') as f:\n",
    "            f.write(text)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing to file '{filename}': {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1d3ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process URLs in batches of 10\n",
    "async def run_summarize_in_batches(urls, batch_size=10, delay=60):\n",
    "    results = []\n",
    "\n",
    "    for i in range(0, len(urls), batch_size):\n",
    "        batch = urls[i:i+batch_size]\n",
    "        print(f\"\\nüöÄ Running batch {i//batch_size + 1} ({len(batch)} URLs)...\")\n",
    "\n",
    "        # Run this batch in parallel\n",
    "        tasks = [asyncio.create_task(summarize(url)) for url in batch]\n",
    "        batch_results = await asyncio.gather(*tasks)\n",
    "        results.extend(batch_results)\n",
    "\n",
    "        # If there are more batches left, sleep\n",
    "        if i + batch_size < len(urls):\n",
    "            print(f\"‚è≥ Waiting {delay}s before next batch...\\n\")\n",
    "            await asyncio.sleep(delay)\n",
    "\n",
    "    print(\"\\n‚úÖ All batches completed.\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6eafc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def scrape_recursive(current_url, depth, max_depth, visited_urls, filename, type=\"Infotrack\"):\n",
    "    \"\"\"\n",
    "    Recursively scrapes a URL and its related URLs up to max_depth.\n",
    "    \n",
    "    Args:\n",
    "        current_url: URL to scrape\n",
    "        depth: Current depth level\n",
    "        max_depth: Maximum depth to scrape\n",
    "        visited_urls: Set of already visited URLs\n",
    "        all_content: List to collect content\n",
    "    \"\"\"\n",
    "    #print(f\"scrape_recursive url: {current_url}\")\n",
    "    # Write this 2 if statement to see behavior of function\n",
    "    if depth > max_depth:\n",
    "        print(f\"scrape_recursive exist with depth: {depth}, max: {max_depth}, \")\n",
    "        return\n",
    "    \n",
    "    if current_url in visited_urls:\n",
    "        print(f\"scrape_recursive exist with current_url: {current_url} has already scraped\")\n",
    "        return\n",
    "    print(f\"scrape_recursive url: {current_url} with depth level: {depth}\")\n",
    "    visited_urls.append(current_url)\n",
    "    \n",
    "    # Extract content from current URL\n",
    "    content = await summarize(current_url)\n",
    "    append_to_file(filename, f\"## Type {type} Level {depth}: {current_url}\\n\\n{content}\\n\\n\")\n",
    "    \n",
    "    # Add delay to avoid the rate limit. only need for free account\n",
    "    #delay = 5\n",
    "    #print(f\"‚è≥ Waiting {delay}s before next run...\\n\")\n",
    "    #await asyncio.sleep(delay)\n",
    "    \n",
    "    # Only extract related URLs if not at max depth\n",
    "    if depth < max_depth:\n",
    "        related_urls = await get_relevant_links(current_url)\n",
    "        for link in related_urls.links:\n",
    "            print(f\"Type: {link.type}, URL: {link.url}\")\n",
    "        for related_url in related_urls.links:\n",
    "            await scrape_recursive(related_url.url, depth + 1, max_depth, visited_urls, filename, related_url.type) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648784c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def scrape_website_with_depth(url, max_depth=2, filename: str = \"output.md\"):\n",
    "    \"\"\"\n",
    "    Scrapes website content up to specified depth level.\n",
    "    \n",
    "    Args:\n",
    "        url: Starting URL to scrape\n",
    "        max_depth: Maximum depth level (default: 2)\n",
    "    \n",
    "    Returns:\n",
    "        Markdown formatted summary of all extracted content\n",
    "    \"\"\"\n",
    "    visited_urls = []\n",
    "    await scrape_recursive(url, 1, max_depth, visited_urls, filename, \"Root Page\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634d2841",
   "metadata": {},
   "outputs": [],
   "source": [
    "await scrape_website_with_depth(COMPANY_URL, 3, \"CBTW.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0334f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def fetch_page_and_summurize(url, company_name):\n",
    "    print(f\"start fetch {url}\")\n",
    "    contents = await summarize(url)\n",
    "    relevant_links = await get_relevant_links(url)\n",
    "    summarized_contents = f\"## Landing page:\\n\\n{contents}\\n## Relevant Information:\"\n",
    "\n",
    "    results = await run_summarize_in_batches(relevant_links.links, batch_size=8, delay=60)\n",
    "    for relevant_link, summary in zip(relevant_links.links, results):\n",
    "        summarized_contents += f\"\\n\\n### Link: {relevant_link}\\n\"\n",
    "        summarized_contents += summary\n",
    "\n",
    "    write_markdown(summarized_contents, f\"{company_name}.md\")\n",
    "    return summarized_contents\n",
    "\n",
    "print(await fetch_page_and_summurize(COMPANY_URL, \"Infotrack\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfffedf4",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
