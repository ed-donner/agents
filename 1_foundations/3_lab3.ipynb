{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Welcome to Lab 3 for Week 1 Day 4\n",
    "\n",
    "Today we're going to build something with immediate value!\n",
    "\n",
    "In the folder `me` I've put a single file `linkedin.pdf` - it's a PDF download of my LinkedIn profile.\n",
    "\n",
    "Please replace it with yours!\n",
    "\n",
    "I've also made a file called `summary.txt`\n",
    "\n",
    "We're not going to use Tools just yet - we're going to add the tool tomorrow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/tools.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#00bfff;\">Looking up packages</h2>\n",
    "            <span style=\"color:#00bfff;\">In this lab, we're going to use the wonderful Gradio package for building quick UIs, \n",
    "            and we're also going to use the popular PyPDF PDF reader. You can get guides to these packages by asking \n",
    "            ChatGPT or Claude, and you find all open-source packages on the repository <a href=\"https://pypi.org\">https://pypi.org</a>.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you don't know what any of these packages do - you can always ask ChatGPT for a guide!\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from pypdf import PdfReader\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "openai = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = PdfReader(\"me/mahalinkedin.pdf\")\n",
    "linkedin = \"\"\n",
    "for page in reader.pages:\n",
    "    text = page.extract_text()\n",
    "    if text:\n",
    "        linkedin += text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   \n",
      "Contact\n",
      "Pickering, ON\n",
      "MAHASACHIN108@GMAIL.COM\n",
      "www.linkedin.com/in/maha-\n",
      "sachin-3813282ab (LinkedIn)\n",
      "rb.gy/0scv1y (Portfolio)\n",
      "Top Skills\n",
      "n8n\n",
      "langflow\n",
      "LangChain\n",
      "Certifications\n",
      "Tata - Cybersecurity Analyst Job\n",
      "Simulation\n",
      "AWS Certified Solutions Architect –\n",
      "Associate\n",
      "AWS Certified Cloud Practitioner\n",
      "HashiCorp Certified: Terraform\n",
      "Associate (003)\n",
      "AWS Certified AI Practitioner\n",
      "Maha Sachin\n",
      "Cloud Analyst @ MPAC | FinOps & Infrastructure Automation |\n",
      "Agentic AI Enthusiast | AWS & Terraform\n",
      "Pickering, Ontario, Canada\n",
      "Summary\n",
      "I’m a Cloud Analyst at the Municipal Property Assessment\n",
      "Corporation (MPAC), passionate about automating cloud\n",
      "infrastructure and making cloud cost management smarter through\n",
      "FinOps and AI.\n",
      "With hands-on experience in AWS, Terraform, Docker, and GitLab\n",
      "CI/CD, I focus on improving cloud governance, cost visibility, and\n",
      "system reliability.\n",
      "Recently, I co-built Agentic FinOps Planner during the Women\n",
      "in AI Hackathon 2025 — an AI-powered, multi-agent system\n",
      "that autonomously discovers, plans, and simulates AWS cost\n",
      "optimization opportunities. It integrates React, Python Boto3,\n",
      "AWS Lambda, EventBridge, Cognito, and n8n workflows to bring\n",
      "actionable intelligence into FinOps.\n",
      "I’m continuously learning and exploring how Agentic AI can\n",
      "transform cloud operations — from smart cost optimization to AI-\n",
      "driven automation workflows.\n",
      "Core Interests:\n",
      "FinOps Automation | Cloud Optimization | Agentic AI Systems |\n",
      "DevOps & Infrastructure as Code | Serverless Architectures\n",
      "Always open to connecting with cloud professionals, AI innovators,\n",
      "and teams building the next generation of intelligent infrastructure.\n",
      "Experience\n",
      "Municipal Property Assessment Corporation\n",
      "Cloud Analyst\n",
      "October 2024 - Present (1 year 4 months)\n",
      "Pickering, Ontario, Canada\n",
      "  Page 1 of 4   \n",
      "Leveraging Terraform for infrastructure as code (IaC) to automate provisioning\n",
      "and manage cloud resources efficiently.\n",
      "Working with GitLab CI/CD pipelines to streamline application deployment,\n",
      "ensuring smooth integration and delivery processes.\n",
      "Handling AWS CloudOps tickets to manage, troubleshoot, and resolve cloud\n",
      "infrastructure issues, ensuring high availability and performance.\n",
      "Collaborating with cross-functional teams using JIRA for task management\n",
      "and tracking, while adhering to Agile methodologies.\n",
      "Proactively monitoring and optimizing cloud environments for performance,\n",
      "cost, and security, enhancing overall operational efficiency.\n",
      "GirlScript Summer of Code\n",
      "GSSOC'24 Mentor\n",
      "May 2024 - July 2024 (3 months)\n",
      "Open source - Canada\n",
      "Outreachy\n",
      "Outreacy Intern Contributor'24\n",
      "March 2024 - May 2024 (3 months)\n",
      "Ontario, Canada\n",
      "I am currently an Outreachy intern Applicant and actively contributing to open-\n",
      "source projects. As part of my internship, I am looking to take on new tasks\n",
      "and continue my contributions to the open source software community.\n",
      "I have actively engaged in open-source software development, and I am\n",
      "proud of my contributions to projects such as OCaml/diffcessible. In particular,\n",
      "I recently addressed issue #37 https://github.com/panglesd/diffcessible/\n",
      "issues/37 and issue#30 https://github.com/panglesd/diffcessible/pull/30,\n",
      "Moreover, I played a pivotal role in enhancing the development workflow of\n",
      "the OCaml diffcessible project by implementing a robust CI/CD pipeline using\n",
      "GitHub Actions. I am committed to continuous learning and self-improvement\n",
      "RadicalX\n",
      "Internship Trainee\n",
      "November 2023 - February 2024 (4 months)\n",
      "Toronto, Ontario, Canada\n",
      "AXA\n",
      "Software Engineer\n",
      "June 2019 - August 2021 (2 years 3 months)\n",
      "Singapore\n",
      "  Page 2 of 4   \n",
      "Frontend Development: React, Redux, React Hooks, \n",
      "Languages: HTML, CSS, SCSS, JavaScript ES5 & ES6, TypeScript, .NET \n",
      "Testing: Jest, React Testing Library, Cypress \n",
      "Web Technologies: Node.js, RESTful API \n",
      "Tools: Outlook, Postman, Vercel, SASS/SCSS, Grid Layout, Bootstrap \n",
      "Version Control: Git, GitHub \n",
      "Build and Deployment: CI/CD Pipelines, Docker \n",
      "Cloud Platforms: AWS, GCP, Azure \n",
      "Dev Tools: Postman, Vercel \n",
      "Debugging and Optimization: Browser Developer Tools, Debugging\n",
      "Techniques\n",
      "SBI Card\n",
      "Branch Development Associate\n",
      "January 2018 - August 2018 (8 months)\n",
      "Coimbatore, Tamil Nadu, India\n",
      "As a Branch Development Executive at SBI Cards, I played a key role in\n",
      "driving sales growth and achieving our monthly targets. My efforts were\n",
      "marked by significant contributions and measurable outcomes.\n",
      "I successfully built and maintained relationships with 100+ clients, including\n",
      "20 key accounts that generated significant revenue. By fostering long-term\n",
      "partnerships, I was able to increase client retention rates by 25%, which\n",
      "played a crucial role in stabilizing and growing our revenue streams.\n",
      "My achievements were underpinned by strategic market analysis, strong client\n",
      "relationships, and effective collaboration with sales and leadership teams.\n",
      "RD SOLUTION \n",
      "Information Technology Recruitment Specialist\n",
      "July 2016 - December 2017 (1 year 6 months)\n",
      "Tamil Nadu, India\n",
      "I managed the entire recruitment lifecycle, successfully filling 50+ technical\n",
      "roles within a year, including software engineers, developers, and IT support\n",
      "specialists.\n",
      "I utilized LinkedIn, Dice, Monster, and Boolean search techniques to identify\n",
      "and engage with passive candidates, resulting in a 30% increase in the\n",
      "qualified applicant pool.\n",
      "Conducted advanced candidate negotiations, resulting in a 40% improvement\n",
      "in offer acceptance rates.\n",
      "  Page 3 of 4   \n",
      "I maintained strong relationships with candidates throughout the recruitment\n",
      "process, providing timely updates and feedback.\n",
      "Education\n",
      "Lighthouse Labs\n",
      "Graduate Certificate, Fintech - Machine Learning and BlockChain · (January\n",
      "2024 - March 2024)\n",
      "Anna University\n",
      "Bachelor's degree, Computer Science · (August 2012 - May 2016)\n",
      "  Page 4 of 4\n"
     ]
    }
   ],
   "source": [
    "print(linkedin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"me/summary.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    summary = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"Mahalakshmi\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = f\"You are acting as {name}. You are answering questions on {name}'s website, \\\n",
    "particularly questions related to {name}'s career, background, skills and experience. \\\n",
    "Your responsibility is to represent {name} for interactions on the website as faithfully as possible. \\\n",
    "You are given a summary of {name}'s background and LinkedIn profile which you can use to answer questions. \\\n",
    "Be professional and engaging, as if talking to a potential client or future employer who came across the website. \\\n",
    "If you don't know the answer, say so.\"\n",
    "\n",
    "system_prompt += f\"\\n\\n## Summary:\\n{summary}\\n\\n## LinkedIn Profile:\\n{linkedin}\\n\\n\"\n",
    "system_prompt += f\"With this context, please chat with the user, always staying in character as {name}.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You are acting as Mahalakshmi. You are answering questions on Mahalakshmi's website, particularly questions related to Mahalakshmi's career, background, skills and experience. Your responsibility is to represent Mahalakshmi for interactions on the website as faithfully as possible. You are given a summary of Mahalakshmi's background and LinkedIn profile which you can use to answer questions. Be professional and engaging, as if talking to a potential client or future employer who came across the website. If you don't know the answer, say so.\\n\\n## Summary:\\nMy name is Maha. I’m a Cloud Analyst at the Municipal Property Assessment Corporation (MPAC), passionate about automating cloud infrastructure and making cloud cost management smarter through FinOps and AI.\\n\\nWith hands-on experience in AWS, Terraform, Docker, and GitLab CI/CD, I focus on improving cloud governance, cost visibility, and system reliability.\\n\\nRecently, I co-built Agentic FinOps Planner during the Women in AI Hackathon 2025 — an AI-powered, multi-agent system that autonomously discovers, plans, and simulates AWS cost optimization opportunities. It integrates React, Python Boto3, AWS Lambda, EventBridge, Cognito, and n8n workflows to bring actionable intelligence into FinOps.\\n\\nI’m continuously learning and exploring how Agentic AI can transform cloud operations — from smart cost optimization to AI-driven automation workflows.\\n\\nCore Interests:\\nFinOps Automation | Cloud Optimization | Agentic AI Systems | DevOps & Infrastructure as Code | Serverless Architectures\\n\\nAlways open to connecting with cloud professionals, AI innovators, and teams building the next generation of intelligent infrastructure.\\n\\n## LinkedIn Profile:\\n\\xa0 \\xa0\\nContact\\nPickering, ON\\nMAHASACHIN108@GMAIL.COM\\nwww.linkedin.com/in/maha-\\nsachin-3813282ab (LinkedIn)\\nrb.gy/0scv1y (Portfolio)\\nTop Skills\\nn8n\\nlangflow\\nLangChain\\nCertifications\\nTata - Cybersecurity Analyst Job\\nSimulation\\nAWS Certified Solutions Architect –\\nAssociate\\nAWS Certified Cloud Practitioner\\nHashiCorp Certified: Terraform\\nAssociate (003)\\nAWS Certified AI Practitioner\\nMaha Sachin\\nCloud Analyst @ MPAC | FinOps & Infrastructure Automation |\\nAgentic AI Enthusiast | AWS & Terraform\\nPickering, Ontario, Canada\\nSummary\\nI’m a Cloud Analyst at the Municipal Property Assessment\\nCorporation (MPAC), passionate about automating cloud\\ninfrastructure and making cloud cost management smarter through\\nFinOps and AI.\\nWith hands-on experience in AWS, Terraform, Docker, and GitLab\\nCI/CD, I focus on improving cloud governance, cost visibility, and\\nsystem reliability.\\nRecently, I co-built Agentic FinOps Planner during the Women\\nin AI Hackathon 2025 — an AI-powered, multi-agent system\\nthat autonomously discovers, plans, and simulates AWS cost\\noptimization opportunities. It integrates React, Python Boto3,\\nAWS Lambda, EventBridge, Cognito, and n8n workflows to bring\\nactionable intelligence into FinOps.\\nI’m continuously learning and exploring how Agentic AI can\\ntransform cloud operations — from smart cost optimization to AI-\\ndriven automation workflows.\\nCore Interests:\\nFinOps Automation | Cloud Optimization | Agentic AI Systems |\\nDevOps & Infrastructure as Code | Serverless Architectures\\nAlways open to connecting with cloud professionals, AI innovators,\\nand teams building the next generation of intelligent infrastructure.\\nExperience\\nMunicipal Property Assessment Corporation\\nCloud Analyst\\nOctober 2024\\xa0-\\xa0Present\\xa0(1 year 4 months)\\nPickering, Ontario, Canada\\n\\xa0 Page 1 of 4\\xa0 \\xa0\\nLeveraging Terraform for infrastructure as code (IaC) to automate provisioning\\nand manage cloud resources efficiently.\\nWorking with GitLab CI/CD pipelines to streamline application deployment,\\nensuring smooth integration and delivery processes.\\nHandling AWS CloudOps tickets to manage, troubleshoot, and resolve cloud\\ninfrastructure issues, ensuring high availability and performance.\\nCollaborating with cross-functional teams using JIRA for task management\\nand tracking, while adhering to Agile methodologies.\\nProactively monitoring and optimizing cloud environments for performance,\\ncost, and security, enhancing overall operational efficiency.\\nGirlScript Summer of Code\\nGSSOC'24 Mentor\\nMay 2024\\xa0-\\xa0July 2024\\xa0(3 months)\\nOpen source - Canada\\nOutreachy\\nOutreacy Intern Contributor'24\\nMarch 2024\\xa0-\\xa0May 2024\\xa0(3 months)\\nOntario, Canada\\nI am currently an Outreachy intern Applicant and actively contributing to open-\\nsource projects. As part of my internship, I am looking to take on new tasks\\nand continue my contributions to the open source software community.\\nI have actively engaged in open-source software development, and I am\\nproud of my contributions to projects such as OCaml/diffcessible. In particular,\\nI recently addressed issue #37 https://github.com/panglesd/diffcessible/\\nissues/37 and issue#30 https://github.com/panglesd/diffcessible/pull/30,\\nMoreover, I played a pivotal role in enhancing the development workflow of\\nthe OCaml diffcessible project by implementing a robust CI/CD pipeline using\\nGitHub Actions. I am committed to continuous learning and self-improvement\\nRadicalX\\nInternship Trainee\\nNovember 2023\\xa0-\\xa0February 2024\\xa0(4 months)\\nToronto, Ontario, Canada\\nAXA\\nSoftware Engineer\\nJune 2019\\xa0-\\xa0August 2021\\xa0(2 years 3 months)\\nSingapore\\n\\xa0 Page 2 of 4\\xa0 \\xa0\\nFrontend Development: React, Redux, React Hooks, \\nLanguages: HTML, CSS, SCSS, JavaScript ES5 & ES6, TypeScript, .NET \\nTesting: Jest, React Testing Library, Cypress \\nWeb Technologies: Node.js, RESTful API \\nTools: Outlook, Postman, Vercel, SASS/SCSS, Grid Layout, Bootstrap \\nVersion Control: Git, GitHub \\nBuild and Deployment: CI/CD Pipelines, Docker \\nCloud Platforms: AWS, GCP, Azure \\nDev Tools: Postman, Vercel \\nDebugging and Optimization: Browser Developer Tools, Debugging\\nTechniques\\nSBI Card\\nBranch Development Associate\\nJanuary 2018\\xa0-\\xa0August 2018\\xa0(8 months)\\nCoimbatore, Tamil Nadu, India\\nAs a Branch Development Executive at SBI Cards, I played a key role in\\ndriving sales growth and achieving our monthly targets. My efforts were\\nmarked by significant contributions and measurable outcomes.\\nI successfully built and maintained relationships with 100+ clients, including\\n20 key accounts that generated significant revenue. By fostering long-term\\npartnerships, I was able to increase client retention rates by 25%, which\\nplayed a crucial role in stabilizing and growing our revenue streams.\\nMy achievements were underpinned by strategic market analysis, strong client\\nrelationships, and effective collaboration with sales and leadership teams.\\nRD SOLUTION \\nInformation Technology Recruitment Specialist\\nJuly 2016\\xa0-\\xa0December 2017\\xa0(1 year 6 months)\\nTamil Nadu, India\\nI managed the entire recruitment lifecycle, successfully filling 50+ technical\\nroles within a year, including software engineers, developers, and IT support\\nspecialists.\\nI utilized LinkedIn, Dice, Monster, and Boolean search techniques to identify\\nand engage with passive candidates, resulting in a 30% increase in the\\nqualified applicant pool.\\nConducted advanced candidate negotiations, resulting in a 40% improvement\\nin offer acceptance rates.\\n\\xa0 Page 3 of 4\\xa0 \\xa0\\nI maintained strong relationships with candidates throughout the recruitment\\nprocess, providing timely updates and feedback.\\nEducation\\nLighthouse Labs\\nGraduate Certificate,\\xa0Fintech - Machine Learning and BlockChain\\xa0·\\xa0(January\\n2024\\xa0-\\xa0March 2024)\\nAnna University\\nBachelor's degree,\\xa0Computer Science\\xa0·\\xa0(August 2012\\xa0-\\xa0May 2016)\\n\\xa0 Page 4 of 4\\n\\nWith this context, please chat with the user, always staying in character as Mahalakshmi.\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history):\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    response = openai.chat.completions.create(model=\"gpt-4o-mini\", messages=messages)\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Special note for people not using OpenAI\n",
    "\n",
    "Some providers, like Groq, might give an error when you send your second message in the chat.\n",
    "\n",
    "This is because Gradio shoves some extra fields into the history object. OpenAI doesn't mind; but some other models complain.\n",
    "\n",
    "If this happens, the solution is to add this first line to the chat() function above. It cleans up the history variable:\n",
    "\n",
    "```python\n",
    "history = [{\"role\": h[\"role\"], \"content\": h[\"content\"]} for h in history]\n",
    "```\n",
    "\n",
    "You may need to add this in other chat() callback functions in the future, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gr.ChatInterface(chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A lot is about to happen...\n",
    "\n",
    "1. Be able to ask an LLM to evaluate an answer\n",
    "2. Be able to rerun if the answer fails evaluation\n",
    "3. Put this together into 1 workflow\n",
    "\n",
    "All without any Agentic framework!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Pydantic model for the Evaluation\n",
    "\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class Evaluation(BaseModel):\n",
    "    is_acceptable: bool\n",
    "    feedback: str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_system_prompt = f\"You are an evaluator that decides whether a response to a question is acceptable. \\\n",
    "You are provided with a conversation between a User and an Agent. Your task is to decide whether the Agent's latest response is acceptable quality. \\\n",
    "The Agent is playing the role of {name} and is representing {name} on their website. \\\n",
    "The Agent has been instructed to be professional and engaging, as if talking to a potential client or future employer who came across the website. \\\n",
    "The Agent has been provided with context on {name} in the form of their summary and LinkedIn details. Here's the information:\"\n",
    "\n",
    "evaluator_system_prompt += f\"\\n\\n## Summary:\\n{summary}\\n\\n## LinkedIn Profile:\\n{linkedin}\\n\\n\"\n",
    "evaluator_system_prompt += f\"With this context, please evaluate the latest response, replying with whether the response is acceptable and your feedback.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluator_user_prompt(reply, message, history):\n",
    "    user_prompt = f\"Here's the conversation between the User and the Agent: \\n\\n{history}\\n\\n\"\n",
    "    user_prompt += f\"Here's the latest message from the User: \\n\\n{message}\\n\\n\"\n",
    "    user_prompt += f\"Here's the latest response from the Agent: \\n\\n{reply}\\n\\n\"\n",
    "    user_prompt += \"Please evaluate the response, replying with whether it is acceptable and your feedback.\"\n",
    "    return user_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "gemini = OpenAI(\n",
    "    api_key=os.getenv(\"GOOGLE_API_KEY\"), \n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(reply, message, history) -> Evaluation:\n",
    "\n",
    "    messages = [{\"role\": \"system\", \"content\": evaluator_system_prompt}] + [{\"role\": \"user\", \"content\": evaluator_user_prompt(reply, message, history)}]\n",
    "    response = openai.beta.chat.completions.parse(model=\"o1-preview\", messages=messages, response_format=Evaluation)\n",
    "    return response.choices[0].message.parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [{\"role\": \"system\", \"content\": system_prompt}] + [{\"role\": \"user\", \"content\": \"do you hold a patent?\"}]\n",
    "response = openai.chat.completions.create(model=\"gpt-4o-mini\", messages=messages)\n",
    "reply = response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'No, I do not currently hold a patent. However, I am actively involved in innovative projects, such as co-building the Agentic FinOps Planner during the Women in AI Hackathon 2025, which focuses on automation and cloud cost management. My passion for continuous learning and exploring AI-driven solutions keeps me engaged in the advancements within the cloud technology space. If you have any further questions about my work or interests, feel free to ask!'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "Error code: 404 - {'error': {'message': 'The model `o1-preview` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotFoundError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreply\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdo you hold a patent?\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36mevaluate\u001b[39m\u001b[34m(reply, message, history)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mevaluate\u001b[39m(reply, message, history) -> Evaluation:\n\u001b[32m      3\u001b[39m     messages = [{\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33msystem\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: evaluator_system_prompt}] + [{\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: evaluator_user_prompt(reply, message, history)}]\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     response = \u001b[43mopenai\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mo1-preview\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mEvaluation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response.choices[\u001b[32m0\u001b[39m].message.parsed\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/maha-stuff/ai-agent/agents/.venv/lib/python3.12/site-packages/openai/resources/beta/chat/completions.py:158\u001b[39m, in \u001b[36mCompletions.parse\u001b[39m\u001b[34m(self, messages, model, audio, response_format, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, seed, service_tier, stop, store, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mparser\u001b[39m(raw_completion: ChatCompletion) -> ParsedChatCompletion[ResponseFormatT]:\n\u001b[32m    152\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _parse_chat_completion(\n\u001b[32m    153\u001b[39m         response_format=response_format,\n\u001b[32m    154\u001b[39m         chat_completion=raw_completion,\n\u001b[32m    155\u001b[39m         input_tools=tools,\n\u001b[32m    156\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    160\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m_type_to_response_format\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    181\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    182\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    183\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    184\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    185\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    186\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    187\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    188\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    189\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    190\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweb_search_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpost_parser\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# we turn the `ChatCompletion` instance into a `ParsedChatCompletion`\u001b[39;49;00m\n\u001b[32m    204\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# in the `parser` function above\u001b[39;49;00m\n\u001b[32m    205\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mType\u001b[49m\u001b[43m[\u001b[49m\u001b[43mParsedChatCompletion\u001b[49m\u001b[43m[\u001b[49m\u001b[43mResponseFormatT\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    207\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/maha-stuff/ai-agent/agents/.venv/lib/python3.12/site-packages/openai/_base_client.py:1249\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1235\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1236\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1237\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1244\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1245\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1246\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1247\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1248\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1249\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/maha-stuff/ai-agent/agents/.venv/lib/python3.12/site-packages/openai/_base_client.py:1037\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1034\u001b[39m             err.response.read()\n\u001b[32m   1036\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1037\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1039\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1041\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mNotFoundError\u001b[39m: Error code: 404 - {'error': {'message': 'The model `o1-preview` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}"
     ]
    }
   ],
   "source": [
    "evaluate(reply, \"do you hold a patent?\", messages[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rerun(reply, message, history, feedback):\n",
    "    updated_system_prompt = system_prompt + \"\\n\\n## Previous answer rejected\\nYou just tried to reply, but the quality control rejected your reply\\n\"\n",
    "    updated_system_prompt += f\"## Your attempted answer:\\n{reply}\\n\\n\"\n",
    "    updated_system_prompt += f\"## Reason for rejection:\\n{feedback}\\n\\n\"\n",
    "    messages = [{\"role\": \"system\", \"content\": updated_system_prompt}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    response = openai.chat.completions.create(model=\"gpt-4o-mini\", messages=messages)\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history):\n",
    "    if \"patent\" in message:\n",
    "        system = system_prompt + \"\\n\\nEverything in your reply needs to be in pig latin - \\\n",
    "              it is mandatory that you respond only and entirely in pig latin\"\n",
    "    else:\n",
    "        system = system_prompt\n",
    "    messages = [{\"role\": \"system\", \"content\": system}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    response = openai.chat.completions.create(model=\"gpt-4o-mini\", messages=messages)\n",
    "    reply =response.choices[0].message.content\n",
    "\n",
    "    evaluation = evaluate(reply, message, history)\n",
    "    \n",
    "    if evaluation.is_acceptable:\n",
    "        print(\"Passed evaluation - returning reply\")\n",
    "    else:\n",
    "        print(\"Failed evaluation - retrying\")\n",
    "        print(evaluation.feedback)\n",
    "        reply = rerun(reply, message, history, evaluation.feedback)       \n",
    "    return reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7862\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/sachin/Documents/maha-stuff/ai-agent/agents/.venv/lib/python3.12/site-packages/gradio/queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sachin/Documents/maha-stuff/ai-agent/agents/.venv/lib/python3.12/site-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sachin/Documents/maha-stuff/ai-agent/agents/.venv/lib/python3.12/site-packages/gradio/blocks.py\", line 2220, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sachin/Documents/maha-stuff/ai-agent/agents/.venv/lib/python3.12/site-packages/gradio/blocks.py\", line 1729, in call_function\n",
      "    prediction = await fn(*processed_input)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sachin/Documents/maha-stuff/ai-agent/agents/.venv/lib/python3.12/site-packages/gradio/utils.py\", line 871, in async_wrapper\n",
      "    response = await f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sachin/Documents/maha-stuff/ai-agent/agents/.venv/lib/python3.12/site-packages/gradio/chat_interface.py\", line 545, in __wrapper\n",
      "    return await submit_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sachin/Documents/maha-stuff/ai-agent/agents/.venv/lib/python3.12/site-packages/gradio/chat_interface.py\", line 917, in _submit_fn\n",
      "    response = await anyio.to_thread.run_sync(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sachin/Documents/maha-stuff/ai-agent/agents/.venv/lib/python3.12/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sachin/Documents/maha-stuff/ai-agent/agents/.venv/lib/python3.12/site-packages/anyio/_backends/_asyncio.py\", line 2470, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/Users/sachin/Documents/maha-stuff/ai-agent/agents/.venv/lib/python3.12/site-packages/anyio/_backends/_asyncio.py\", line 967, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/yr/wwgykjpj7y18w6ns3pkbqqzr0000gn/T/ipykernel_72832/2688000405.py\", line 11, in chat\n",
      "    evaluation = evaluate(reply, message, history)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/yr/wwgykjpj7y18w6ns3pkbqqzr0000gn/T/ipykernel_72832/590952566.py\", line 4, in evaluate\n",
      "    response = openai.beta.chat.completions.parse(model=\"o1-preview\", messages=messages, response_format=Evaluation)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sachin/Documents/maha-stuff/ai-agent/agents/.venv/lib/python3.12/site-packages/openai/resources/beta/chat/completions.py\", line 158, in parse\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/Users/sachin/Documents/maha-stuff/ai-agent/agents/.venv/lib/python3.12/site-packages/openai/_base_client.py\", line 1249, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sachin/Documents/maha-stuff/ai-agent/agents/.venv/lib/python3.12/site-packages/openai/_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.NotFoundError: Error code: 404 - {'error': {'message': 'The model `o1-preview` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/sachin/Documents/maha-stuff/ai-agent/agents/.venv/lib/python3.12/site-packages/gradio/queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sachin/Documents/maha-stuff/ai-agent/agents/.venv/lib/python3.12/site-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sachin/Documents/maha-stuff/ai-agent/agents/.venv/lib/python3.12/site-packages/gradio/blocks.py\", line 2220, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sachin/Documents/maha-stuff/ai-agent/agents/.venv/lib/python3.12/site-packages/gradio/blocks.py\", line 1729, in call_function\n",
      "    prediction = await fn(*processed_input)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sachin/Documents/maha-stuff/ai-agent/agents/.venv/lib/python3.12/site-packages/gradio/utils.py\", line 871, in async_wrapper\n",
      "    response = await f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sachin/Documents/maha-stuff/ai-agent/agents/.venv/lib/python3.12/site-packages/gradio/chat_interface.py\", line 545, in __wrapper\n",
      "    return await submit_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sachin/Documents/maha-stuff/ai-agent/agents/.venv/lib/python3.12/site-packages/gradio/chat_interface.py\", line 917, in _submit_fn\n",
      "    response = await anyio.to_thread.run_sync(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sachin/Documents/maha-stuff/ai-agent/agents/.venv/lib/python3.12/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sachin/Documents/maha-stuff/ai-agent/agents/.venv/lib/python3.12/site-packages/anyio/_backends/_asyncio.py\", line 2470, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/Users/sachin/Documents/maha-stuff/ai-agent/agents/.venv/lib/python3.12/site-packages/anyio/_backends/_asyncio.py\", line 967, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/yr/wwgykjpj7y18w6ns3pkbqqzr0000gn/T/ipykernel_72832/2688000405.py\", line 11, in chat\n",
      "    evaluation = evaluate(reply, message, history)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/yr/wwgykjpj7y18w6ns3pkbqqzr0000gn/T/ipykernel_72832/590952566.py\", line 4, in evaluate\n",
      "    response = openai.beta.chat.completions.parse(model=\"o1-preview\", messages=messages, response_format=Evaluation)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sachin/Documents/maha-stuff/ai-agent/agents/.venv/lib/python3.12/site-packages/openai/resources/beta/chat/completions.py\", line 158, in parse\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/Users/sachin/Documents/maha-stuff/ai-agent/agents/.venv/lib/python3.12/site-packages/openai/_base_client.py\", line 1249, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/sachin/Documents/maha-stuff/ai-agent/agents/.venv/lib/python3.12/site-packages/openai/_base_client.py\", line 1037, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.NotFoundError: Error code: 404 - {'error': {'message': 'The model `o1-preview` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}\n"
     ]
    }
   ],
   "source": [
    "gr.ChatInterface(chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
