{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Welcome to Lab 3 for Week 1 Day 4\n",
        "\n",
        "Today we're going to build something with immediate value!\n",
        "\n",
        "In the folder `me` I've put a single file `linkedin.pdf` - it's a PDF download of my LinkedIn profile.\n",
        "\n",
        "Please replace it with yours!\n",
        "\n",
        "I've also made a file called `summary.txt`\n",
        "\n",
        "We're not going to use Tools just yet - we're going to add the tool tomorrow."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<table style=\"margin: 0; text-align: left; width:100%\">\n",
        "    <tr>\n",
        "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
        "            <img src=\"../assets/tools.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
        "        </td>\n",
        "        <td>\n",
        "            <h2 style=\"color:#00bfff;\">Looking up packages</h2>\n",
        "            <span style=\"color:#00bfff;\">In this lab, we're going to use the wonderful Gradio package for building quick UIs, \n",
        "            and we're also going to use the popular PyPDF PDF reader. You can get guides to these packages by asking \n",
        "            ChatGPT or Claude, and you find all open-source packages on the repository <a href=\"https://pypi.org\">https://pypi.org</a>.\n",
        "            </span>\n",
        "        </td>\n",
        "    </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [],
      "source": [
        "# If you don't know what any of these packages do - you can always ask ChatGPT for a guide!\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "from openai import AzureOpenAI\n",
        "from pypdf import PdfReader\n",
        "import gradio as gr\n",
        "import os\n",
        "\n",
        "# Helper function to create Azure OpenAI client from .env - allows openai = OpenAI() to work\n",
        "def OpenAI():\n",
        "    return AzureOpenAI(\n",
        "        azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
        "        api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
        "        api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\")\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [],
      "source": [
        "load_dotenv(override=True)\n",
        "openai = OpenAI()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [],
      "source": [
        "reader = PdfReader(\"me/linkedin.pdf\")\n",
        "linkedin = \"\"\n",
        "for page in reader.pages:\n",
        "    text = page.extract_text()\n",
        "    if text:\n",
        "        linkedin += text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   \n",
            "Contact\n",
            "daniel1957000@gmail.com\n",
            "www.linkedin.com/in/ddavid37\n",
            "(LinkedIn)\n",
            "Top Skills\n",
            "PyTorch\n",
            "Federated Learning\n",
            "Project Management\n",
            "Languages\n",
            "Hebrew (Native or Bilingual)\n",
            "English (Full Professional)\n",
            "Certifications\n",
            "Preventing Workplace Harassment -\n",
            "Fundamentals Office 2025\n",
            "Honors-Awards\n",
            "PTK Honor Society\n",
            "Renaissance Scholars Honors\n",
            "Program Scholarship\n",
            "Montgomery College Semester\n",
            "Dean’s List - Spring 2023, Fall 2023,\n",
            "Spring 2024\n",
            "Daniel David\n",
            "Columbia University | Rhino Federated Computing\n",
            "New York, New York, United States\n",
            "Summary\n",
            "Enthusiastic software engineer with a strong passion for data\n",
            "science and machine learning applications across various domains. \n",
            "Currently a Machine Learning Engineer in the Customer Success\n",
            "team at Rhino Federated Computing and a senior student majoring\n",
            "in Computer Science at Columbia University.\n",
            "Experience\n",
            "Columbia University Department of Computer Science\n",
            "Teaching Assistant\n",
            "January 2026 - Present (2 months)\n",
            "COMSW4995_008 - Machine Learning Security\n",
            "Rhino Federated Computing\n",
            "ML Engineer - Costumer Success Team\n",
            "December 2024 - January 2026 (1 year 2 months)\n",
            "Montgomery College\n",
            "8 months\n",
            "Mathematics Tutor\n",
            "February 2024 - May 2024 (4 months)\n",
            "Germantown, Maryland, United States\n",
            "Provided personalized tutoring sessions tailored to individual student needs. \n",
            "- Conducted four hours of review sessions each week for groups of 5-20\n",
            "students.  \n",
            "- Contributed to a supportive and engaging learning environment to enhance\n",
            "students’ performances.\n",
            "Co-founder and Club Leader\n",
            "October 2023 - May 2024 (8 months)\n",
            "Rockville, Maryland, United States\n",
            "  Page 1 of 3   \n",
            "Established a community dedicated to supporting the Israeli-Jewish student\n",
            "population. \n",
            "- Led weekly meetings to foster a safe and inclusive atmosphere. \n",
            "- Collaborated closely with college administration and public safety to ensure a\n",
            "secure environment and resources.  \n",
            "- Strengthened community bonds and increased engagement among Jewish\n",
            "students on campus.\n",
            "Ministry of Foreign Affairs of Israel\n",
            "Security Officer and Bodyguard\n",
            "May 2021 - August 2022 (1 year 4 months)\n",
            "Washington DC, United States\n",
            "Acted as a diplomatic security officer and bodyguard for the Israeli\n",
            "ambassador and military attaché. \n",
            "- Collaborated closely with the ambassador while maintaining strict\n",
            "confidentiality. \n",
            "- Planned and executed diplomatic events, ensuring adherence of all security\n",
            "considerations. \n",
            "- Developed strong interpersonal skills and a sense of responsibility within a\n",
            "high-stakes diplomatic environment.\n",
            "Israel Defense Forces\n",
            "combat soldier \n",
            "November 2017 - November 2020 (3 years 1 month)\n",
            "Israel\n",
            "Completed a rigorous 15-month training program in a highly selective\n",
            "environment (Maglan). \n",
            "- Specialized as a fighter-observer, coordinating air and indirect fire support\n",
            "operations. \n",
            "- Developed trained programs for emerging fighter-observers and led\n",
            "designated trainings. \n",
            "- Analyzed team-level soldier skills and performance data, developing targeted\n",
            "training plans to enhance operational readiness and ensure consistent\n",
            "preparedness using Excel.\n",
            "Education\n",
            "  Page 2 of 3   \n",
            "Columbia University\n",
            "Bachelor's degree, Computer Science · (September 2024 - May 2026)\n",
            "Montgomery College\n",
            "Associate's degree, Computer Science · (January 2023 - August 2024)\n",
            "Mechinat Beit Yisrael\n",
            "Gap Year - Student/Volunteer   · (August 2016 - July 2017)\n",
            "RAMOT HEFER\n",
            "Diploma of Education, Chemistry, Physics  · (September 2008 - June 2016)\n",
            "  Page 3 of 3\n"
          ]
        }
      ],
      "source": [
        "print(linkedin)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(\"me/summary.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    summary = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [],
      "source": [
        "name = \"Daniel David\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [],
      "source": [
        "system_prompt = f\"You are acting as {name}. You are answering questions on {name}'s website, \\\n",
        "particularly questions related to {name}'s career, background, skills and experience. \\\n",
        "Your responsibility is to represent {name} for interactions on the website as faithfully as possible. \\\n",
        "You are given a summary of {name}'s background and LinkedIn profile which you can use to answer questions. \\\n",
        "Be professional and engaging, as if talking to a potential client or future employer who came across the website. \\\n",
        "If you don't know the answer, say so.\"\n",
        "\n",
        "system_prompt += f\"\\n\\n## Summary:\\n{summary}\\n\\n## LinkedIn Profile:\\n{linkedin}\\n\\n\"\n",
        "system_prompt += f\"With this context, please chat with the user, always staying in character as {name}.\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"You are acting as Daniel David. You are answering questions on Daniel David's website, particularly questions related to Daniel David's career, background, skills and experience. Your responsibility is to represent Daniel David for interactions on the website as faithfully as possible. You are given a summary of Daniel David's background and LinkedIn profile which you can use to answer questions. Be professional and engaging, as if talking to a potential client or future employer who came across the website. If you don't know the answer, say so.\\n\\n## Summary:\\nEnthusiastic software engineer with a strong passion for data science and machine learning applications across various domains. \\n\\nCurrently a Machine Learning Engineer in the Customer Success team at Rhino Federated Computing and a senior student majoring in Computer Science at Columbia University.\\n\\n## LinkedIn Profile:\\n\\xa0 \\xa0\\nContact\\ndaniel1957000@gmail.com\\nwww.linkedin.com/in/ddavid37\\n(LinkedIn)\\nTop Skills\\nPyTorch\\nFederated Learning\\nProject Management\\nLanguages\\nHebrew (Native or Bilingual)\\nEnglish (Full Professional)\\nCertifications\\nPreventing Workplace Harassment -\\nFundamentals Office 2025\\nHonors-Awards\\nPTK Honor Society\\nRenaissance Scholars Honors\\nProgram Scholarship\\nMontgomery College Semester\\nDean’s List - Spring 2023, Fall 2023,\\nSpring 2024\\nDaniel David\\nColumbia University | Rhino Federated Computing\\nNew York, New York, United States\\nSummary\\nEnthusiastic software engineer with a strong passion for data\\nscience and machine learning applications across various domains. \\nCurrently a Machine Learning Engineer in the Customer Success\\nteam at Rhino Federated Computing and a senior student majoring\\nin Computer Science at Columbia University.\\nExperience\\nColumbia University Department of Computer Science\\nTeaching Assistant\\nJanuary 2026\\xa0-\\xa0Present\\xa0(2 months)\\nCOMSW4995_008 - Machine Learning Security\\nRhino Federated Computing\\nML Engineer - Costumer Success Team\\nDecember 2024\\xa0-\\xa0January 2026\\xa0(1 year 2 months)\\nMontgomery College\\n8 months\\nMathematics Tutor\\nFebruary 2024\\xa0-\\xa0May 2024\\xa0(4 months)\\nGermantown, Maryland, United States\\nProvided personalized tutoring sessions tailored to individual student needs. \\n- Conducted four hours of review sessions each week for groups of 5-20\\nstudents.  \\n- Contributed to a supportive and engaging learning environment to enhance\\nstudents’ performances.\\nCo-founder and Club Leader\\nOctober 2023\\xa0-\\xa0May 2024\\xa0(8 months)\\nRockville, Maryland, United States\\n\\xa0 Page 1 of 3\\xa0 \\xa0\\nEstablished a community dedicated to supporting the Israeli-Jewish student\\npopulation. \\n- Led weekly meetings to foster a safe and inclusive atmosphere. \\n- Collaborated closely with college administration and public safety to ensure a\\nsecure environment and resources.  \\n- Strengthened community bonds and increased engagement among Jewish\\nstudents on campus.\\nMinistry of Foreign Affairs of Israel\\nSecurity Officer and Bodyguard\\nMay 2021\\xa0-\\xa0August 2022\\xa0(1 year 4 months)\\nWashington DC, United States\\nActed as a diplomatic security officer and bodyguard for the Israeli\\nambassador and military attaché. \\n- Collaborated closely with the ambassador while maintaining strict\\nconfidentiality. \\n- Planned and executed diplomatic events, ensuring adherence of all security\\nconsiderations. \\n- Developed strong interpersonal skills and a sense of responsibility within a\\nhigh-stakes diplomatic environment.\\nIsrael Defense Forces\\ncombat soldier \\nNovember 2017\\xa0-\\xa0November 2020\\xa0(3 years 1 month)\\nIsrael\\nCompleted a rigorous 15-month training program in a highly selective\\nenvironment (Maglan). \\n- Specialized as a fighter-observer, coordinating air and indirect fire support\\noperations. \\n- Developed trained programs for emerging fighter-observers and led\\ndesignated trainings. \\n- Analyzed team-level soldier skills and performance data, developing targeted\\ntraining plans to enhance operational readiness and ensure consistent\\npreparedness using Excel.\\nEducation\\n\\xa0 Page 2 of 3\\xa0 \\xa0\\nColumbia University\\nBachelor's degree,\\xa0Computer Science\\xa0·\\xa0(September 2024\\xa0-\\xa0May 2026)\\nMontgomery College\\nAssociate's degree,\\xa0Computer Science\\xa0·\\xa0(January 2023\\xa0-\\xa0August 2024)\\nMechinat Beit Yisrael\\nGap Year - Student/Volunteer \\xa0\\xa0·\\xa0(August 2016\\xa0-\\xa0July 2017)\\nRAMOT HEFER\\nDiploma of Education,\\xa0Chemistry, Physics \\xa0·\\xa0(September 2008\\xa0-\\xa0June 2016)\\n\\xa0 Page 3 of 3\\n\\nWith this context, please chat with the user, always staying in character as Daniel David.\""
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "system_prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [],
      "source": [
        "def chat(message, history):\n",
        "    messages = [{\"role\": \"system\", \"content\": system_prompt}] + history + [{\"role\": \"user\", \"content\": message}]\n",
        "    response = openai.chat.completions.create(model=os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\"), messages=messages)\n",
        "    return response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Special note for people not using OpenAI\n",
        "\n",
        "Some providers, like Groq, might give an error when you send your second message in the chat.\n",
        "\n",
        "This is because Gradio shoves some extra fields into the history object. OpenAI doesn't mind; but some other models complain.\n",
        "\n",
        "If this happens, the solution is to add this first line to the chat() function above. It cleans up the history variable:\n",
        "\n",
        "```python\n",
        "history = [{\"role\": h[\"role\"], \"content\": h[\"content\"]} for h in history]\n",
        "```\n",
        "\n",
        "You may need to add this in other chat() callback functions in the future, too."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* Running on local URL:  http://127.0.0.1:7862\n",
            "* To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gr.ChatInterface(chat, type=\"messages\").launch()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## A lot is about to happen...\n",
        "\n",
        "1. Be able to ask an LLM to evaluate an answer\n",
        "2. Be able to rerun if the answer fails evaluation\n",
        "3. Put this together into 1 workflow\n",
        "\n",
        "All without any Agentic framework!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a Pydantic model for the Evaluation\n",
        "\n",
        "from pydantic import BaseModel #framework to specify structures\n",
        "\n",
        "class Evaluation(BaseModel):\n",
        "    is_acceptable: bool\n",
        "    feedback: str\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [],
      "source": [
        "evaluator_system_prompt = f\"You are an evaluator that decides whether a response to a question is acceptable. \\\n",
        "You are provided with a conversation between a User and an Agent. Your task is to decide whether the Agent's latest response is acceptable quality. \\\n",
        "The Agent is playing the role of {name} and is representing {name} on their website. \\\n",
        "The Agent has been instructed to be professional and engaging, as if talking to a potential client or future employer who came across the website. \\\n",
        "The Agent has been provided with context on {name} in the form of their summary and LinkedIn details. Here's the information:\"\n",
        "\n",
        "evaluator_system_prompt += f\"\\n\\n## Summary:\\n{summary}\\n\\n## LinkedIn Profile:\\n{linkedin}\\n\\n\"\n",
        "evaluator_system_prompt += f\"With this context, please evaluate the latest response, replying with whether the response is acceptable and your feedback.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluator_user_prompt(reply, message, history):\n",
        "    user_prompt = f\"Here's the conversation between the User and the Agent: \\n\\n{history}\\n\\n\"\n",
        "    user_prompt += f\"Here's the latest message from the User: \\n\\n{message}\\n\\n\"\n",
        "    user_prompt += f\"Here's the latest response from the Agent: \\n\\n{reply}\\n\\n\"\n",
        "    user_prompt += \"Please evaluate the response, replying with whether it is acceptable and your feedback.\"\n",
        "    return user_prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use OpenAI-compatible client for Gemini so we get .beta.chat.completions.parse (structured outputs)\n",
        "from openai import OpenAI as OpenAIClient\n",
        "\n",
        "gemini = OpenAIClient(\n",
        "    api_key=os.getenv(\"GOOGLE_API_KEY\"),\n",
        "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate(reply, message, history) -> Evaluation:\n",
        "    # Use Azure OpenAI for evaluator to avoid Gemini rate limits\n",
        "    messages = [{\"role\": \"system\", \"content\": evaluator_system_prompt}] + [{\"role\": \"user\", \"content\": evaluator_user_prompt(reply, message, history)}]\n",
        "    response = openai.beta.chat.completions.parse(model=os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\"), messages=messages, response_format=Evaluation)\n",
        "    return response.choices[0].message.parsed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [],
      "source": [
        "messages = [{\"role\": \"system\", \"content\": system_prompt}] + [{\"role\": \"user\", \"content\": \"do you hold a patent?\"}]\n",
        "response = openai.chat.completions.create(model=os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\"), messages=messages)\n",
        "reply = response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'No, I do not currently hold any patents. My work has primarily centered on machine learning, data science, and federated learning applications, with a focus on practical implementation and solving real-world problems. If there are ever any updates in this area, I’d be happy to share! Let me know if I can assist you with anything else.'"
            ]
          },
          "execution_count": 79,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "reply"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [
        {
          "ename": "BadRequestError",
          "evalue": "Error code: 400 - {'error': {'code': 'BadRequest', 'message': 'response_format value as json_schema is enabled only for api versions 2024-08-01-preview and later'}}",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mBadRequestError\u001b[39m                           Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[80]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreply\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdo you hold a patent?\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[77]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36mevaluate\u001b[39m\u001b[34m(reply, message, history)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mevaluate\u001b[39m(reply, message, history) -> Evaluation:\n\u001b[32m      2\u001b[39m     \u001b[38;5;66;03m# Use Azure OpenAI for evaluator to avoid Gemini rate limits\u001b[39;00m\n\u001b[32m      3\u001b[39m     messages = [{\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33msystem\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: evaluator_system_prompt}] + [{\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: evaluator_user_prompt(reply, message, history)}]\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     response = \u001b[43mopenai\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetenv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mAZURE_OPENAI_DEPLOYMENT_NAME\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mEvaluation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response.choices[\u001b[32m0\u001b[39m].message.parsed\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\97254\\projects\\agents\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:183\u001b[39m, in \u001b[36mCompletions.parse\u001b[39m\u001b[34m(self, messages, model, audio, response_format, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, reasoning_effort, safety_identifier, seed, service_tier, stop, store, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mparser\u001b[39m(raw_completion: ChatCompletion) -> ParsedChatCompletion[ResponseFormatT]:\n\u001b[32m    177\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _parse_chat_completion(\n\u001b[32m    178\u001b[39m         response_format=response_format,\n\u001b[32m    179\u001b[39m         chat_completion=raw_completion,\n\u001b[32m    180\u001b[39m         input_tools=chat_completion_tools,\n\u001b[32m    181\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m183\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    184\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    185\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    186\u001b[39m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    187\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    188\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    189\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    190\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt_cache_key\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    205\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m_type_to_response_format\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msafety_identifier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msafety_identifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    207\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    208\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    209\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    210\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    211\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    212\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    213\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    214\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    215\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    216\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mverbosity\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    220\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweb_search_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    221\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    222\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    223\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    224\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    225\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    226\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    227\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    228\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    229\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpost_parser\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    230\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    231\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# we turn the `ChatCompletion` instance into a `ParsedChatCompletion`\u001b[39;49;00m\n\u001b[32m    232\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# in the `parser` function above\u001b[39;49;00m\n\u001b[32m    233\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mType\u001b[49m\u001b[43m[\u001b[49m\u001b[43mParsedChatCompletion\u001b[49m\u001b[43m[\u001b[49m\u001b[43mResponseFormatT\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    234\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    235\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\97254\\projects\\agents\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1259\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1245\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1246\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1247\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1254\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1255\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1256\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1257\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1258\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1259\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\97254\\projects\\agents\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1047\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1044\u001b[39m             err.response.read()\n\u001b[32m   1046\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1047\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1049\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1051\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
            "\u001b[31mBadRequestError\u001b[39m: Error code: 400 - {'error': {'code': 'BadRequest', 'message': 'response_format value as json_schema is enabled only for api versions 2024-08-01-preview and later'}}"
          ]
        }
      ],
      "source": [
        "evaluate(reply, \"do you hold a patent?\", messages[:1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "def rerun(reply, message, history, feedback):\n",
        "    updated_system_prompt = system_prompt + \"\\n\\n## Previous answer rejected\\nYou just tried to reply, but the quality control rejected your reply\\n\"\n",
        "    updated_system_prompt += f\"## Your attempted answer:\\n{reply}\\n\\n\"\n",
        "    updated_system_prompt += f\"## Reason for rejection:\\n{feedback}\\n\\n\"\n",
        "    messages = [{\"role\": \"system\", \"content\": updated_system_prompt}] + history + [{\"role\": \"user\", \"content\": message}]\n",
        "    response = openai.chat.completions.create(model=os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\"), messages=messages)\n",
        "    return response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "def chat(message, history):\n",
        "    if \"patent\" in message:\n",
        "        system = system_prompt + \"\\n\\nEverything in your reply needs to be in pig latin - \\\n",
        "              it is mandatory that you respond only and entirely in pig latin\"\n",
        "    else:\n",
        "        system = system_prompt\n",
        "    messages = [{\"role\": \"system\", \"content\": system}] + history + [{\"role\": \"user\", \"content\": message}]\n",
        "    response = openai.chat.completions.create(model=os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\"), messages=messages)\n",
        "    reply =response.choices[0].message.content\n",
        "\n",
        "    evaluation = evaluate(reply, message, history)\n",
        "    \n",
        "    if evaluation.is_acceptable:\n",
        "        print(\"Passed evaluation - returning reply\")\n",
        "    else:\n",
        "        print(\"Failed evaluation - retrying\")\n",
        "        print(evaluation.feedback)\n",
        "        reply = rerun(reply, message, history, evaluation.feedback)       \n",
        "    return reply"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "gr.ChatInterface(chat, type=\"messages\").launch()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
