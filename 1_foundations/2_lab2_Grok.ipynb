{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Welcome to the Second Lab - Week 1, Day 3\n",
    "\n",
    "Today we will work with lots of models! This is a way to get comfortable with APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with imports - ask ChatGPT to explain any package that you don't know\n",
    "\n",
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Always remember to do this!\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key exists and begins sk-proj-\n",
      "Anthropic API Key exists and begins sk-ant-\n",
      "Google API Key exists and begins AI\n",
      "DeepSeek API Key exists and begins sk-\n",
      "Groq API Key exists and begins gsk_\n",
      "Grok API Key exists and begins xai-\n",
      "Ollama API Key exists and begins olla\n"
     ]
    }
   ],
   "source": [
    "# Print the key prefixes to help with any debugging\n",
    "\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "deepseek_api_key = os.getenv('DEEPSEEK_API_KEY')\n",
    "groq_api_key = os.getenv('GROQ_API_KEY')\n",
    "grok_api_key = os.getenv('GROK_API_KEY')\n",
    "ollama_api_key = os.getenv('OLLAMA_API_KEY')\n",
    "\n",
    "openai_llm_model = os.getenv('OPENAI_LLM_MODEL')\n",
    "anthropic_llm_model = os.getenv('ANTHROPIC_LLM_MODEL')\n",
    "google_llm_model = os.getenv('GOOGLE_LLM_MODEL')\n",
    "deepseek_llm_model = os.getenv('DEEPSEEK_LLM_MODEL')\n",
    "groq_llm_model = os.getenv('GROQ_LLM_MODEL')\n",
    "grok_llm_model = os.getenv('GROK_LLM_MODEL')\n",
    "ollama_llm_model = os.getenv('OLLAMA_LLM_MODEL')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "if anthropic_api_key:\n",
    "    print(f\"Anthropic API Key exists and begins {anthropic_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"Anthropic API Key not set (and this is optional)\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key exists and begins {google_api_key[:2]}\")\n",
    "else:\n",
    "    print(\"Google API Key not set (and this is optional)\")\n",
    "\n",
    "if deepseek_api_key:\n",
    "    print(f\"DeepSeek API Key exists and begins {deepseek_api_key[:3]}\")\n",
    "else:\n",
    "    print(\"DeepSeek API Key not set (and this is optional)\")\n",
    "\n",
    "if groq_api_key:\n",
    "    print(f\"Groq API Key exists and begins {groq_api_key[:4]}\")\n",
    "else:\n",
    "    print(\"Groq API Key not set (and this is optional)\")\n",
    "\n",
    "if grok_api_key:\n",
    "    print(f\"Grok API Key exists and begins {grok_api_key[:4]}\")\n",
    "else:\n",
    "    print(\"Grok API Key not set (and this is optional)\")\n",
    "\n",
    "if ollama_api_key:\n",
    "    print(f\"Ollama API Key exists and begins {ollama_api_key[:4]}\")\n",
    "else:\n",
    "    print(\"Ollama API Key not set (and this is optional)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "request = \"Please come up with a challenging, nuanced question that I can ask a number of LLMs to evaluate their intelligence. \"\n",
    "request += \"Answer only with the question, no explanation.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xai_sdk import Client\n",
    "from xai_sdk.chat import user, system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How do you think the ethical implications of artificial intelligence in decision-making processes, such as in judicial systems or hiring practices, balance against the potential benefits of increased efficiency and reduced human bias, and what specific safeguards should be implemented to address these ethical concerns?\n"
     ]
    }
   ],
   "source": [
    "client = Client(api_key=grok_api_key)\n",
    "\n",
    "chat = client.chat.create(model=grok_llm_model)\n",
    "\n",
    "chat.append(user(request))\n",
    "\n",
    "question = chat.sample().content\n",
    "print(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ethical implications of artificial intelligence (AI) in decision-making processes, such as in judicial systems or hiring practices, are complex and multifaceted, requiring a careful balance between the potential benefits and the risks of harm or unfairness. AI offers significant advantages, including increased efficiency, scalability, and the potential to reduce certain types of human bias. However, it also introduces new ethical challenges, such as the risk of perpetuating or amplifying existing biases, lack of transparency, and the erosion of human accountability. Below, I’ll break this down into benefits, ethical concerns, and specific safeguards.\n",
      "\n",
      "### Benefits of AI in Decision-Making\n",
      "1. **Increased Efficiency**: AI systems can process vast amounts of data quickly, reducing the time and cost associated with decision-making. For instance, in judicial systems, AI tools can assist with case management, legal research, or predicting recidivism rates, allowing judges to focus on nuanced aspects of cases. In hiring, AI can screen thousands of resumes in minutes, identifying candidates based on predefined criteria.\n",
      "2. **Reduction of Human Bias (in Theory)**: Human decision-making is often influenced by unconscious biases related to race, gender, socioeconomic status, or personal experiences. AI, if designed well, could theoretically make decisions based on objective data, minimizing subjective errors. For example, an AI hiring tool might focus purely on skills and qualifications rather than a candidate’s name or appearance.\n",
      "3. **Consistency**: AI systems apply the same rules and criteria uniformly, avoiding the variability that can occur with human decision-makers who might be influenced by mood, fatigue, or other factors.\n",
      "\n",
      "### Ethical Implications and Challenges\n",
      "1. **Bias in AI Systems**: AI is not inherently neutral; it learns from data that often reflects historical and societal biases. For example, in judicial systems, tools like COMPAS (used for recidivism prediction in the U.S.) have been criticized for disproportionately flagging Black defendants as high-risk, reflecting biases in historical arrest data. Similarly, AI hiring tools have been shown to favor male candidates when trained on data from industries with gender imbalances (e.g., Amazon’s scrapped AI recruiting tool in 2018).\n",
      "2. **Lack of Transparency**: Many AI systems, especially those using complex machine learning models like neural networks, operate as \"black boxes,\" where the decision-making process is opaque even to their creators. This raises concerns in contexts like judicial sentencing or hiring, where individuals have a right to understand why a decision was made.\n",
      "3. **Accountability Issues**: When AI makes a decision, it’s unclear who is responsible for errors or harm— the developers, the organization using the tool, or the AI itself. This is particularly problematic in high-stakes areas like criminal justice, where a wrongful decision could result in loss of liberty.\n",
      "4. **Dehumanization and Loss of Agency**: Over-reliance on AI risks reducing human judgment to mere rubber-stamping of algorithmic outputs, potentially undermining the role of empathy, context, and moral reasoning in decisions. For instance, a judge might feel compelled to follow an AI recommendation on sentencing, even if their intuition suggests otherwise.\n",
      "5. **Privacy Concerns**: AI systems often rely on large datasets, which may include sensitive personal information. In hiring, for example, AI might analyze social media profiles or other personal data, raising questions about consent and data security.\n",
      "\n",
      "### Balancing Benefits and Ethical Concerns\n",
      "The benefits of efficiency and reduced human bias must be weighed against the potential for harm and unfairness. AI should not be seen as a replacement for human judgment but as a tool to augment it. The goal should be to leverage AI’s strengths (speed, consistency) while preserving human oversight for nuanced, value-based decisions. For instance, AI can flag potential risks or patterns in data, but final decisions in judicial or hiring contexts should remain with humans who can consider broader ethical and contextual factors.\n",
      "\n",
      "Moreover, the risk of bias in AI is not a reason to abandon the technology but rather a call to address systemic inequalities in the data and design process. If historical data reflects bias, efforts must be made to correct or contextualize it, and AI systems must be regularly audited for fairness.\n",
      "\n",
      "### Specific Safeguards to Address Ethical Concerns\n",
      "1. **Bias Mitigation and Fairness Audits**:\n",
      "   - Regularly audit AI systems for bias in outcomes across demographic groups (e.g., race, gender, socioeconomic status). This includes testing algorithms on diverse datasets and using fairness metrics to evaluate performance.\n",
      "   - Use techniques like \"debiasing\" training data or implementing fairness-aware algorithms to reduce discriminatory patterns. For example, in hiring, AI tools should be trained on balanced datasets that represent diverse candidate pools.\n",
      "2. **Transparency and Explainability**:\n",
      "   - Develop AI systems with explainable outputs, ensuring that decisions can be traced back to specific data points or logic. For instance, if an AI tool denies a job application, it should provide a clear reason (e.g., lack of specific skills) rather than a generic rejection.\n",
      "   - Mandate public disclosure of how AI tools are used in high-stakes decisions, such as in judicial sentencing or parole decisions, to allow for scrutiny and accountability.\n",
      "3. **Human Oversight and Appeal Mechanisms**:\n",
      "   - Ensure that AI decisions are always subject to human review, especially in critical areas like criminal justice. For example, a judge should have the authority to override an AI recommendation on recidivism risk if contextual factors suggest a different outcome.\n",
      "   - Establish clear appeal processes for individuals affected by AI decisions, allowing them to challenge outcomes and seek human intervention.\n",
      "4. **Ethical Design and Inclusive Development**:\n",
      "   - Involve diverse teams in AI development to minimize blind spots related to cultural or social biases. This includes ethicists, sociologists, and representatives from affected communities.\n",
      "   - Adhere to ethical guidelines, such as the EU’s AI Act or UNESCO’s Recommendation on the Ethics of AI, which emphasize human rights, fairness, and accountability in AI design.\n",
      "5. **Data Privacy Protections**:\n",
      "   - Implement strict data protection measures to ensure that personal information used by AI systems is anonymized, securely stored, and collected with informed consent.\n",
      "   - Limit the scope of data used by AI to what is strictly necessary for the task, avoiding overreach (e.g., analyzing unrelated personal details in hiring decisions).\n",
      "6. **Regulatory Frameworks and Accountability**:\n",
      "   - Enact regulations that hold organizations accountable for AI outcomes, including penalties for discriminatory or harmful decisions. For example, companies using AI in hiring should face legal consequences if their tools are found to systematically disadvantage protected groups.\n",
      "   - Create independent oversight bodies to monitor AI deployment in sensitive areas like judicial systems, ensuring compliance with ethical standards.\n",
      "7. **Public Education and Engagement**:\n",
      "   - Educate the public about how AI is used in decision-making processes to build trust and enable informed discourse. For instance, individuals should understand how AI influences hiring or legal outcomes that affect them.\n",
      "   - Encourage participatory design processes where stakeholders (e.g., defendants, job applicants) can provide input on AI systems that impact their lives.\n",
      "\n",
      "### Conclusion\n",
      "The integration of AI into decision-making processes holds immense potential to enhance efficiency and consistency, but it must be approached with caution to avoid perpetuating harm or eroding trust. Ethical concerns like bias, transparency, and accountability are significant but not insurmountable. By implementing safeguards such as fairness audits, human oversight, and robust regulatory frameworks, we can harness AI’s benefits while minimizing its risks. Ultimately, the balance lies in treating AI as a supportive tool rather than a decision-maker, ensuring that human values and judgment remain at the core of critical processes like justice and employment. This approach requires ongoing vigilance, collaboration across disciplines, and a commitment to equity in both technology and society.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Markdown' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m answer \u001b[38;5;241m=\u001b[39m chat\u001b[38;5;241m.\u001b[39msample()\u001b[38;5;241m.\u001b[39mcontent\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(answer)\n\u001b[0;32m---> 11\u001b[0m display(\u001b[43mMarkdown\u001b[49m(answer))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Markdown' is not defined"
     ]
    }
   ],
   "source": [
    "#GROK from xAI\n",
    "\n",
    "client = Client(api_key=grok_api_key)\n",
    "chat = client.chat.create(model=grok_llm_model)\n",
    "\n",
    "chat.append(user(question))\n",
    "\n",
    "answer = chat.sample().content\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The ethical implications of artificial intelligence (AI) in decision-making processes, such as in judicial systems or hiring practices, are complex and multifaceted, requiring a careful balance between the potential benefits and the risks of harm or unfairness. AI offers significant advantages, including increased efficiency, scalability, and the potential to reduce certain types of human bias. However, it also introduces new ethical challenges, such as the risk of perpetuating or amplifying existing biases, lack of transparency, and the erosion of human accountability. Below, I’ll break this down into benefits, ethical concerns, and specific safeguards.\n",
       "\n",
       "### Benefits of AI in Decision-Making\n",
       "1. **Increased Efficiency**: AI systems can process vast amounts of data quickly, reducing the time and cost associated with decision-making. For instance, in judicial systems, AI tools can assist with case management, legal research, or predicting recidivism rates, allowing judges to focus on nuanced aspects of cases. In hiring, AI can screen thousands of resumes in minutes, identifying candidates based on predefined criteria.\n",
       "2. **Reduction of Human Bias (in Theory)**: Human decision-making is often influenced by unconscious biases related to race, gender, socioeconomic status, or personal experiences. AI, if designed well, could theoretically make decisions based on objective data, minimizing subjective errors. For example, an AI hiring tool might focus purely on skills and qualifications rather than a candidate’s name or appearance.\n",
       "3. **Consistency**: AI systems apply the same rules and criteria uniformly, avoiding the variability that can occur with human decision-makers who might be influenced by mood, fatigue, or other factors.\n",
       "\n",
       "### Ethical Implications and Challenges\n",
       "1. **Bias in AI Systems**: AI is not inherently neutral; it learns from data that often reflects historical and societal biases. For example, in judicial systems, tools like COMPAS (used for recidivism prediction in the U.S.) have been criticized for disproportionately flagging Black defendants as high-risk, reflecting biases in historical arrest data. Similarly, AI hiring tools have been shown to favor male candidates when trained on data from industries with gender imbalances (e.g., Amazon’s scrapped AI recruiting tool in 2018).\n",
       "2. **Lack of Transparency**: Many AI systems, especially those using complex machine learning models like neural networks, operate as \"black boxes,\" where the decision-making process is opaque even to their creators. This raises concerns in contexts like judicial sentencing or hiring, where individuals have a right to understand why a decision was made.\n",
       "3. **Accountability Issues**: When AI makes a decision, it’s unclear who is responsible for errors or harm— the developers, the organization using the tool, or the AI itself. This is particularly problematic in high-stakes areas like criminal justice, where a wrongful decision could result in loss of liberty.\n",
       "4. **Dehumanization and Loss of Agency**: Over-reliance on AI risks reducing human judgment to mere rubber-stamping of algorithmic outputs, potentially undermining the role of empathy, context, and moral reasoning in decisions. For instance, a judge might feel compelled to follow an AI recommendation on sentencing, even if their intuition suggests otherwise.\n",
       "5. **Privacy Concerns**: AI systems often rely on large datasets, which may include sensitive personal information. In hiring, for example, AI might analyze social media profiles or other personal data, raising questions about consent and data security.\n",
       "\n",
       "### Balancing Benefits and Ethical Concerns\n",
       "The benefits of efficiency and reduced human bias must be weighed against the potential for harm and unfairness. AI should not be seen as a replacement for human judgment but as a tool to augment it. The goal should be to leverage AI’s strengths (speed, consistency) while preserving human oversight for nuanced, value-based decisions. For instance, AI can flag potential risks or patterns in data, but final decisions in judicial or hiring contexts should remain with humans who can consider broader ethical and contextual factors.\n",
       "\n",
       "Moreover, the risk of bias in AI is not a reason to abandon the technology but rather a call to address systemic inequalities in the data and design process. If historical data reflects bias, efforts must be made to correct or contextualize it, and AI systems must be regularly audited for fairness.\n",
       "\n",
       "### Specific Safeguards to Address Ethical Concerns\n",
       "1. **Bias Mitigation and Fairness Audits**:\n",
       "   - Regularly audit AI systems for bias in outcomes across demographic groups (e.g., race, gender, socioeconomic status). This includes testing algorithms on diverse datasets and using fairness metrics to evaluate performance.\n",
       "   - Use techniques like \"debiasing\" training data or implementing fairness-aware algorithms to reduce discriminatory patterns. For example, in hiring, AI tools should be trained on balanced datasets that represent diverse candidate pools.\n",
       "2. **Transparency and Explainability**:\n",
       "   - Develop AI systems with explainable outputs, ensuring that decisions can be traced back to specific data points or logic. For instance, if an AI tool denies a job application, it should provide a clear reason (e.g., lack of specific skills) rather than a generic rejection.\n",
       "   - Mandate public disclosure of how AI tools are used in high-stakes decisions, such as in judicial sentencing or parole decisions, to allow for scrutiny and accountability.\n",
       "3. **Human Oversight and Appeal Mechanisms**:\n",
       "   - Ensure that AI decisions are always subject to human review, especially in critical areas like criminal justice. For example, a judge should have the authority to override an AI recommendation on recidivism risk if contextual factors suggest a different outcome.\n",
       "   - Establish clear appeal processes for individuals affected by AI decisions, allowing them to challenge outcomes and seek human intervention.\n",
       "4. **Ethical Design and Inclusive Development**:\n",
       "   - Involve diverse teams in AI development to minimize blind spots related to cultural or social biases. This includes ethicists, sociologists, and representatives from affected communities.\n",
       "   - Adhere to ethical guidelines, such as the EU’s AI Act or UNESCO’s Recommendation on the Ethics of AI, which emphasize human rights, fairness, and accountability in AI design.\n",
       "5. **Data Privacy Protections**:\n",
       "   - Implement strict data protection measures to ensure that personal information used by AI systems is anonymized, securely stored, and collected with informed consent.\n",
       "   - Limit the scope of data used by AI to what is strictly necessary for the task, avoiding overreach (e.g., analyzing unrelated personal details in hiring decisions).\n",
       "6. **Regulatory Frameworks and Accountability**:\n",
       "   - Enact regulations that hold organizations accountable for AI outcomes, including penalties for discriminatory or harmful decisions. For example, companies using AI in hiring should face legal consequences if their tools are found to systematically disadvantage protected groups.\n",
       "   - Create independent oversight bodies to monitor AI deployment in sensitive areas like judicial systems, ensuring compliance with ethical standards.\n",
       "7. **Public Education and Engagement**:\n",
       "   - Educate the public about how AI is used in decision-making processes to build trust and enable informed discourse. For instance, individuals should understand how AI influences hiring or legal outcomes that affect them.\n",
       "   - Encourage participatory design processes where stakeholders (e.g., defendants, job applicants) can provide input on AI systems that impact their lives.\n",
       "\n",
       "### Conclusion\n",
       "The integration of AI into decision-making processes holds immense potential to enhance efficiency and consistency, but it must be approached with caution to avoid perpetuating harm or eroding trust. Ethical concerns like bias, transparency, and accountability are significant but not insurmountable. By implementing safeguards such as fairness audits, human oversight, and robust regulatory frameworks, we can harness AI’s benefits while minimizing its risks. Ultimately, the balance lies in treating AI as a supportive tool rather than a decision-maker, ensuring that human values and judgment remain at the core of critical processes like justice and employment. This approach requires ongoing vigilance, collaboration across disciplines, and a commitment to equity in both technology and society."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "display(Markdown(answer))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
