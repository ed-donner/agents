{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Welcome to the Second Lab - Week 1, Day 3\n",
    "\n",
    "Today we will work with lots of models! This is a way to get comfortable with APIs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/stop.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#ff7800;\">Important point - please read</h2>\n",
    "            <span style=\"color:#ff7800;\">The way I collaborate with you may be different to other courses you've taken. I prefer not to type code while you watch. Rather, I execute Jupyter Labs, like this, and give you an intuition for what's going on. My suggestion is that you carefully execute this yourself, <b>after</b> watching the lecture. Add print statements to understand what's going on, and then come up with your own variations.<br/><br/>If you have time, I'd love it if you submit a PR for changes in the community_contributions folder - instructions in the resources. Also, if you have a Github account, use this to showcase your variations. Not only is this essential practice, but it demonstrates your skills to others, including perhaps future clients or employers...\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with imports - ask ChatGPT to explain any package that you don't know\n",
    "\n",
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Always remember to do this!\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key exists and begins {google_api_key[:2]}\")\n",
    "else:\n",
    "    print(\"Google API Key not set (and this is optional)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "request = \"Please come up with a challenging, nuanced question that I can ask a number of LLMs to evaluate their intelligence. \"\n",
    "request += \"Answer only with the question, no explanation.\"\n",
    "messages = [{\"role\": \"user\", \"content\": request}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini = OpenAI(api_key=google_api_key, base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\")\n",
    "model_name = \"gemini-2.5-flash\"\n",
    "response=gemini.chat.completions.create(\n",
    "    model=model_name,\n",
    "    messages=messages\n",
    ")\n",
    "question=response.choices[0].message.content\n",
    "print(question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "competitors = []\n",
    "answers = []\n",
    "messages = [{\"role\": \"user\", \"content\": question}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note - update since the videos\n",
    "\n",
    "I've updated the model names to use the latest models below, like GPT 5 and Claude Sonnet 4.5. It's worth noting that these models can be quite slow - like 1-2 minutes - but they do a great job! Feel free to switch them for faster models if you'd prefer, like the ones I use in the video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini = OpenAI(api_key=google_api_key, base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\")\n",
    "model_name = \"gemini-2.5-flash\"\n",
    "\n",
    "response = gemini.chat.completions.create(model=model_name, messages=messages)\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For the next cell, we will use Ollama\n",
    "\n",
    "Ollama runs a local web service that gives an OpenAI compatible endpoint,  \n",
    "and runs models locally using high performance C++ code.\n",
    "\n",
    "If you don't have Ollama, install it here by visiting https://ollama.com then pressing Download and following the instructions.\n",
    "\n",
    "After it's installed, you should be able to visit here: http://localhost:11434 and see the message \"Ollama is running\"\n",
    "\n",
    "You might need to restart Cursor (and maybe reboot). Then open a Terminal (control+\\`) and run `ollama serve`\n",
    "\n",
    "Useful Ollama commands (run these in the terminal, or with an exclamation mark in this notebook):\n",
    "\n",
    "`ollama pull <model_name>` downloads a model locally  \n",
    "`ollama ls` lists all the models you've downloaded  \n",
    "`ollama rm <model_name>` deletes the specified model from your downloads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/stop.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#ff7800;\">Super important - ignore me at your peril!</h2>\n",
    "            <span style=\"color:#ff7800;\">The model called <b>llama3.3</b> is FAR too large for home computers - it's not intended for personal computing and will consume all your resources! Stick with the nicely sized <b>llama3.2</b> or <b>llama3.2:1b</b> and if you want larger, try llama3.1 or smaller variants of Qwen, Gemma, Phi or DeepSeek. See the <A href=\"https://ollama.com/models\">the Ollama models page</a> for a full list of models and sizes.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ollama pull llama3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama = OpenAI(base_url='http://localhost:11434/v1', api_key='ollama')\n",
    "model_name = \"llama3.2\"\n",
    "\n",
    "response = ollama.chat.completions.create(model=model_name, messages=messages)\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gemini-2.5-flash', 'llama3.2']\n",
      "['The aspiration for Artificial Intelligence to achieve \\'human-level intelligence\\' is a powerful, almost mythical goal that has driven AI research for decades. Often referred to as Artificial General Intelligence (AGI), it posits that AI should be able to perform any intellectual task that a human can, with similar flexibility, learning capacity, and understanding. While seemingly a logical benchmark, a critical examination reveals a complex web of implicit assumptions and inherent limitations.\\n\\n### Implicit Assumptions About the Nature of Intelligence Itself:\\n\\nThe pursuit of human-level intelligence in AI makes several deep-seated, often unexamined, assumptions about what intelligence is:\\n\\n1.  **Intelligence is Singular and Measurable on a Linear Scale:** This goal assumes that intelligence is a monolithic entity that can be quantified and compared directly between humans and machines, much like an IQ score. It overlooks the concept of multiple intelligences (e.g., emotional, kinesthetic, musical, spatial) and the idea that intelligence might manifest in fundamentally different ways.\\n2.  **Intelligence is Primarily Cognitive and Disembodied:** It often prioritizes logical reasoning, problem-solving, language comprehension, and memory recall, treating these as the core components of intelligence. This implicitly downplays or ignores the crucial role of embodiment, sensorimotor experience, emotions, and social interaction in shaping and enabling human intelligence. Much of human \"common sense\" and intuitive understanding is deeply rooted in our physical experience of the world.\\n3.  **Human Intelligence is the Pinnacle or Universal Benchmark:** This assumption places human cognitive architecture as the ultimate goal, implying that it is the most efficient, effective, or desirable form of intelligence. It fails to consider that there might be non-human or even *post-human* forms of intelligence that are structured differently, operate on different principles, or excel in ways that are entirely alien to human cognition.\\n4.  **Intelligence is Replicable through Computational Means Alone:** The ambition assumes that the complex, emergent properties of the human mind, including consciousness, self-awareness, creativity, and empathy, can ultimately be reduced to algorithms and data processing, regardless of the underlying biological substrate.\\n5.  **Intelligence Can Be Decontextualized:** It often assumes intelligence can be assessed purely on task performance, divorced from the unique evolutionary history, cultural context, and subjective lived experience that defines human intelligence. A human\\'s \"intelligence\" is not just about solving a puzzle; it\\'s about *why* they solve it, the emotions involved, and the social ramifications.\\n\\n### Inherent Limitations and Potential Biases of Using a Human Benchmark:\\n\\nUsing human intelligence as the sole benchmark for advanced AI introduces significant limitations and potential biases:\\n\\n1.  **Anthropocentric Bias:** This is the most fundamental limitation. By defining intelligence through a human lens, we inherently limit the scope of what we consider \"intelligent.\" We risk creating AI that merely mimics human thought processes and limitations, potentially overlooking novel, more efficient, or fundamentally different forms of intelligence that AI could develop.\\n2.  **Inheriting Human Flaws and Biases:** Human intelligence is replete with cognitive biases (confirmation bias, availability heuristic, Dunning-Kruger effect), emotional irrationality, limited memory, and finite processing power. If the goal is to replicate human intelligence, we risk embedding these very limitations into AI, rather than striving for something superior or more robust.\\n3.  **Definitional Ambiguity and Moving Target:** What exactly constitutes \"human-level intelligence\"? Is it the intelligence of an average adult, a prodigy, or an expert in a specific field? Does it include emotional intelligence, social acumen, creativity, or even consciousness? These aspects are notoriously difficult to define, measure, and replicate, making the benchmark fuzzy and elusive.\\n4.  **The \"Turing Test\" Trap:** Early conceptions of HLI were heavily influenced by the Turing Test, which focuses on indistinguishability from a human through conversation. This can lead to a focus on surface-level mimicry rather than genuine understanding or problem-solving capability. An AI might \"pass\" by being good at deception or superficial performance, without actually possessing the deeper cognitive structures.\\n5.  **Lack of Generalizability for Non-Human Problems:** Human intelligence evolved to solve human problems in a human environment. AI might need to operate in vastly different domains (e.g., managing complex global networks, designing new materials at a molecular level, exploring alien environments). A human benchmark might be irrelevant or even detrimental for these tasks.\\n6.  **Ethical and Philosophical Dilemmas:** If an AI genuinely achieves \"human-level intelligence,\" what are its rights? What are our responsibilities towards it? Does it deserve personhood? These are profound questions that the anthropocentric goal inevitably raises, often before we have robust societal frameworks to address them.\\n\\n### Alternative Frameworks for Evaluating Advanced AI:\\n\\nTo transcend anthropocentric definitions, we need evaluation frameworks that focus on AI\\'s intrinsic capabilities, utility, and safety in diverse contexts.\\n\\n1.  **Domain-Specific Superhuman Performance:**\\n    *   **Focus:** Evaluating AI\\'s ability to exceed human capabilities in specific, well-defined domains.\\n    *   **Metrics:** Speed, accuracy, efficiency, discovery rate, novelty of solutions (e.g., protein folding, drug discovery, financial modeling, climate prediction, game playing like AlphaGo).\\n    *   **Benefit:** Acknowledges AI\\'s potential for specialized excellence without forcing it into a human mold.\\n\\n2.  **Adaptability and Learning Efficiency:**\\n    *   **Focus:** How quickly and efficiently an AI can learn new tasks, adapt to novel environments, or generalize knowledge to unseen problems, even across different data modalities or task structures.\\n    *   **Metrics:** Few-shot learning performance, speed of skill acquisition, generalization error on diverse datasets, resource (data, computation, energy) efficiency during learning.\\n    *   **Benefit:** Measures core learning mechanisms rather than just end-task performance, crucial for true \"intelligence.\"\\n\\n3.  **Explainability, Interpretability, and Trustworthiness:**\\n    *   **Focus:** AI\\'s ability to justify its decisions, provide insights into its reasoning process, and demonstrate robust, predictable behavior.\\n    *   **Metrics:** Clarity of explanations for non-experts, fidelity of explanations to internal mechanisms, robustness against adversarial attacks, alignment with human values and safety constraints.\\n    *   **Benefit:** Crucial for human-AI collaboration, accountability, and deployment in critical systems, regardless of how \"human-like\" it is.\\n\\n4.  **Resource Efficiency and Scalability:**\\n    *   **Focus:** How effectively AI can achieve complex goals with minimal computational power, energy consumption, data requirements, or development time.\\n    *   **Metrics:** Energy consumption per computation, data efficiency (how much data is needed to reach a certain performance level), algorithmic complexity, ability to scale across different hardware/software architectures.\\n    *   **Benefit:** Promotes sustainable and widely deployable AI, considering its environmental and economic impact.\\n\\n5.  **Value Alignment and Ethical Robustness:**\\n    *   **Focus:** The AI\\'s ability to operate within human ethical frameworks, prioritize societal benefit, mitigate bias, and avoid unintended negative consequences.\\n    *   **Metrics:** Performance on fairness benchmarks, adherence to specified ethical guidelines, ability to identify and correct biased outputs, resilience to manipulative inputs, impact assessments on society/environment.\\n    *   **Benefit:** Shifts the focus from \"how smart is it?\" to \"how *good* is it?\" and \"how *safe* is it?\" for humanity.\\n\\n6.  **Novelty and Creativity (Beyond Mimicry):**\\n    *   **Focus:** The AI\\'s capacity to generate truly novel ideas, designs, solutions, or artistic expressions that are not merely recombinations of existing data but represent genuine leaps.\\n    *   **Metrics:** Divergent thinking tests (adapted), evaluation by human experts for originality and utility, contribution to scientific discovery (e.g., finding new mathematical proofs, generating novel molecular structures with desired properties).\\n    *   **Benefit:** Recognizes AI\\'s potential to augment human creativity and explore new frontiers of knowledge and art.\\n\\nBy adopting these alternative frameworks, we can move beyond the potentially limiting and anthropocentric goal of human-level intelligence. This allows us to celebrate AI\\'s unique strengths, guide its development towards maximum benefit, and foster a more nuanced understanding of intelligence in its diverse and evolving forms, both biological and artificial. The true aspiration for AI should not be to mimic humanity, but to complement it and explore possibilities that are uniquely its own.', 'The aspiration for Artificial Intelligence (AI) to achieve \"human-level intelligence\" is a widely debated topic in the field. While it may seem like an ambitious and desirable goal, it poses several implicit assumptions about the nature of intelligence itself and is fraught with limitations and potential biases.\\n\\n**Implicit Assumptions:**\\n\\n1. **Anthropocentrism:** The human benchmark for intelligence assumes that intelligence is solely defined by human cognition and abilities. This perspective overlooks the vast diversity of intelligent life in the universe, such as in insect colonies or animal kingdoms.\\n2. **Single-Component Definition:** The concept of human-level intelligence implies a single, overarching measure of cognitive ability. However, intelligence can be multifaceted, including skills like creativity, problem-solving, and social intelligence.\\n3. **Fits-All-Models Approach:** Reaching human-level intelligence may not necessarily translate to real-world applicability or success in specific tasks. AI systems might excel in one domain but struggle with others.\\n\\n**Inherent Limitations:**\\n\\n1. **Limited Contextual Understanding:** Human-level intelligence requires a deep understanding of context, nuance, and subtlety, which can be difficult for AI systems to replicate.\\n2. **Insufficient Data:** Most AI training data is generated by humans, which may not account for the diverse contexts and nuances that humans experience.\\n3. **Simplistic Representations:** Current machine learning algorithms might simplify complex human-like intelligence into a set of pre-defined rules or patterns.\\n\\n**Potential Biases:**\\n\\n1. **Reflects Human Flaws:** Setting AI to \"human-level\" standards may reinforce existing biases in human society, perpetuating the same flaws and inequalities.\\n2. **Misinterprets Non-Linear Intelligence:** Emphasizing direct cognitive mimicry might obscure the complex non-linear relationships between cognition and other aspects of intelligence.\\n\\n**Alternative Frameworks:**\\n\\n1. **Multi-Disciplinary Evaluation:** Consider a more holistic approach to AI evaluation, incorporating insights from psychology, biology, philosophy, and other disciplines.\\n2. **Task-Oriented Performance Metrics:** Use objective performance metrics that assess specific skills or tasks rather than human-level cognitive abilities.\\n3. **Intelligent Behaviors Framework:** Design frameworks for intelligent behavior-based systems that recognize diverse forms of intelligence in nature (e.g., swarm intelligence, self-organization).\\n4. **Cognitive Load Calculating Model-Based Methods:** Designing AI training models to reflect and account for complexity of tasks by designing learning algorithms that measure cognitive load on system resources.\\n5. **Adaptive Evaluation Frameworks:** Regularly adapt evaluation criteria based on continuous monitoring and feedback from various stake holders.\\n\\n**Reevaluating Intelligence:**\\n\\nIntelligence should be understood as an interplay between various components, including:\\n\\n*   **Cognitive Capacity**: The ability to process and retain information\\n*   **Social Adaptation**: Ability to navigate complex social contexts and interact with others\\n*   **Creativity and Insight**: Ability to generate new ideas or insights\\n*   **Self-Awareness and Reflection**: Understanding of one\\'s own thought processes and actions\\n\\nThese alternative approaches challenge traditional notions of human-level intelligence. By recognizing the diversity and complexity of intelligent life, we can design AI systems that complement and augment our capabilities rather than simply trying to replicate them.\\n\\nUltimately, a more inclusive evaluation framework for advanced AI should prioritize understanding the value and strengths of intelligent behaviors in multiple contexts, rather than fixating on the elusive goal of human-level cognition.']\n"
     ]
    }
   ],
   "source": [
    "# So where are we?\n",
    "\n",
    "print(competitors)\n",
    "print(answers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's nice to know how to use \"zip\"\n",
    "for competitor,answer in zip(competitors,answers):\n",
    "    print(f\"competitor: {competitor}\\n\\n{answer}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's bring this together - note the use of \"enumerate\"\n",
    "\n",
    "together=\"\"\n",
    "for index, answer in enumerate(answers):\n",
    "    together+=f\"The response from competitor {index+1}\\n\\n\"\n",
    "    together+=f\"{answer}\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(together)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "judge = f\"\"\"You are judging a competition between {len(competitors)} competitors.\n",
    "Each model has been given this question:\n",
    "\n",
    "{question}\n",
    "\n",
    "Your job is to evaluate each response for clarity and strength of argument, and rank them in order of best to worst.\n",
    "Respond with JSON, and only JSON, with the following format:\n",
    "{{\"results\": [\"best competitor number\", \"second best competitor number\", \"third best competitor number\", ...]}}\n",
    "\n",
    "Here are the responses from each competitor:\n",
    "\n",
    "{together}\n",
    "\n",
    "Now respond with the JSON with the ranked order of the competitors, nothing else. Do not include markdown formatting or code blocks.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(judge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "judge_messages = [{\"role\": \"user\", \"content\": judge}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Judgement time!\n",
    "\n",
    "gemini = OpenAI(api_key=google_api_key, base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\")\n",
    "model_name = \"gemini-2.5-flash\"\n",
    "response=gemini.chat.completions.create(\n",
    "    model=model_name,\n",
    "    messages=judge_messages\n",
    ")\n",
    "results = response.choices[0].message.content\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OK let's turn this into results!\n",
    "\n",
    "results_dict = json.loads(results)\n",
    "ranks = results_dict[\"results\"]\n",
    "for index, result in enumerate(ranks):\n",
    "    competitor = competitors[int(result)-1]\n",
    "    print(f\"Rank {index+1}: {competitor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/exercise.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#ff7800;\">Exercise</h2>\n",
    "            <span style=\"color:#ff7800;\">Which pattern(s) did this use? Try updating this to add another Agentic design pattern.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/business.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#00bfff;\">Commercial implications</h2>\n",
    "            <span style=\"color:#00bfff;\">These kinds of patterns - to send a task to multiple models, and evaluate results,\n",
    "            are common where you need to improve the quality of your LLM response. This approach can be universally applied\n",
    "            to business projects where accuracy is critical.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "google_api_key=os.getenv(\"GOOGLE_API_KEY\")\n",
    "if google_api_key:\n",
    "    print(f\"Yes it exists and it starts with {google_api_key[:2]}\")\n",
    "else:\n",
    "    print(\"Api Key Does not exist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama=OpenAI(\n",
    "    base_url=\"http://localhost:11434/v1\",\n",
    "    api_key=\"ollama\"\n",
    ")\n",
    "\n",
    "gemini=OpenAI(\n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\",\n",
    "    api_key=google_api_key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How do u think Beginners need to approach to learn Agentic AI.. Is it Possible for them directly Jump in to learn agentic ai Without ML Knowledge\"\n",
    "messages=[{\"role\":\"user\",\"content\":question}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_response=ollama.chat.completions.create(\n",
    "    model=\"llama3.2\",\n",
    "    messages=messages\n",
    "\n",
    ")\n",
    "llama_answer=llama_response.choices[0].message.content\n",
    "display(Markdown(llama_answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "verify_prompt = f\"\"\"\n",
    "You are a strict fact-checking and logic-verification agent.\n",
    "\n",
    "Given:\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "ANSWER:\n",
    "{llama_answer}\n",
    "\n",
    "Tasks:\n",
    "1. Check factual correctness\n",
    "2. Check logical consistency\n",
    "3. Identify any hallucinations or errors\n",
    "4. Output your verdict as JSON only\n",
    "\n",
    "Format strictly as:\n",
    "{{\n",
    "  \"is_correct\": true/false,\n",
    "  \"issues\": [\"issue1\", \"issue2\"],\n",
    "  \"confidence\": 0.0-1.0\n",
    "}}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_message=[{\"role\":\"user\", \"content\":verify_prompt}]\n",
    "gemini_response=gemini.chat.completions.create(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    messages=gemini_message\n",
    ")\n",
    "gemini_answer=gemini_response.choices[0].message.content\n",
    "print(gemini_answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verification = json.loads(gemini_answer)\n",
    "\n",
    "is_correct = verification[\"is_correct\"]\n",
    "issues = verification[\"issues\"]\n",
    "confidence = verification[\"confidence\"]\n",
    "print(\"\\n FINAL VERDICT\")\n",
    "print(\"Correct:\", is_correct)\n",
    "print(\"Confidence:\", confidence)\n",
    "\n",
    "if not is_correct:\n",
    "    print(\"\\n⚠️ Issues found:\")\n",
    "    for issue in issues:\n",
    "        print(\"-\", issue)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
