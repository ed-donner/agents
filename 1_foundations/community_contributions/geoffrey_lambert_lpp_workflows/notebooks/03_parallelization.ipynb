{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4705719",
   "metadata": {},
   "source": [
    "# 03 - Parallelization: Multi-Model Consensus\n",
    "\n",
    "## What is Parallelization?\n",
    "\n",
    "Parallelization sends the same task to multiple models simultaneously and compares their results. Instead of trusting one model's judgment, you get consensus or flag disagreements.\n",
    "\n",
    "## Why use it for privilege review?\n",
    "\n",
    "Privilege decisions are high-stakes:\n",
    "- A false negative (missing privilege) could waive privilege permanently\n",
    "- A false positive (over-claiming) wastes lawyer time but is safer\n",
    "\n",
    "Multiple models provide:\n",
    "- **Consensus:** If 3 models agree, higher confidence\n",
    "- **Disagreement detection:** If models disagree, flag for human review\n",
    "- **Defensibility:** \"Three independent AI systems reached the same conclusion\"\n",
    "\n",
    "## How it works\n",
    "```\n",
    "Document ──┬──→ Model A ──→ PRIVILEGED\n",
    "           ├──→ Model B ──→ PRIVILEGED\n",
    "           └──→ Model C ──→ NOT_PRIVILEGED\n",
    "                              ↓\n",
    "                    Disagreement detected\n",
    "                              ↓\n",
    "                    Escalate to human review\n",
    "```\n",
    "\n",
    "## Australian Law Reference\n",
    "\n",
    "- Evidence Act 1995 (Cth) ss 118-119\n",
    "- For high-stakes privilege decisions, consensus reduces risk of error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d65362",
   "metadata": {},
   "source": [
    "## Step 1: Setup\n",
    "\n",
    "Import libraries and create clients for multiple models.\n",
    "\n",
    "**Prerequisites - Install Ollama:**\n",
    "1. Download and install Ollama from https://ollama.ai\n",
    "2. In terminal, run: `ollama pull llama3.2:1b`\n",
    "3. Verify with: `ollama list`\n",
    "4. Ollama runs automatically at `http://localhost:11434`\n",
    "\n",
    "**What this does:**\n",
    "- `from openai import OpenAI` — loads the OpenAI library\n",
    "- `openai_client = OpenAI()` — connects to OpenAI cloud API\n",
    "- `ollama_client = OpenAI(base_url=...)` — connects to local Ollama server\n",
    "- Ollama provides an OpenAI-compatible API, so we use the same library\n",
    "- This gives us two independent models: one cloud, one local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a580bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# OpenAI client (cloud)\n",
    "openai_client = OpenAI()\n",
    "\n",
    "# Ollama client (local) - uses OpenAI-compatible API\n",
    "ollama_client = OpenAI(\n",
    "    base_url=\"http://localhost:11434/v1\",\n",
    "    api_key=\"ollama\"  # Ollama doesn't need a real key\n",
    ")\n",
    "\n",
    "# Models\n",
    "MODEL_OPENAI = \"gpt-4.1-nano\"\n",
    "MODEL_LLAMA = \"llama3.2:1b\"\n",
    "\n",
    "print(\"Clients configured:\")\n",
    "print(f\"  Cloud: OpenAI {MODEL_OPENAI}\")\n",
    "print(f\"  Local: Ollama {MODEL_LLAMA}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb0f33a",
   "metadata": {},
   "source": [
    "## Step 2: Create Test Email\n",
    "\n",
    "A synthetic email for testing parallel classification.\n",
    "\n",
    "**What this does:**\n",
    "- Creates a test email with some ambiguity\n",
    "- This email is CC'd to a non-lawyer, which creates waiver risk\n",
    "- We want to see if both models agree on the classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423fc9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_email = {\n",
    "    \"id\": \"DOC001\",\n",
    "    \"content\": \"\"\"\n",
    "From: sarah.chen@acmecorp.com.au\n",
    "To: michael.wong@wongpartners.com.au\n",
    "CC: john.smith@acmecorp.com.au\n",
    "Date: 2024-03-15\n",
    "Subject: RE: BuildRight dispute - next steps\n",
    "\n",
    "Michael,\n",
    "\n",
    "Thanks for your advice on our potential liability exposure.\n",
    "\n",
    "I've copied in John Smith (our CFO) as he needs to understand \n",
    "the financial implications of your recommendations.\n",
    "\n",
    "Can you please confirm:\n",
    "1. Whether the $500k limitation clause is enforceable\n",
    "2. If we should make a settlement offer\n",
    "\n",
    "John - please treat this as confidential.\n",
    "\n",
    "Regards,\n",
    "Sarah Chen\n",
    "General Counsel\n",
    "ACME Corporation Pty Ltd\n",
    "\"\"\"\n",
    "}\n",
    "\n",
    "print(f\"Test email created: {test_email['id']}\")\n",
    "print(\"Note: This email has a CC to a non-lawyer (CFO) - potential waiver issue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49718cb4",
   "metadata": {},
   "source": [
    "## Step 3: Create Classification Functions for Each Model\n",
    "\n",
    "Define separate functions to call each model with the same prompt.\n",
    "\n",
    "**What this does:**\n",
    "- `classify_with_openai()` — sends the document to GPT-4.1-nano (cloud)\n",
    "- `classify_with_llama()` — sends the document to Llama 3.2 (local)\n",
    "- Both use identical prompts so results are comparable\n",
    "- Each model analyses independently - no knowledge of the other's answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ea6f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "PRIVILEGE_PROMPT = \"\"\"You are an Australian legal privilege expert.\n",
    "Apply Evidence Act 1995 (Cth) ss 118-119 and the dominant purpose test from Esso v Commissioner of Taxation (1999).\n",
    "\n",
    "Analyse this document for legal professional privilege:\n",
    "\n",
    "{content}\n",
    "\n",
    "Consider:\n",
    "1. Is a lawyer involved?\n",
    "2. Is legal advice being sought or given?\n",
    "3. Was it made in confidence?\n",
    "4. Has privilege been waived by disclosure to third parties?\n",
    "\n",
    "Respond in this format:\n",
    "CLASSIFICATION: [PRIVILEGED/NOT_PRIVILEGED/UNCERTAIN]\n",
    "CONFIDENCE_SCORE: [0-100]\n",
    "WAIVER_RISK: [None/Low/Medium/High]\n",
    "REASONING: [2-3 sentences explaining your decision]\n",
    "\"\"\"\n",
    "\n",
    "def classify_with_openai(document):\n",
    "    \"\"\"Classify using OpenAI GPT (cloud)\"\"\"\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=MODEL_OPENAI,\n",
    "        messages=[{\"role\": \"user\", \"content\": PRIVILEGE_PROMPT.format(content=document['content'])}]\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def classify_with_llama(document):\n",
    "    \"\"\"Classify using Llama 3.2 (local via Ollama)\"\"\"\n",
    "    response = ollama_client.chat.completions.create(\n",
    "        model=MODEL_LLAMA,\n",
    "        messages=[{\"role\": \"user\", \"content\": PRIVILEGE_PROMPT.format(content=document['content'])}]\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "print(\"Classification functions created:\")\n",
    "print(\"  - classify_with_openai()\")\n",
    "print(\"  - classify_with_llama()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce32e068",
   "metadata": {},
   "source": [
    "## Step 4: Run Models in Parallel\n",
    "\n",
    "Execute both models on the same document and compare results.\n",
    "\n",
    "**What this does:**\n",
    "- Sends the test email to both OpenAI and Llama simultaneously\n",
    "- Each model analyses independently with no knowledge of the other\n",
    "- Displays results side by side for comparison\n",
    "- This is the core of parallelization - multiple independent opinions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56365de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "\n",
    "def run_parallel_classification(document):\n",
    "    \"\"\"Run both models in parallel and return results\"\"\"\n",
    "    \n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        # Submit both tasks\n",
    "        future_openai = executor.submit(classify_with_openai, document)\n",
    "        future_llama = executor.submit(classify_with_llama, document)\n",
    "        \n",
    "        # Get results\n",
    "        result_openai = future_openai.result()\n",
    "        result_llama = future_llama.result()\n",
    "    \n",
    "    return {\n",
    "        \"openai\": result_openai,\n",
    "        \"llama\": result_llama\n",
    "    }\n",
    "\n",
    "# Run parallel classification\n",
    "print(\"Running parallel classification...\")\n",
    "results = run_parallel_classification(test_email)\n",
    "\n",
    "display(Markdown(\"### OpenAI GPT-4.1-nano Result:\\n\"))\n",
    "display(Markdown(results[\"openai\"]))\n",
    "\n",
    "display(Markdown(\"\\n---\\n\"))\n",
    "\n",
    "display(Markdown(\"### Llama 3.2 (Local) Result:\\n\"))\n",
    "display(Markdown(results[\"llama\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ef4867",
   "metadata": {},
   "source": [
    "## Step 5: Compare Results and Detect Disagreements\n",
    "\n",
    "Analyse the parallel results to determine consensus or flag disagreements.\n",
    "\n",
    "**What this does:**\n",
    "- Extracts the classification from each model's response\n",
    "- Compares the classifications\n",
    "- If models agree → high confidence in the result\n",
    "- If models disagree → flag for human review\n",
    "- This is the safety net that single-model approaches lack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814012d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_classification(result_text):\n",
    "    \"\"\"Extract classification from model response\"\"\"\n",
    "    for line in result_text.split('\\n'):\n",
    "        if line.startswith(\"CLASSIFICATION:\"):\n",
    "            return line.split(':')[1].strip()\n",
    "    return \"UNKNOWN\"\n",
    "\n",
    "def parse_confidence(result_text):\n",
    "    \"\"\"Extract confidence score from model response\"\"\"\n",
    "    for line in result_text.split('\\n'):\n",
    "        if line.startswith(\"CONFIDENCE_SCORE:\"):\n",
    "            return line.split(':')[1].strip()\n",
    "    return \"0\"\n",
    "\n",
    "def compare_results(results):\n",
    "    \"\"\"Compare model results and determine consensus\"\"\"\n",
    "    \n",
    "    openai_class = parse_classification(results[\"openai\"])\n",
    "    llama_class = parse_classification(results[\"llama\"])\n",
    "    \n",
    "    openai_conf = parse_confidence(results[\"openai\"])\n",
    "    llama_conf = parse_confidence(results[\"llama\"])\n",
    "    \n",
    "    # Determine consensus\n",
    "    if openai_class == llama_class:\n",
    "        consensus = True\n",
    "        final_classification = openai_class\n",
    "        escalation_required = \"No\"\n",
    "        escalation_reason = \"Models agree\"\n",
    "    else:\n",
    "        consensus = False\n",
    "        final_classification = \"UNCERTAIN\"\n",
    "        escalation_required = \"Yes\"\n",
    "        escalation_reason = f\"Model disagreement: OpenAI={openai_class}, Llama={llama_class}\"\n",
    "    \n",
    "    return {\n",
    "        \"openai_classification\": openai_class,\n",
    "        \"openai_confidence\": openai_conf,\n",
    "        \"llama_classification\": llama_class,\n",
    "        \"llama_confidence\": llama_conf,\n",
    "        \"consensus\": consensus,\n",
    "        \"final_classification\": final_classification,\n",
    "        \"escalation_required\": escalation_required,\n",
    "        \"escalation_reason\": escalation_reason\n",
    "    }\n",
    "\n",
    "# Compare the results\n",
    "comparison = compare_results(results)\n",
    "\n",
    "display(Markdown(f\"\"\"\n",
    "### Consensus Analysis\n",
    "\n",
    "| Model | Classification | Confidence |\n",
    "|-------|---------------|------------|\n",
    "| OpenAI GPT-4.1-nano | {comparison['openai_classification']} | {comparison['openai_confidence']}% |\n",
    "| Llama 3.2 (Local) | {comparison['llama_classification']} | {comparison['llama_confidence']}% |\n",
    "\n",
    "**Consensus reached:** {comparison['consensus']}\n",
    "\n",
    "**Final classification:** {comparison['final_classification']}\n",
    "\n",
    "**Escalation required:** {comparison['escalation_required']}\n",
    "\n",
    "**Reason:** {comparison['escalation_reason']}\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d989ca",
   "metadata": {},
   "source": [
    "## Step 6: Export to CSV for Senior Lawyer Review\n",
    "\n",
    "Create a CSV output showing both model results and consensus analysis.\n",
    "\n",
    "**What this does:**\n",
    "- Records both models' classifications and confidence scores\n",
    "- Shows whether consensus was reached\n",
    "- Flags disagreements for escalation\n",
    "- Includes blank columns for senior lawyer to make final decision\n",
    "- The human reviewer can see exactly why the document was escalated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20068fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Build the CSV row\n",
    "csv_row = {\n",
    "    \"doc_id\": test_email['id'],\n",
    "    \"model_a\": \"OpenAI GPT-4.1-nano\",\n",
    "    \"model_a_classification\": comparison['openai_classification'],\n",
    "    \"model_a_confidence\": comparison['openai_confidence'],\n",
    "    \"model_b\": \"Llama 3.2 (Local)\",\n",
    "    \"model_b_classification\": comparison['llama_classification'],\n",
    "    \"model_b_confidence\": comparison['llama_confidence'],\n",
    "    \"consensus_reached\": comparison['consensus'],\n",
    "    \"final_classification\": comparison['final_classification'],\n",
    "    \"escalation_required\": comparison['escalation_required'],\n",
    "    \"escalation_reason\": comparison['escalation_reason'],\n",
    "    # Blank columns for senior lawyer HITL review\n",
    "    \"reviewer_notes\": \"\",\n",
    "    \"reviewer_decision\": \"\",\n",
    "    \"reviewed_by\": \"\",\n",
    "    \"review_date\": \"\"\n",
    "}\n",
    "\n",
    "# Create DataFrame and export\n",
    "df = pd.DataFrame([csv_row])\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "csv_filename = f\"privilege_review_parallelization_{timestamp}.csv\"\n",
    "\n",
    "# Display preview\n",
    "display(Markdown(\"### CSV Preview for HITL Review\"))\n",
    "display(df[['doc_id', 'model_a_classification', 'model_b_classification', 'consensus_reached', 'escalation_required']])\n",
    "\n",
    "# Save to file\n",
    "df.to_csv(csv_filename, index=False)\n",
    "display(Markdown(f\"**Exported:** `{csv_filename}`\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7095d238",
   "metadata": {},
   "source": [
    "## Conclusion: Parallelization for LPP Classification\n",
    "\n",
    "### What We Built\n",
    "\n",
    "A multi-model consensus system that runs two independent models in parallel:\n",
    "```\n",
    "Document ──┬──→ OpenAI GPT-4.1-nano ──→ PRIVILEGED (85%)\n",
    "           │\n",
    "           └──→ Llama 3.2 (Local) ────→ UNCERTAIN (10%)\n",
    "                                              ↓\n",
    "                                   Disagreement detected\n",
    "                                              ↓\n",
    "                                   Escalate to human review\n",
    "```\n",
    "\n",
    "### Why Parallelization Works for Privilege\n",
    "\n",
    "- **Safety net:** One model's error is caught by the other\n",
    "- **Defensibility:** \"Two independent AI systems were consulted\"\n",
    "- **Appropriate caution:** Disagreements default to human review\n",
    "- **Cost balance:** Cloud model (accurate) + local model (free) = affordable redundancy\n",
    "\n",
    "### What We Observed\n",
    "\n",
    "| Model | Classification | Confidence | Notes |\n",
    "|-------|---------------|------------|-------|\n",
    "| OpenAI GPT-4.1-nano | PRIVILEGED | 85% | Focused on lawyer involvement and dominant purpose |\n",
    "| Llama 3.2 (Local) | UNCERTAIN | 10% | Questioned the lawyer-client relationship |\n",
    "\n",
    "The disagreement was legitimate - the email had a third party (CFO) CC'd, which creates genuine ambiguity about waiver.\n",
    "\n",
    "### Comparison to Other Patterns\n",
    "\n",
    "| Aspect | Prompt Chaining | Routing | Parallelization |\n",
    "|--------|-----------------|---------|-----------------|\n",
    "| Models used | 1 | 1 per type | Multiple |\n",
    "| Error detection | None | None | Disagreement flagged |\n",
    "| Cost | Low | Medium | Higher |\n",
    "| Best for | Clear-cut cases | Mixed document types | High-stakes decisions |\n",
    "\n",
    "### Limitations\n",
    "\n",
    "**Cost increases with more models**\n",
    "- Each additional model multiplies API costs\n",
    "- Balance accuracy needs against budget\n",
    "\n",
    "**Consensus isn't always correct**\n",
    "- Two models can agree on the wrong answer\n",
    "- Parallelization reduces but doesn't eliminate error\n",
    "\n",
    "**Different model strengths**\n",
    "- Llama 3.2:1b is small and less capable than GPT-4\n",
    "- Consider using similarly-capable models for fairer comparison\n",
    "\n",
    "### Next Notebook\n",
    "\n",
    "`04_orchestrator_worker.ipynb` - Dynamic task breakdown for complex documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905029c8",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
