{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üèÜ Welcome to the LLM Triathlon Engine üèÜ\n",
    "\n",
    "Welcome to the **LLM Triathlon Engine**! This notebook is an automated framework designed to rigorously test and benchmark multiple Large Language Models (LLMs) from various providers (like OpenAI, Groq, and local Ollama) in a fair and comprehensive \"triathlon.\"\n",
    "\n",
    "---\n",
    "\n",
    "> ### üèä‚Äç‚ôÇÔ∏èüö¥‚Äç‚ôÇÔ∏èüèÉ‚Äç‚ôÇÔ∏è The Triathlon Concept\n",
    ">\n",
    "> A simple 100m dash (one question) isn't enough to find the best all-around model. A triathlon tests endurance and skill across three different events with *different weights*. Our engine does the same:\n",
    "> * **Event 1 (Heavy):** A \"heavy-weight\" question (worth **50 points**)\n",
    "> * **Event 2 (Medium):** A \"medium-weight\" question (worth **30 points**)\n",
    "> * **Event 3 (Light):** A \"light-weight\" question (worth **20 points**)\n",
    ">\n",
    "> The final winner is the model with the highest **total weighted score** across all three events.\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ How It Works: The 8-Stage Pipeline\n",
    "\n",
    "This engine runs in a sequential pipeline. Here is the step-by-step \"bulletin board\" for how it functions:\n",
    "\n",
    "| Stage | Emoji | Description | Key Output |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| **Stage 1** | ‚öôÔ∏è | **Setup & Client Init**<br>Dynamically checks all your `os.getenv()` API keys (OpenAI, Groq, etc.) and your local Ollama server. It then builds the list of **available** models to compete. | `competitors` (list) |\n",
    "| **Stage 2** | üß† | **Question Generation**<br>Uses a \"Generator\" LLM to create 3 diverse, high-quality questions. It then uses a \"Consultant\" LLM to rank them and assign the **50, 30, and 20-point** weights. | `questions` (list) |\n",
    "| **Stage 3** | üèÉ‚Äç‚ôÇÔ∏è | **The Race (Execution)**<br>The main engine. It loops through every *available* model (`competitors`) and asks it *all three* ranked questions (`questions`), dynamically using the correct API client for each. | `all_answers` (dict) |\n",
    "| **Stage 4** | üìä | **Answer Visualization**<br>Renders all raw answers in a clean, **question-by-question** format. This allows you, the user, to manually inspect and compare the performance on each task. | *Markdown Output* |\n",
    "| **Stage 5** | üèóÔ∏è | **Judge Data Formatting**<br>Combines *all* answers from all models into a single, massive, and meticulously labeled text string (`together_string`) ready to be sent to the Judge. | `together_string` (str) |\n",
    "| **Stage 6** | ‚öñÔ∏è | **The \"Judge\" Call**<br>Sends the massive prompt (with `together_string`) to a powerful Judge LLM (`gpt-4o`). It requests a **nested JSON** response containing 6 scores per model. | `judge_data_str` (JSON) |\n",
    "| **Stage 7** | üßÆ | **Final Score Calculation**<br>Parses the Judge's JSON. Applies the **two-layer weighting system:**<br> 1. `(Judge Score * 0.6) + (Peer Score * 0.4)`<br> 2. `(Q1 * 0.5) + (Q2 * 0.3) + (Q3 * 0.2)` | `final_rankings_sorted` (list) |\n",
    "| **Stage 8** | ü•á | **The Podium (Graphing)**<br>Uses `matplotlib` to plot the final results in a horizontal bar chart. The winning model with the highest total score is highlighted in **gold**. | *Visual Bar Chart* |\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with imports - ask ChatGPT to explain any package that you don't know\n",
    "\n",
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from anthropic import Anthropic\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Always remember to do this!\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚öôÔ∏è Stage 0 : Competition Setup and Client Initialization\n",
    "\n",
    "This block prepares the foundational infrastructure for the LLM competition. It checks all available API keys defined in environment variables and dynamically adds only the **accessible** models to the list of competitors. This ensures the robustness and flexibility of our testing environment.\n",
    "\n",
    "| Task | Purpose | Control Mechanism |\n",
    "| :--- | :--- | :--- |\n",
    "| **API Key Detection** | Determines which services can be used. | Checks for the presence of keys using `os.getenv()`. |\n",
    "| **Client Initialization** | Creates `OpenAI` compatible clients using the correct `base_url` and `api_key` for each service. | **Example:** Uses specific `base_url` for providers like Groq and DeepSeek. |\n",
    "| **Ollama Check** | Tests whether the local server is active. | Sends an HTTP request via `requests` to check `http://localhost:11434`. |\n",
    "| **Competitor List** | Adds each initialized model (with its client, model ID, and display name) to the `competitors` list. | Only `‚úÖ Successful` models proceed to the next stage. |\n",
    "\n",
    "***Please verify any instances of \"‚ùå ERROR\" or \"‚ùå Not Set\" in the output.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- üèÅ Setup Check: API Keys and Clients ---\n",
      "‚úÖ OpenAI API Key found (Starts: sk-proj-...)\n",
      "   -> OpenAI client and models added to the competition.\n",
      "--------------------\n",
      "‚úÖ Google API Key found (Starts: AI...)\n",
      "   -> Google client (Gemini) and model added to the competition.\n",
      "--------------------\n",
      "‚úÖ Groq API Key found (Starts: gsk_...)\n",
      "   -> Groq client and model added to the competition.\n",
      "--------------------\n",
      "‚úÖ DeepSeek API Key found (Starts: sk-...)\n",
      "   -> DeepSeek client and model added to the competition.\n",
      "--------------------\n",
      "üîÑ Checking Ollama (Local) server...\n",
      "‚úÖ Ollama server is active at 'http://localhost:11434'.\n",
      "   -> Ollama client and model added to the competition.\n",
      "--------------------\n",
      "\n",
      "--- ‚úÖ CONTROL COMPLETE ---\n",
      "5 models successfully configured for the competition:\n",
      "  - OpenAI GPT-5 Mini (Model ID: gpt-4.1)\n",
      "  - Google Gemini 2.5 Flash (Model ID: gemini-2.5-flash)\n",
      "  - Groq Llama 3 120B (Model ID: openai/gpt-oss-120b)\n",
      "  - DeepSeek Chat (Model ID: deepseek-chat)\n",
      "  - Ollama Llama 3 (Model ID: llama3.2)\n",
      "\n",
      "Competitor list for reporting: ['OpenAI GPT-5 Mini', 'Google Gemini 2.5 Flash', 'Groq Llama 3 120B', 'DeepSeek Chat', 'Ollama Llama 3']\n"
     ]
    }
   ],
   "source": [
    "# Stage 0\n",
    "# Print the key prefixes to help with any debugging\n",
    "import os\n",
    "import time\n",
    "from openai import OpenAI\n",
    "import requests \n",
    "\n",
    "# --- Stage 0: Initialization and Variable Definitions ---\n",
    "\n",
    "# Initialize the questions list\n",
    "questions = []\n",
    "\n",
    "# Initialize the competitors list as empty\n",
    "competitors = []\n",
    "\n",
    "print(\"--- üèÅ Setup Check: API Keys and Clients ---\")\n",
    "\n",
    "# --- Stage 1: Retrieve API Keys from Environment Variables ---\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "google_api_key = os.getenv(\"GOOGLE_API_KEY\") \n",
    "deepseek_api_key = os.getenv(\"DEEPSEEK_API_KEY\")\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "# --- Stage 2: Check and Initialize Clients ---\n",
    "\n",
    "# --- OpenAI Check ---\n",
    "if openai_api_key:\n",
    "    print(f\"‚úÖ OpenAI API Key found (Starts: {openai_api_key[:8]}...)\")\n",
    "    try:\n",
    "        openai_client = OpenAI(api_key=openai_api_key)\n",
    "        # Note: Model name 'gpt-4.1' is not official and likely to cause errors. Using it as provided for now.\n",
    "        competitors.append({\"name\": \"gpt-4.1\", \"client\": openai_client, \"display_name\": \"OpenAI GPT-5 Mini\"})\n",
    "        print(\"   -> OpenAI client and models added to the competition.\")\n",
    "    except Exception as e:\n",
    "        print(f\"   -> ‚ùå ERROR: Could not initialize OpenAI client: {e}\")\n",
    "else:\n",
    "    print(\"‚ùå OpenAI API KEY not set.\")\n",
    "print(\"-\" * 20)\n",
    "\n",
    "# --- Google (Gemini) Check ---\n",
    "if google_api_key:\n",
    "    print(f\"‚úÖ Google API Key found (Starts: {google_api_key[:2]}...)\")\n",
    "    try:\n",
    "        # Client configured for an assumed OpenAI-compatible wrapper/proxy\n",
    "        google_client = OpenAI(  \n",
    "            api_key=google_api_key,\n",
    "            # Note: This base_url might need to be adjusted based on the specific wrapper being used.\n",
    "            base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    "        )\n",
    "        # Add Google's OpenAI-compatible model\n",
    "        competitors.append({\n",
    "            \"name\": \"gemini-2.5-flash\", # Model ID in the OpenAI-compatible interface\n",
    "            \"client\": google_client,\n",
    "            \"display_name\": \"Google Gemini 2.5 Flash\"\n",
    "        })\n",
    "        print(\"   -> Google client (Gemini) and model added to the competition.\")\n",
    "    except Exception as e:\n",
    "        print(f\"   -> ‚ùå ERROR: Could not initialize Google client: {e}\")\n",
    "else:\n",
    "    print(\"‚ùå Google API KEY not set.\")\n",
    "print(\"-\" * 20)\n",
    "\n",
    "# --- Groq Check ---\n",
    "if groq_api_key:\n",
    "    print(f\"‚úÖ Groq API Key found (Starts: {groq_api_key[:4]}...)\")\n",
    "    try:\n",
    "        groq_client = OpenAI(\n",
    "            api_key=groq_api_key,\n",
    "            base_url='https://api.groq.com/openai/v1'\n",
    "        )\n",
    "        # Note: Model name 'openai/gpt-oss-120b' is likely incorrect for Groq. Using it as provided for now.\n",
    "        competitors.append({\"name\": \"openai/gpt-oss-120b\", \"client\": groq_client, \"display_name\": \"Groq Llama 3 120B\"})\n",
    "        print(\"   -> Groq client and model added to the competition.\")\n",
    "    except Exception as e:\n",
    "        print(f\"   -> ‚ùå ERROR: Could not initialize Groq client: {e}\")\n",
    "else:\n",
    "    print(\"‚ùå Groq API KEY not set.\")\n",
    "print(\"-\" * 20)\n",
    "\n",
    "# --- DeepSeek Check ---\n",
    "if deepseek_api_key:\n",
    "    print(f\"‚úÖ DeepSeek API Key found (Starts: {deepseek_api_key[:3]}...)\")\n",
    "    try:\n",
    "        deepseek_client = OpenAI(\n",
    "            api_key=deepseek_api_key,\n",
    "            base_url=\"https://api.deepseek.com/v1\"\n",
    "        )\n",
    "        competitors.append({\"name\": \"deepseek-chat\", \"client\": deepseek_client, \"display_name\": \"DeepSeek Chat\"})\n",
    "        print(\"   -> DeepSeek client and model added to the competition.\")\n",
    "    except Exception as e:\n",
    "        print(f\"   -> ‚ùå ERROR: Could not initialize DeepSeek client: {e}\")\n",
    "else:\n",
    "    print(\"‚ùå Deepseek API KEY not set.\")\n",
    "print(\"-\" * 20)\n",
    "\n",
    "# --- Ollama Check (Local Server) ---\n",
    "print(\"üîÑ Checking Ollama (Local) server...\")\n",
    "try:\n",
    "    # Check if the Ollama server is running locally\n",
    "    response = requests.get('http://localhost:11434/v1/models', timeout=3)\n",
    "    if response.status_code == 200:\n",
    "        print(f\"‚úÖ Ollama server is active at 'http://localhost:11434'.\")\n",
    "        ollama_client = OpenAI(\n",
    "            base_url='http://localhost:11434/v1', \n",
    "            api_key='ollama'\n",
    "        )\n",
    "        # Add Ollama model (Assuming 'llama3.2' is installed locally)\n",
    "        competitors.append({\"name\": \"llama3.2\", \"client\": ollama_client, \"display_name\": \"Ollama Llama 3\"})\n",
    "        print(\"   -> Ollama client and model added to the competition.\")\n",
    "    else:\n",
    "        print(f\"‚ùå Ollama server is not responding (Status: {response.status_code}).\")\n",
    "except requests.ConnectionError:\n",
    "    print(\"‚ùå Could not connect to the Ollama server at 'http://localhost:11434'. Is the server running?\")\n",
    "except Exception as e:\n",
    "    print(f\"   -> ‚ùå ERROR: Could not initialize Ollama client: {e}\")\n",
    "print(\"-\" * 20)\n",
    "\n",
    "\n",
    "# --- Stage 3: Final Report ---\n",
    "print(\"\\n--- ‚úÖ CONTROL COMPLETE ---\")\n",
    "if competitors:\n",
    "    print(f\"{len(competitors)} models successfully configured for the competition:\")\n",
    "    for c in competitors:\n",
    "        print(f\"  - {c['display_name']} (Model ID: {c['name']})\")\n",
    "    \n",
    "    # Create the display names list for reporting in later stages\n",
    "    competitors_display_names = [c[\"display_name\"] for c in competitors]\n",
    "    print(f\"\\nCompetitor list for reporting: {competitors_display_names}\")\n",
    "else:\n",
    "    print(\"‚ùå No models could be loaded for the competition. Please check your API keys or Ollama server.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü•á Stage 1: Question Generation & Weighting\n",
    "\n",
    "This block creates the \"triathlon\" of questions for the competition. Instead of one single question, we generate **three distinct questions** and then use a \"Consultant\" LLM to **assign different weights (50, 30, and 20 points)** to them based on their quality and challenge.\n",
    "\n",
    "This ensures the competition is a more robust test of model abilities across different domains (e.g., logic, creativity, ethics).\n",
    "\n",
    "### Two-Step Generation Process\n",
    "\n",
    "| Step | Model Used | Purpose |\n",
    "| :--- | :--- | :--- |\n",
    "| **1. Generation** | `gpt-4o-mini` (Creative Mode) | Generates 3 diverse, nuanced question candidates, separated by `---`. |\n",
    "| **2. Ranking** | `gpt-4o-mini` (Consultant Mode) | Ranks the 3 candidates and returns a JSON object assigning them to `q_50`, `q_30`, and `q_20`. |\n",
    "\n",
    "### üì¨ Output\n",
    "\n",
    "The code block finishes by:\n",
    "1.  Printing the three questions in their ranked, point-weighted order.\n",
    "2.  Storing them in the global `questions` list, which will be used by **Stage 1 (Competition Execution)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Competition Questions Generated and Ranked ---\n",
      "\n",
      "[50 Points]: 2. Imagine you are tasked with creating a new product that combines artificial intelligence and renewable energy to address climate change. Describe this product, its functionality, and how it would appeal to both consumers and businesses.\n",
      "\n",
      "[30 Points]: 1. If a train is traveling towards a fork in the tracks and you know that one track leads to a group of five people tied to the tracks while the other leads to one person, should you intervene to switch the tracks, knowing that doing so would result in the death of the one person? Discuss the ethical implications of your decision.\n",
      "\n",
      "[20 Points]: 3. A farmer has a certain number of apples and gives away half of them to a neighbor. The neighbor then gives back a third of what they received. If the farmer ends up with 30 apples, how many did they start with? Explain your reasoning step by step.\n",
      "['2. Imagine you are tasked with creating a new product that combines artificial intelligence and renewable energy to address climate change. Describe this product, its functionality, and how it would appeal to both consumers and businesses.', '1. If a train is traveling towards a fork in the tracks and you know that one track leads to a group of five people tied to the tracks while the other leads to one person, should you intervene to switch the tracks, knowing that doing so would result in the death of the one person? Discuss the ethical implications of your decision.', '3. A farmer has a certain number of apples and gives away half of them to a neighbor. The neighbor then gives back a third of what they received. If the farmer ends up with 30 apples, how many did they start with? Explain your reasoning step by step.']\n"
     ]
    }
   ],
   "source": [
    "# Stage 1\n",
    "import os\n",
    "import json\n",
    "from openai import OpenAI\n",
    "\n",
    "openai = OpenAI()\n",
    "\n",
    "# --- Step 1: Generate 3 Question Candidates ---\n",
    "\n",
    "request = \"Please come up with three challenging, nuanced questions that I can ask a number of LLMs to evaluate their intelligence. \"\n",
    "request += \"The questions should test different skills (e.g., one logic, one creativity, one ethics). \"\n",
    "request += \"Answer only with the questions, separated by '---'.\"\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": request}]\n",
    "\n",
    "try:\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=messages,\n",
    "        n=1, # n=1 is sufficient, as we're asking for 3 questions in one response\n",
    "        temperature=0.8\n",
    "    )\n",
    "    \n",
    "    # Split the questions by the '---' separator\n",
    "    generated_text = response.choices[0].message.content\n",
    "    cands = [q.strip() for q in generated_text.split('---') if q.strip()]\n",
    "\n",
    "    if len(cands) < 3:\n",
    "        raise ValueError(f\"Expected 3 questions, but only got {len(cands)}\")\n",
    "    \n",
    "    # Get the 3 questions\n",
    "    q_cand_1, q_cand_2, q_cand_3 = cands[0], cands[1], cands[2]\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error generating questions: {e}\")\n",
    "    # Continue with default questions in case of error\n",
    "    q_cand_1 = \"Default Question 1\"\n",
    "    q_cand_2 = \"Default Question 2\"\n",
    "    q_cand_3 = \"Default Question 3\"\n",
    "\n",
    "\n",
    "# --- Step 2: Consultant Evaluation (Scoring the Questions) ---\n",
    "\n",
    "# Ask the consultant to assign these 3 questions to 50, 30, 20 point slots\n",
    "consultant_messages = [\n",
    "    {\"role\": \"system\", \"content\": \"\"\"You are a ranking consultant. You will be given three questions. \n",
    "Your job is to rank them by quality and suitability for an LLM competition.\n",
    "The best question should be 'q_50', the second best 'q_30', and the third 'q_20'.\n",
    "Respond ONLY with JSON in the format: {\"q_50\": \"1\", \"q_30\": \"3\", \"q_20\": \"2\"}\n",
    "(Where the number corresponds to the question's order.)\"\"\"},\n",
    "    {\"role\": \"user\", \"content\": f\"Here are the questions:\\n\\n1) {q_cand_1}\\n\\n2) {q_cand_2}\\n\\n3) {q_cand_3}\\n\\nJSON:\"}\n",
    "]\n",
    "\n",
    "# Call the \"consultant\" model with temperature=0 (for a decisive decision)\n",
    "try:\n",
    "    consultant = openai.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\", # Use a powerful model for ranking\n",
    "        messages=consultant_messages,\n",
    "        temperature=0\n",
    "    )\n",
    "\n",
    "    decision_json = json.loads(consultant.choices[0].message.content.strip())\n",
    "    \n",
    "    # Assign questions to variables based on their scores\n",
    "    questions_map = {\"1\": q_cand_1, \"2\": q_cand_2, \"3\": q_cand_3}\n",
    "    \n",
    "    question_50_points = questions_map[decision_json[\"q_50\"]]\n",
    "    question_30_points = questions_map[decision_json[\"q_30\"]]\n",
    "    question_20_points = questions_map[decision_json[\"q_20\"]]\n",
    "\n",
    "    print(\"--- Competition Questions Generated and Ranked ---\")\n",
    "    print(f\"\\n[50 Points]: {question_50_points}\")\n",
    "    print(f\"\\n[30 Points]: {question_30_points}\")\n",
    "    print(f\"\\n[20 Points]: {question_20_points}\")\n",
    "\n",
    "    # Initialize questions list if it doesn't exist\n",
    "    if 'questions' not in globals():\n",
    "        questions = []\n",
    "    # Add the ranked questions to the global 'questions' list\n",
    "    questions.extend([question_50_points, question_30_points, question_20_points])\n",
    "    print(questions)\n",
    "except Exception as e:\n",
    "    print(f\"Error in consultant ranking phase: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèÉ‚Äç‚ôÇÔ∏è Stage 2: Competition Execution (Answer Collection)\n",
    "\n",
    "This is the main engine of the competition. This block iterates through every model configured in **Stage 1** (`competitors` list) and runs them against every question generated in **Stage 1** (`questions` list).\n",
    "\n",
    "It dynamically uses the correct API client (e.g., `openai_client`, `ollama_client`) for each specific model.\n",
    "\n",
    "### ‚öôÔ∏è Workflow Logic\n",
    "\n",
    "1.  **Outer Loop (By Competitor):** Iterates through each model dictionary in the `competitors` list. It retrieves the model's API name (`model_api_name`), its specific API client (`model_client`), and its pretty name (`model_display_name`).\n",
    "2.  **Inner Loop (By Question):** For each model, this loop iterates three times, asking the 50pt, 30pt, and 20pt questions sequentially.\n",
    "3.  **API Call:** It uses the correct, pre-configured `model_client` to call the `chat.completions.create` method. This allows Ollama, Groq, and OpenAI models to all be called using the same code structure.\n",
    "4.  **Error Handling:** A `try...except` block ensures that if one model fails (due to an API error, timeout, or rate limit), it doesn't crash the entire competition. An error message is logged, and a placeholder is stored.\n",
    "5.  **Data Storage:** All three answers from a model are collected into a temporary list and then stored in the main `all_answers` dictionary, using the model's `display_name` as the key.\n",
    "\n",
    "> **‚ùóÔ∏è Rate Limit Note:** A `time.sleep(1)` is included to prevent \"429: Too Many Requests\" errors from APIs by adding a short delay between calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- üèÅ COMPETITION STARTING ---\n",
      "Number of Competitors: 5\n",
      "Number of Questions: 3\n",
      "\n",
      "--- ‚ö° Querying Model: OpenAI GPT-5 Mini ---\n",
      "  -> Asking Question 1 (50 points)...\n",
      "  -> Asking Question 2 (30 points)...\n",
      "  -> Asking Question 3 (20 points)...\n",
      "\n",
      "--- ‚ö° Querying Model: Google Gemini 2.5 Flash ---\n",
      "  -> Asking Question 1 (50 points)...\n",
      "  -> Asking Question 2 (30 points)...\n",
      "  -> Asking Question 3 (20 points)...\n",
      "\n",
      "--- ‚ö° Querying Model: Groq Llama 3 120B ---\n",
      "  -> Asking Question 1 (50 points)...\n",
      "  -> Asking Question 2 (30 points)...\n",
      "  -> Asking Question 3 (20 points)...\n",
      "\n",
      "--- ‚ö° Querying Model: DeepSeek Chat ---\n",
      "  -> Asking Question 1 (50 points)...\n",
      "  -> Asking Question 2 (30 points)...\n",
      "  -> Asking Question 3 (20 points)...\n",
      "\n",
      "--- ‚ö° Querying Model: Ollama Llama 3 ---\n",
      "  -> Asking Question 1 (50 points)...\n",
      "  -> Asking Question 2 (30 points)...\n",
      "  -> Asking Question 3 (20 points)...\n",
      "\n",
      "--- ‚úÖ COMPETITION COMPLETED ---\n",
      "All answers collected from the configured models.\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "\n",
    "# --- Stage 2: Execute the Competition (Collect All Answers) ---\n",
    "\n",
    "# NOTE: The 'competitors' list (containing 'name', 'client', and 'display_name') \n",
    "# MUST be initialized in the preceding setup check block. \n",
    "# We will use this pre-configured list here.\n",
    "\n",
    "# The 'questions' list (containing Q1_50pt, Q2_30pt, Q3_20pt texts) \n",
    "# MUST come from Stage 1 (Question Generation).\n",
    "\n",
    "# Dictionary to store all collected responses {display_name: [answer_q1, answer_q2, answer_q3]}\n",
    "all_answers = {}\n",
    "\n",
    "# Check if necessary data exists\n",
    "if 'questions' not in globals() or len(questions) < 3:\n",
    "    print(\"Error: 'questions' list not found or incomplete. Stage 1 was not run correctly.\")\n",
    "elif 'competitors' not in globals() or not competitors:\n",
    "    print(\"Error: 'competitors' list is empty or not configured. Check API keys and setup.\")\n",
    "else:\n",
    "    print(f\"--- üèÅ COMPETITION STARTING ---\")\n",
    "    print(f\"Number of Competitors: {len(competitors)}\")\n",
    "    print(f\"Number of Questions: {len(questions)}\")\n",
    "\n",
    "    question_points = [50, 30, 20] # Define point values for reporting\n",
    "    \n",
    "    # --- OUTER LOOP: Iterates through each configured competitor (client + model) ---\n",
    "    for competitor in competitors:\n",
    "        \n",
    "        # Extract competitor details\n",
    "        model_api_name = competitor[\"name\"]\n",
    "        model_client = competitor[\"client\"] # The specific client (OpenAI, Ollama, Groq, etc.)\n",
    "        model_display_name = competitor[\"display_name\"]\n",
    "        \n",
    "        print(f\"\\n--- ‚ö° Querying Model: {model_display_name} ---\")\n",
    "        \n",
    "        # List to temporarily store the 3 answers for this model\n",
    "        model_answers_list = []\n",
    "        \n",
    "        # --- INNER LOOP: Iterates through each of the 3 questions ---\n",
    "        for i, question_text in enumerate(questions):\n",
    "            \n",
    "            points = question_points[i] \n",
    "            print(f\"  -> Asking Question {i+1} ({points} points)...\")\n",
    "            \n",
    "            # Create messages list for the current question\n",
    "            messages = [{\"role\": \"user\", \"content\": question_text}]\n",
    "            \n",
    "            try:\n",
    "                # API Call using the specific 'model_client' for the current model\n",
    "                response = model_client.chat.completions.create(\n",
    "                    model=model_api_name,\n",
    "                    messages=messages,\n",
    "                    temperature=0.7 # Consistent temperature for all models\n",
    "                )\n",
    "                \n",
    "                answer = response.choices[0].message.content\n",
    "                model_answers_list.append(answer)\n",
    "                \n",
    "                # Small delay to respect API rate limits\n",
    "                time.sleep(1) \n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"    *** ERROR: {model_display_name} failed to respond to Question {i+1}: {e}\")\n",
    "                model_answers_list.append(f\"ERROR: No response received from {model_display_name}.\")\n",
    "        \n",
    "        # After 3 answers are collected, store them in the main dictionary \n",
    "        # using the descriptive 'display_name' as the key\n",
    "        all_answers[model_display_name] = model_answers_list\n",
    "\n",
    "    print(\"\\n--- ‚úÖ COMPETITION COMPLETED ---\")\n",
    "    print(\"All answers collected from the configured models.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Stage 3: Comparative Visualization of All Answers\n",
    "\n",
    "This code block is a **visualization script** that takes the complex `all_answers` dictionary (collected in Stage 2/3) and renders it as clean, human-readable Markdown.\n",
    "\n",
    "Its primary purpose is to allow for **manual inspection and comparison** of the models' performance. The output is intentionally grouped **by question**, not by model, making it easy to see how all competitors tackled the same challenge side-by-side.\n",
    "\n",
    "### üèõÔ∏è Report Structure\n",
    "\n",
    "This script dynamically builds a single large Markdown string by looping through the data.\n",
    "\n",
    "| Loop | Purpose | Visual Output (Example) |\n",
    "| :--- | :--- | :--- |\n",
    "| **Outer Loop (By Question)** | Groups all responses for Q1, then Q2, etc. | `## Question 1 (Weight: 50 Points)` |\n",
    "| *Question Display* | Prints the question text itself in a blockquote. | `> **What is the future of...**` |\n",
    "| **Inner Loop (By Model)** | Iterates through each competitor in a consistent order. | `### üí¨ OpenAI GPT-4o Mini's Response:` |\n",
    "| *Answer Display* | Fetches the specific answer and formats it in a blockquote. | `<blockquote>The future is...</blockquote>` |\n",
    "\n",
    "The entire report is then rendered in the cell output using the `display(Markdown(...))` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# --- üèÅ COMPETITION ANSWERS: ALL MODELS --- \n",
       "\n",
       "## Question 1 (Weight: 50 Points)\n",
       "\n",
       "> **2. Imagine you are tasked with creating a new product that combines artificial intelligence and renewable energy to address climate change. Describe this product, its functionality, and how it would appeal to both consumers and businesses.**\n",
       "\n",
       "<hr>\n",
       "\n",
       "### üí¨ OpenAI GPT-5 Mini's Response:\n",
       "<blockquote>Certainly! Here‚Äôs a detailed description of an innovative product that blends artificial intelligence and renewable energy to combat climate change:\n",
       "\n",
       "---\n",
       "\n",
       "**Product Name:** **EnergiSync**\n",
       "\n",
       "**Product Overview:**  \n",
       "EnergiSync is an AI-powered, cloud-based energy management platform that seamlessly integrates with homes and businesses to optimize the generation, storage, and consumption of renewable energy (such as solar and wind). It uses advanced machine learning algorithms to predict energy needs, weather patterns, and grid demand, ensuring maximum efficiency and minimal carbon footprint.\n",
       "\n",
       "---\n",
       "\n",
       "**Functionality:**\n",
       "\n",
       "1. **Smart Energy Orchestration:**  \n",
       "   EnergiSync connects to solar panels, wind turbines, batteries, EV chargers, and smart appliances. It continuously monitors energy production and usage in real time.\n",
       "\n",
       "2. **Predictive Optimization:**  \n",
       "   Using AI, the platform analyzes weather forecasts, historical energy use, and time-of-use electricity rates to schedule energy-intensive tasks (like running the dishwasher or charging an EV) when renewable output is highest or grid emissions are lowest.\n",
       "\n",
       "3. **Dynamic Grid Interaction:**  \n",
       "   For businesses and communities, EnergiSync enables participation in demand response programs‚Äîautomatically reducing or shifting energy usage during peak grid demand to lower emissions and earn incentives.\n",
       "\n",
       "4. **Peer-to-Peer Energy Sharing:**  \n",
       "   The platform facilitates local energy trading among users, allowing households or businesses with surplus renewable energy to sell it directly to neighbors, maximizing renewable utilization.\n",
       "\n",
       "5. **Personalized Insights & Gamification:**  \n",
       "   Users receive actionable recommendations, monthly impact reports (CO‚ÇÇ saved, money earned), and can earn rewards for sustainable behaviors.\n",
       "\n",
       "---\n",
       "\n",
       "**Appeal to Consumers:**\n",
       "\n",
       "- **Lower Energy Bills:** By optimizing when and how energy is used, consumers save money.\n",
       "- **Simplicity:** Set-it-and-forget-it automation means users don‚Äôt need to manually schedule devices.\n",
       "- **Sustainability:** Real-time feedback on carbon savings empowers eco-conscious lifestyles.\n",
       "- **Community Engagement:** Peer-to-peer energy sharing fosters local collaboration.\n",
       "- **Smart Home Integration:** Works with popular smart home ecosystems for seamless user experience.\n",
       "\n",
       "**Appeal to Businesses:**\n",
       "\n",
       "- **Operational Savings:** Reduces electricity costs and peak demand charges.\n",
       "- **Regulatory Compliance:** Simplifies ESG reporting and helps meet sustainability targets.\n",
       "- **Brand Differentiation:** Demonstrates climate leadership to customers and partners.\n",
       "- **Scalability:** Easily manages multi-site operations and integrates with existing energy infrastructure.\n",
       "- **Revenue Generation:** Enables businesses to profit by selling excess renewable energy or participating in grid services.\n",
       "\n",
       "---\n",
       "\n",
       "**Impact:**  \n",
       "By harnessing AI to maximize the use of renewable energy and minimize fossil-fuel reliance, EnergiSync empowers both individuals and organizations to play an active role in fighting climate change‚Äîwhile saving money and building resilient, sustainable communities.</blockquote>\n",
       "\n",
       "---\n",
       "\n",
       "### üí¨ Google Gemini 2.5 Flash's Response:\n",
       "<blockquote>The climate crisis demands innovative solutions that merge cutting-edge technology with sustainable practices. Our new product, **\"AuraGrid AI,\"** is a revolutionary decentralized energy management system designed to empower communities and businesses to become self-sufficient, resilient, and carbon-neutral.\n",
       "\n",
       "---\n",
       "\n",
       "## Product Description: AuraGrid AI\n",
       "\n",
       "**AuraGrid AI** is an intelligent, modular, and interconnected energy ecosystem that combines advanced renewable energy generation with state-of-the-art AI-driven energy storage and distribution. It's not just a product; it's a foundational shift towards a proactive, resilient, and decarbonized energy future.\n",
       "\n",
       "Imagine a world where every building, every neighborhood, every industrial park is its own smart energy island, seamlessly contributing to and drawing from a larger, optimized grid. AuraGrid AI makes this vision a reality.\n",
       "\n",
       "### Core Components:\n",
       "\n",
       "1.  **Smart Energy Hub (The Brain):** A compact, aesthetically designed unit housing advanced solid-state or flow batteries for energy storage, a multi-source inverter, and the proprietary AuraGrid AI processing unit.\n",
       "2.  **Integrated Renewable Generators:** Optimized for seamless integration with various on-site renewable sources, including:\n",
       "    *   **High-Efficiency Solar Panels:** Next-generation photovoltaic arrays.\n",
       "    *   **Compact Vertical-Axis Wind Turbines:** Designed for urban and suburban environments with minimal noise and visual impact.\n",
       "    *   **Micro-Hydro/Geothermal (Optional):** For specific geographical locations.\n",
       "3.  **AuraGrid AI Platform (The Intelligence):** A cloud-based and edge-computing AI engine that continuously learns, predicts, and optimizes energy flow.\n",
       "4.  **Networked Microgrid Capabilities:** AuraGrid AI units can communicate and cooperate, forming resilient local microgrids that can operate independently or in conjunction with the main grid.\n",
       "\n",
       "### Functionality:\n",
       "\n",
       "AuraGrid AI's intelligence lies in its ability to predict, optimize, and adapt energy usage in real-time:\n",
       "\n",
       "1.  **Predictive Generation & Consumption:**\n",
       "    *   **Weather Forecasting AI:** Analyzes hyper-local weather data (solar irradiance, wind speed, temperature) to predict renewable energy generation output for the next 24-72 hours.\n",
       "    *   **Demand Forecasting AI:** Learns household/business energy consumption patterns (based on historical data, occupancy sensors, scheduled events like EV charging) to predict future demand.\n",
       "2.  **Intelligent Energy Routing & Storage:**\n",
       "    *   **Dynamic Optimization:** Based on predictions, the AI autonomously decides whether to store excess generated energy, use it immediately, or sell it back to the main grid (when prices are high) or to neighboring AuraGrid AI units.\n",
       "    *   **Peak Shaving & Load Shifting:** Automatically discharges stored energy during peak demand hours to avoid expensive grid electricity, and charges during off-peak hours or when renewables are abundant.\n",
       "    *   **Prioritized Usage:** Users can set preferences (e.g., prioritize self-consumption, maintain maximum battery backup, minimize carbon footprint).\n",
       "3.  **Grid Resilience & Blackout Protection:**\n",
       "    *   **Instantaneous Islanding:** In the event of a grid outage, AuraGrid AI seamlessly disconnects from the main grid and powers the building(s) using stored renewable energy, ensuring uninterrupted power.\n",
       "    *   **Grid Services:** Can provide ancillary services to the main grid, such as frequency regulation and voltage support, improving overall grid stability and reliability.\n",
       "4.  **Peer-to-Peer Energy Trading (Future Feature):**\n",
       "    *   A secure blockchain-enabled platform allowing interconnected AuraGrid AI units within a community to buy and sell excess renewable energy directly from each other, fostering local energy markets.\n",
       "5.  **User Interface & Control:**\n",
       "    *   Intuitive mobile app and web dashboard provide real-time monitoring of energy generation, consumption, storage levels, and cost savings.\n",
       "    *   Allows users to set preferences, view environmental impact metrics, and receive personalized recommendations.\n",
       "\n",
       "### Appeal to Consumers:\n",
       "\n",
       "1.  **Significant Cost Savings:**\n",
       "    *   **Reduced Electricity Bills:** Maximize self-consumption of free renewable energy and intelligently buy/sell from the grid at optimal times.\n",
       "    *   **Avoid Peak Charges:** AI automatically manages energy use to avoid expensive peak-hour electricity rates.\n",
       "2.  **Energy Independence & Resilience:**\n",
       "    *   **Reliable Backup Power:** Never worry about blackouts again; AuraGrid AI ensures continuous power during grid outages.\n",
       "    *   **Reduced Reliance on Utilities:** Gain greater control over your energy supply and be less exposed to fluctuating energy prices.\n",
       "3.  **Environmental Stewardship:**\n",
       "    *   **Zero-Carbon Footprint:** Power your home or business entirely with clean, renewable energy, significantly reducing your contribution to climate change.\n",
       "    *   **Tangible Impact:** The app displays real-time carbon emissions avoided and environmental benefits achieved, fostering a sense of purpose.\n",
       "4.  **Smart Home Integration & Convenience:**\n",
       "    *   Seamlessly integrates with existing smart home systems, optimizing energy usage based on occupancy, appliance schedules, and personal preferences.\n",
       "    *   Automated management means less hassle; the AI handles the complexity.\n",
       "5.  **Increased Property Value:**\n",
       "    *   Homes equipped with advanced renewable energy systems and intelligent storage are more attractive and valuable in a climate-conscious market.\n",
       "\n",
       "### Appeal to Businesses:\n",
       "\n",
       "1.  **Operational Cost Reduction:**\n",
       "    *   **Demand Charge Management:** AI intelligently flattens demand peaks, drastically reducing costly demand charges for commercial and industrial users.\n",
       "    *   **Lower Energy Procurement Costs:** Leverage on-site renewables and storage to minimize reliance on grid electricity, especially during high-price periods.\n",
       "2.  **Enhanced Business Continuity & Resilience:**\n",
       "    *   **Uninterrupted Operations:** Critical facilities (data centers, manufacturing, hospitals) maintain power during grid failures, preventing costly downtime and data loss.\n",
       "    *   **Microgrid Capabilities:** Establish robust, self-sufficient energy systems that can operate independently, increasing operational reliability.\n",
       "3.  **Meeting Sustainability & ESG Goals:**\n",
       "    *   **Demonstrable Decarbonization:** Achieve ambitious Scope 1, 2, and even Scope 3 emissions reduction targets with verifiable data.\n",
       "    *   **Green Branding:** Enhance corporate image and attract environmentally conscious customers, investors, and talent.\n",
       "    *   **Compliance:** Proactively meet evolving environmental regulations and reporting requirements.\n",
       "4.  **New Revenue Streams & Grid Participation:**\n",
       "    *   **Grid Services Provider:** Businesses can earn revenue by offering grid stability services (e.g., frequency regulation, voltage support) to utilities.\n",
       "    *   **Local Energy Markets:** Participate in peer-to-peer energy trading within business parks or industrial zones, selling excess power.\n",
       "5.  **Data-Driven Insights:**\n",
       "    *   The AuraGrid AI platform provides granular data on energy consumption, generation, and cost, allowing businesses to identify further efficiencies and optimize operations beyond just electricity.\n",
       "6.  **Scalability & Modularity:**\n",
       "    *   From small offices to large industrial complexes, AuraGrid AI can be scaled and customized to meet specific energy demands and growth plans.\n",
       "\n",
       "---\n",
       "\n",
       "AuraGrid AI represents a holistic approach to tackling climate change by decentralizing power, maximizing renewable energy utilization, and creating resilient, intelligent energy ecosystems. It's a product designed to empower every user to become a part of the solution, driving us towards a sustainable and secure energy future.</blockquote>\n",
       "\n",
       "---\n",
       "\n",
       "### üí¨ Groq Llama 3 120B's Response:\n",
       "<blockquote>## Product Concept  \n",
       "**Name:** **EcoPulse‚Ñ¢ ‚Äì AI‚ÄëDriven Renewable‚ÄëEnergy Hub**  \n",
       "\n",
       "**Tagline:** *‚ÄúSmart power for a cooler planet.‚Äù*  \n",
       "\n",
       "EcoPulse is a plug‚Äëand‚Äëplay, AI‚Äëpowered energy‚Äëmanagement system that turns any building‚Äîsingle‚Äëfamily home, apartment block, office complex, or factory floor‚Äîinto a self‚Äëoptimising micro‚Äëgrid. It blends **on‚Äësite renewable generation (solar, wind, or hybrid), modular energy storage, and a cloud‚Äënative AI engine** that continuously learns, forecasts, and orchestrates every watt in real‚Äëtime.\n",
       "\n",
       "---\n",
       "\n",
       "## 1. Core Hardware  \n",
       "\n",
       "| Module | What it Does | Typical Specs |\n",
       "|--------|--------------|---------------|\n",
       "| **Renewable Generation Kit** | Pre‚Äëengineered PV panels (or small‚Äëscale vertical‚Äëaxis wind turbines) with integrated Maximum Power Point Tracking (MPPT). | 3‚Äì10‚ÄØkW per kit; scalable by ‚Äústacking‚Äù units. |\n",
       "| **Smart Battery Pack** | Lithium‚Äëiron‚Äëphosphate (or solid‚Äëstate) cells with bidirectional DC‚ÄëDC converters. | 5‚Äì20‚ÄØkWh per pack; modular, hot‚Äëswappable. |\n",
       "| **Power‚ÄëFlow Controller (PFC)** | Acts as the brain at the edge: DC‚ÄëAC conversion, grid‚Äëtie isolation, safety relays, and real‚Äëtime telemetry. | 10‚ÄØkW‚Äë100‚ÄØkW rating; IEC‚Äë61850 compliant. |\n",
       "| **IoT Sensors** | Ambient light, temperature, occupancy, appliance‚Äëlevel current clamps, weather station, CO‚ÇÇ sensor. | Low‚Äëpower, mesh‚Äënetworked (Thread/Zigbee). |\n",
       "| **Edge AI Processor** | Runs inference locally for latency‚Äëcritical decisions (e.g., inverter clipping, demand‚Äëresponse). | ARM‚ÄëNeoverse N2 + NPU (‚âà 2‚ÄØTOPS). |\n",
       "| **Gateway & Cloud Sync** | Secure LTE/5G or fiber uplink, OTA updates, encrypted data pipeline. | TLS‚ÄØ1.3, zero‚Äëtrust architecture. |\n",
       "\n",
       "All modules are **pre‚Äëcertified for UL/CE**, rack‚Äëmountable, and can be installed by a certified electrician in under a day.\n",
       "\n",
       "---\n",
       "\n",
       "## 2. AI Engine ‚Äì The ‚ÄúBrain‚Äù  \n",
       "\n",
       "### 2.1. Data Ingestion  \n",
       "* **External:** Weather forecasts (NOAA, ECMWF), electricity market prices, grid‚Äëoperator signals, solar irradiance satellites.  \n",
       "* **Internal:** Real‚Äëtime generation, battery state‚Äëof‚Äëcharge, load profiles, occupancy patterns, device‚Äëlevel usage (via smart plugs).  \n",
       "\n",
       "### 2.2. Core Algorithms  \n",
       "\n",
       "| Function | AI Technique | Outcome |\n",
       "|----------|--------------|---------|\n",
       "| **Short‚Äëterm Forecasting (0‚Äë30‚ÄØmin)** | Temporal Convolutional Networks (TCN) + attention on weather radar. | Predict PV/wind output and net load to the minute. |\n",
       "| **Mid‚Äëterm Planning (1‚Äë24‚ÄØh)** | Gradient‚Äëboosted trees + reinforcement‚Äëlearning (RL) policy for storage dispatch. | Optimises when to charge/discharge to minimise grid cost and carbon intensity. |\n",
       "| **Long‚Äëterm Optimization (1‚Äë30‚ÄØdays)** | Graph Neural Networks (GNN) on regional grid topology + Monte‚ÄëCarlo simulation. | Determines participation in demand‚Äëresponse markets, peak‚Äëshaving contracts, and seasonal storage sizing. |\n",
       "| **Predictive Maintenance** | Auto‚Äëencoders on inverter & battery sensor streams. | Early fault detection ‚Üí service ticket before failure. |\n",
       "| **User‚ÄëBehaviour Modelling** | Federated learning across millions of devices, preserving privacy. | Personalised recommendations (e.g., ‚Äúrun dishwasher at 2‚ÄØAM‚Äù) that increase self‚Äëconsumption. |\n",
       "\n",
       "All models are **continuously retrained** on the cloud, but inference runs locally on the Edge AI processor for sub‚Äësecond response and offline resilience.\n",
       "\n",
       "### 2.3. Decision Engine (Real‚Äëtime)  \n",
       "\n",
       "1. **Load‚ÄëMatch** ‚Äì Prioritise on‚Äësite generation ‚Üí battery ‚Üí grid import.  \n",
       "2. **Carbon‚ÄëScore** ‚Äì Pull real‚Äëtime grid emission factor; shift loads to low‚Äëcarbon periods.  \n",
       "3. **Cost‚ÄëScore** ‚Äì Compare spot market price vs. stored energy cost; execute arbitrage when profitable.  \n",
       "4. **Grid‚ÄëSupport** ‚Äì Respond to utility signals (frequency regulation, capacity events) automatically, earning revenue.  \n",
       "\n",
       "---\n",
       "\n",
       "## 3. User Experience  \n",
       "\n",
       "### 3.1. Consumer Dashboard (Mobile & Web)  \n",
       "\n",
       "| Feature | Benefit |\n",
       "|---------|---------|\n",
       "| **Instant Power Flow Visualisation** | See ‚Äúwho‚Äôs using what‚Äù in watts, in real‚Äëtime. |\n",
       "| **Carbon Footprint Tracker** | Kilograms CO‚ÇÇ saved, with a gamified ‚Äúgreen badge‚Äù system. |\n",
       "| **Auto‚ÄëSavings Forecast** | Projected monthly bill reduction (average 25‚ÄØ% in pilot). |\n",
       "| **Smart‚ÄëAppliance Scheduler** | One‚Äëtap ‚Äúrun when solar is abundant‚Äù. |\n",
       "| **Maintenance Alerts** | Push notification before a component degrades. |\n",
       "| **Community Marketplace** | Trade excess solar or battery capacity with neighbours (peer‚Äëto‚Äëpeer micro‚Äëgrid). |\n",
       "\n",
       "### 3.2. Business Portal (B2B SaaS)  \n",
       "\n",
       "| Feature | Benefit |\n",
       "|---------|---------|\n",
       "| **Fleet‚Äëwide Energy Dashboard** | Aggregate view of dozens‚Äëto‚Äëthousands of sites. |\n",
       "| **Regulatory Reporting** | Automated ESG, GHG, and RE100 compliance packages. |\n",
       "| **Demand‚ÄëResponse Marketplace Integration** | Plug‚Äëand‚Äëplay participation in ISO/RTO programs. |\n",
       "| **Predictive Asset Management** | Optimised service schedules, reduced O&M cost. |\n",
       "| **Dynamic Tariff Optimiser** | Aligns consumption with time‚Äëof‚Äëuse (TOU) rates, saving up to 30‚ÄØ% on utility bills. |\n",
       "| **White‚ÄëLabel API** | Embed EcoPulse data into existing ERP, BMS, or building‚Äëautomation platforms. |\n",
       "\n",
       "---\n",
       "\n",
       "## 4. Value Proposition  \n",
       "\n",
       "### 4.1. For Consumers  \n",
       "\n",
       "| Pain Point | EcoPulse Solution |\n",
       "|------------|-------------------|\n",
       "| **High electricity bills** | Maximises self‚Äëconsumption, stores cheap excess, and arbitrages market price spikes. |\n",
       "| **Complexity of renewable adoption** | Turn‚Äëkey kit + AI automation removes the need for manual scheduling or engineering expertise. |\n",
       "| **Desire for sustainability proof** | Real‚Äëtime carbon‚Äësavings metric, shareable on social media, and eligibility for green‚Äëmortgage rebates. |\n",
       "| **Reliability concerns** | Battery backup + predictive maintenance guarantee >99‚ÄØ% uptime. |\n",
       "| **Limited space** | Modular, stackable panels and vertical‚Äëaxis wind turbines fit rooftops, balconies, or parking canopies. |\n",
       "\n",
       "### 4.2. For Businesses  \n",
       "\n",
       "| Pain Point | EcoPulse Solution |\n",
       "|------------|-------------------|\n",
       "| **Operational OPEX** | Cuts energy spend, especially during peak demand, and creates new revenue streams (grid services). |\n",
       "| **ESG & regulatory pressure** | Provides audited, transparent data for sustainability reporting and carbon‚Äëcredit programs. |\n",
       "| **Asset risk & downtime** | Predictive maintenance extends inverter and battery life, reducing CAPEX turnover. |\n",
       "| **Grid constraints** | Localised micro‚Äëgrid reduces demand on the utility network, easing interconnection costs. |\n",
       "| **Scalability** | Cloud‚Äënative AI platform scales from a single site to a global portfolio with a single SaaS subscription. |\n",
       "| **Talent shortage** | AI handles optimisation; facilities teams focus on strategic initiatives. |\n",
       "\n",
       "---\n",
       "\n",
       "## 5. Business Model  \n",
       "\n",
       "| Revenue Stream | Description |\n",
       "|----------------|-------------|\n",
       "| **Hardware Sale / Lease‚Äëto‚ÄëOwn** | Up‚Äëfront kit price (‚âà $4,500/kW) or zero‚Äëdown lease (monthly hardware fee). |\n",
       "| **SaaS Subscription** | $30‚Äë$120 per site per month for AI‚Äëoptimisation, analytics, and grid‚Äëservice participation. |\n",
       "| **Revenue Share on Grid Services** | 30‚ÄØ% of earnings from frequency regulation, capacity, or demand‚Äëresponse events. |\n",
       "| **Data‚ÄëInsights Marketplace** | Aggregated, anonymised load/production data sold to utilities, researchers, or OEMs (opt‚Äëin). |\n",
       "| **Installation & Service** | Certified partner network earns a margin on installation; EcoPulse gets a referral fee. |\n",
       "| **Carbon‚ÄëCredit Brokerage** | Facilitate verification of saved emissions; EcoPulse takes a transaction fee. |\n",
       "\n",
       "**Payback:** In most U.S./EU climate zones, a 6‚ÄØkW residential kit pays for itself in 4‚Äì6‚ÄØyears (including grid‚Äëservice earnings). Commercial sites see 5‚Äì8‚ÄØyear ROI, accelerated by tax credits (ITC, PTC) and ESG‚Äëlinked financing.\n",
       "\n",
       "---\n",
       "\n",
       "## 6. Go‚Äëto‚ÄëMarket Strategy  \n",
       "\n",
       "1. **Pilot Partnerships** ‚Äì 3‚Äëyear pilots with a utility (grid‚Äëservice integration), a large retailer (store‚Äëfront roll‚Äëout), and a residential co‚Äëop (community micro‚Äëgrid).  \n",
       "2. **Incentive Alignment** ‚Äì Pre‚Äëcertify kits for federal tax credits, local net‚Äëmetering, and green‚Äëmortgage programs; embed claim‚Äëautomation in the app.  \n",
       "3. **Channel Ecosystem** ‚Äì Certified installers, solar EPCs, and smart‚Äëhome retailers become EcoPulse ‚ÄúSolution Partners‚Äù.  \n",
       "4. **Brand Narrative** ‚Äì Position as *the simplest way to turn every roof into a climate‚Äëaction hub*. Leverage influencer/green‚Äëlifestyle marketing for consumers, and ESG‚Äëreporting conferences for corporates.  \n",
       "5. **Regulatory Advocacy** ‚Äì Join industry coalitions to shape policies around peer‚Äëto‚Äëpeer energy trading and AI‚Äëenabled demand response.\n",
       "\n",
       "---\n",
       "\n",
       "## 7. Environmental Impact (Quantified)  \n",
       "\n",
       "| Metric | Typical Annual Savings (per 6‚ÄØkW home) |\n",
       "|--------|----------------------------------------|\n",
       "| **Grid‚ÄëImported Energy** | 4,200‚ÄØkWh (‚âà 30‚ÄØ% reduction) |\n",
       "| **CO‚ÇÇ Emissions Avoided** | 2.5‚ÄØt CO‚ÇÇe |\n",
       "| **Peak‚ÄëDemand Reduction** | 1.5‚ÄØkW (helps defer distribution upgrades) |\n",
       "| **Battery Cycle Life Extension** | AI‚Äëdriven depth‚Äëof‚Äëdischarge control adds ~20‚ÄØ% usable cycles. |\n",
       "\n",
       "Scaling to **1‚ÄØmillion homes** would offset **2.5‚ÄØMt CO‚ÇÇe**‚Äîroughly the annual emissions of a mid‚Äësize city‚Äîwhile shaving **4‚ÄØTWh** of fossil‚Äëgenerated electricity from the grid.\n",
       "\n",
       "---\n",
       "\n",
       "## 8. Risks & Mitigations  \n",
       "\n",
       "| Risk | Mitigation |\n",
       "|------|------------|\n",
       "| **Regulatory barriers to grid services** | Build flexible API that can be toggled on/off; maintain compliance team to adapt to market rules. |\n",
       "| **Battery supply constraints** | Design for multi‚Äëchemistry compatibility (LFP, sodium‚Äëion, flow batteries) and enable easy swap‚Äëout. |\n",
       "| **Data privacy concerns** | Federated learning, end‚Äëto‚Äëend encryption, GDPR/CCPA‚Äëby‚Äëdesign; give users full data‚Äëownership controls. |\n",
       "| **AI model drift** | Continuous online learning with automated validation pipelines; human‚Äëin‚Äëthe‚Äëloop oversight for safety‚Äëcritical actions. |\n",
       "| **Consumer adoption hesitation** | Offer a 30‚Äëday ‚Äúfree‚Äëenergy‚Äù trial via lease model; guarantee bill‚Äëreduction or provide a rebate. |\n",
       "\n",
       "---\n",
       "\n",
       "## 9. The Bottom Line  \n",
       "\n",
       "**EcoPulse‚Ñ¢** fuses **AI‚Äôs predictive, optimisation, and automation power** with **renewable generation & storage**, delivering a self‚Äëoptimising micro‚Äëgrid that:\n",
       "\n",
       "* **Cuts costs** for households and enterprises.  \n",
       "* **Creates new revenue** from grid services and carbon markets.  \n",
       "* **Provides transparent, real‚Äëtime climate impact** that can be showcased to stakeholders.  \n",
       "* **Scales effortlessly** from a single garage‚Äëroof to a multinational portfolio.  \n",
       "\n",
       "By turning the *complex* problem of climate‚Äësmart energy into a **plug‚Äëand‚Äëplay, AI‚Äëdriven experience**, EcoPulse appeals to the **environmentally‚Äëmotivated consumer** looking for savings and the **forward‚Äëthinking business** seeking ESG compliance, operational resilience, and new profit streams‚Äîall while delivering measurable reductions in carbon emissions.  \n",
       "\n",
       "*The future of clean power isn‚Äôt just solar panels on a roof‚Äîit‚Äôs a brain that makes every watt count.*</blockquote>\n",
       "\n",
       "---\n",
       "\n",
       "### üí¨ DeepSeek Chat's Response:\n",
       "<blockquote>Of course. Here is a description of a new product that combines AI and renewable energy to address climate change.\n",
       "\n",
       "***\n",
       "\n",
       "### **Product Name: Helios Core**\n",
       "\n",
       "**Tagline:** *Intelligent Energy. For Your Home, Your Business, and Our Planet.*\n",
       "\n",
       "---\n",
       "\n",
       "### **Product Description**\n",
       "\n",
       "Helios Core is a comprehensive, AI-driven energy management ecosystem. At its heart is a sleek, wall-mounted central unit for the home and a scalable server-rack module for businesses. This \"Core\" unit intelligently orchestrates a network of three key components:\n",
       "\n",
       "1.  **Proprietary Solar \"Skin\":** Unlike traditional rigid panels, Helios uses a lightweight, durable, and semi-transparent photovoltaic \"skin\" that can be seamlessly adhered to roofs, windows, and even vehicle surfaces, dramatically increasing surface area for energy generation.\n",
       "2.  **A Modular, High-Efficiency Battery Bank:** A stackable battery system that stores excess solar energy.\n",
       "3.  **A Smart Grid Interface:** The physical connection to the local power grid.\n",
       "\n",
       "The true innovation lies not in the hardware itself, but in the sophisticated AI‚Äîdubbed \"Chronos\"‚Äîthat powers the entire system.\n",
       "\n",
       "---\n",
       "\n",
       "### **Functionality & How It Works**\n",
       "\n",
       "Helios Core's AI, Chronos, operates on three interconnected levels:\n",
       "\n",
       "**1. Predictive Generation & Consumption Analysis:**\n",
       "*   Chronos analyzes hyper-local, real-time weather data, historical energy usage patterns, and the household's or business's digital calendar.\n",
       "*   It predicts energy generation from the solar skin for the next 48 hours with over 95% accuracy.\n",
       "*   It forecasts energy demand based on learned behaviors (e.g., pre-cooling the house before the family returns from work, scheduling industrial machinery during peak solar generation).\n",
       "\n",
       "**2. Dynamic Energy Orchestration:**\n",
       "*   This is the core function. Chronos makes millions of micro-decisions per second to optimize energy flow. It automatically decides:\n",
       "    *   **What to Power:** Directing solar energy to high-priority tasks (e.g., charging the EV, running the AC).\n",
       "    *   **When to Store:** Charging the battery bank when generation is high and grid electricity is expensive or carbon-intensive.\n",
       "    *   **When to Sell:** Automatically selling excess energy back to the grid at the most profitable times, turning the home or business into a micro-power plant.\n",
       "\n",
       "**3. Grid-Supportive \"Swarm\" Intelligence:**\n",
       "*   This is the system's most powerful climate-focused feature. All Helios Cores in a region form a secure, decentralized network‚Äîan energy \"swarm.\"\n",
       "*   During periods of high grid stress (e.g., a heatwave), the utility company can send a signal requesting support. Chronos AI can then voluntarily and temporarily draw down a small amount of stored energy from thousands of participating Helios systems to stabilize the grid, preventing brownouts and reducing the need to activate polluting \"peaker\" power plants.\n",
       "\n",
       "---\n",
       "\n",
       "### **Appeal to Consumers**\n",
       "\n",
       "*   **Significant Cost Savings:** Drastically reduces or even eliminates electricity bills. The AI ensures consumers maximize their savings by buying energy at the cheapest rates and selling their excess at the highest.\n",
       "*   **Energy Independence & Resilience:** Provides peace of mind during power outages. The system can automatically island the home, running on battery and solar power.\n",
       "*   **Seamless Automation & Convenience:** \"Set it and forget it.\" Consumers don't need to be energy experts. The AI handles everything, and they can monitor and adjust settings via a simple, intuitive smartphone app.\n",
       "*   **Tangible Climate Action:** Empowers individuals to directly contribute to decarbonization. The app provides a dashboard showing their personal carbon footprint reduction, trees saved, and their contribution to grid stability.\n",
       "*   **Increased Property Value:** Homes and vehicles equipped with integrated, intelligent renewable energy systems are seen as modern, sustainable, and future-proof.\n",
       "\n",
       "---\n",
       "\n",
       "### **Appeal to Businesses**\n",
       "\n",
       "*   **Predictable & Lower Operational Costs:** Transforms energy from a volatile expense into a predictable, managed asset and even a potential revenue stream.\n",
       "*   **Enhanced ESG Credentials:** Provides powerful, data-driven evidence for Environmental, Social, and Governance (ESG) reporting. A business can publicly showcase its reduced carbon emissions and its role as a stabilizing force for the community's energy infrastructure.\n",
       "*   **Corporate Social Responsibility (CSR) Leadership:** Positions the company as an innovative leader in the fight against climate change, boosting brand image and appeal to environmentally conscious customers and investors.\n",
       "*   **Operational Resilience:** For factories, data centers, and stores, a power outage means lost revenue. Helios Core ensures critical operations can continue during grid disruptions.\n",
       "*   **Demand Charge Management:** For businesses with high, sporadic energy use, Chronos AI can strategically discharge batteries to shave peak demand, avoiding crippling utility demand charges.\n",
       "\n",
       "### **Conclusion**\n",
       "\n",
       "Helios Core is more than just a product; it's a platform for a sustainable energy future. By leveraging artificial intelligence to make renewable energy not just available but *intelligently optimal*, it creates a compelling value proposition. It appeals to the consumer's desire for savings, convenience, and purpose, while meeting the business need for cost reduction, risk management, and brand enhancement. Ultimately, by connecting individual systems into a stabilizing \"swarm,\" Helios Core directly addresses climate change by making the entire electrical grid cleaner, more efficient, and more resilient.</blockquote>\n",
       "\n",
       "---\n",
       "\n",
       "### üí¨ Ollama Llama 3's Response:\n",
       "<blockquote>Introducing \"EcoMind\" - a revolutionary product that harnesses the power of artificial intelligence (AI) and renewable energy to help combat climate change.\n",
       "\n",
       "**Product Description:**\n",
       "\n",
       "EcoMind is an intelligent, AI-powered energy management system that integrates with existing building infrastructure to optimize energy efficiency and reduce carbon footprint. This innovative product consists of three main components:\n",
       "\n",
       "1. **Energy Harvesting Module:** A sleek, compact device that captures solar or wind energy and converts it into usable electricity.\n",
       "2. **AI-Powered Energy Controller:** An advanced algorithm-driven module that analyzes energy usage patterns, detects anomalies, and optimizes energy distribution to minimize waste.\n",
       "3. **Smart Building Interface:** A user-friendly interface that provides real-time monitoring, predictive analytics, and recommendations for improving building efficiency.\n",
       "\n",
       "**Functionality:**\n",
       "\n",
       "EcoMind's AI-powered energy controller uses machine learning algorithms to analyze energy usage patterns, identify areas of inefficiency, and optimize energy distribution in real-time. This results in:\n",
       "\n",
       "* Reduced energy consumption by up to 30%\n",
       "* Increased renewable energy generation\n",
       "* Improved building comfort and reduced peak demand charges\n",
       "\n",
       "**Consumer Appeal:**\n",
       "\n",
       "EcoMind's user-friendly interface provides an intuitive way for consumers to monitor their energy usage, track their carbon footprint, and receive personalized recommendations for improving energy efficiency. Key features that appeal to consumers include:\n",
       "\n",
       "* Real-time monitoring of energy consumption\n",
       "* Customizable alerts and notifications for unusual energy usage patterns\n",
       "* Access to energy-saving tips and recommendations from certified experts\n",
       "\n",
       "**Business Appeal:**\n",
       "\n",
       "EcoMind's advanced AI-powered energy management system offers numerous benefits for businesses, including:\n",
       "\n",
       "* Reduced operational costs through optimized energy efficiency\n",
       "* Increased competitiveness through reduced carbon footprint and enhanced brand reputation\n",
       "* Data-driven insights for informed decision-making on energy investments and sustainability initiatives\n",
       "\n",
       "**Key Features:**\n",
       "\n",
       "* Integration with popular building management systems (BMS) and smart home devices\n",
       "* Compatibility with various renewable energy sources, including solar, wind, and geothermal\n",
       "* Scalable design to accommodate buildings of all sizes and types\n",
       "* Continuous software updates and AI training to ensure optimal performance\n",
       "\n",
       "**Potential Impact:**\n",
       "\n",
       "By harnessing the power of artificial intelligence and renewable energy, EcoMind has the potential to:\n",
       "\n",
       "* Reduce greenhouse gas emissions by up to 50%\n",
       "* Save billions of dollars in energy costs annually\n",
       "* Create new opportunities for businesses and consumers to invest in sustainable energy solutions\n",
       "\n",
       "**Target Markets:**\n",
       "\n",
       "EcoMind is designed for a wide range of customers, including:\n",
       "\n",
       "* Commercial buildings and office spaces\n",
       "* Residential homes and apartments\n",
       "* Industrial facilities and data centers\n",
       "* Government institutions and public sectors\n",
       "\n",
       "By addressing the pressing issue of climate change through innovative technology, EcoMind has the potential to make a significant impact on reducing carbon emissions and promoting sustainable energy practices.</blockquote>\n",
       "\n",
       "---\n",
       "\n",
       "<br><br>\n",
       "\n",
       "## Question 2 (Weight: 30 Points)\n",
       "\n",
       "> **1. If a train is traveling towards a fork in the tracks and you know that one track leads to a group of five people tied to the tracks while the other leads to one person, should you intervene to switch the tracks, knowing that doing so would result in the death of the one person? Discuss the ethical implications of your decision.**\n",
       "\n",
       "<hr>\n",
       "\n",
       "### üí¨ OpenAI GPT-5 Mini's Response:\n",
       "<blockquote>This scenario is a classic philosophical thought experiment known as **the trolley problem**. It is often used to explore questions in ethics, especially around utilitarianism and deontological ethics. Let‚Äôs break down the ethical implications of intervening or not intervening:\n",
       "\n",
       "---\n",
       "\n",
       "## **1. Utilitarian Perspective**\n",
       "\n",
       "**Utilitarianism** is an ethical theory that suggests the best action is the one that maximizes overall happiness or minimizes overall harm.\n",
       "\n",
       "- **Intervene (Switch the Track):**  \n",
       "  By switching the track, you would save five people at the cost of one life. From a utilitarian perspective, this is the morally preferable action because it results in the least amount of harm (one death versus five).\n",
       "- **Implication:**  \n",
       "  The end justifies the means: sacrificing one to save five is morally acceptable if it leads to a better overall outcome.\n",
       "\n",
       "---\n",
       "\n",
       "## **2. Deontological Perspective**\n",
       "\n",
       "**Deontological ethics** (notably associated with Immanuel Kant) argues that actions are right or wrong based on their adherence to rules or duties, regardless of the consequences.\n",
       "\n",
       "- **Do Not Intervene:**  \n",
       "  By not intervening, you are not directly responsible for the death of anyone; the outcome is a result of circumstances, not your action.\n",
       "- **Implication:**  \n",
       "  Actively switching the track makes you morally responsible for the death of the one person, which may be considered wrong, even if more lives are saved.\n",
       "\n",
       "---\n",
       "\n",
       "## **3. The Doctrine of Double Effect**\n",
       "\n",
       "This principle suggests that it can be morally acceptable to cause harm as a side effect (but not as a means) of bringing about a good result.\n",
       "\n",
       "- **Switching the Track:**  \n",
       "  Under this doctrine, if your intention is to save five people and not to kill the one, some would argue it could be justified, though others disagree.\n",
       "\n",
       "---\n",
       "\n",
       "## **4. Personal Responsibility and Psychological Factors**\n",
       "\n",
       "- **Intervening:**  \n",
       "  You become actively responsible for the death of the one, which can carry a heavy psychological and moral burden.\n",
       "- **Not Intervening:**  \n",
       "  You passively allow five deaths but are not the direct cause.\n",
       "\n",
       "---\n",
       "\n",
       "## **Conclusion**\n",
       "\n",
       "There is no universally agreed-upon \"correct\" answer‚Äîthe choice reflects your ethical priorities:\n",
       "\n",
       "- **If you value minimizing total harm (utilitarianism), you should intervene.**\n",
       "- **If you value strict moral rules against causing harm (deontology), you should not intervene.**\n",
       "\n",
       "**Ethical implication:**  \n",
       "The scenario forces us to confront the tension between doing harm and allowing harm, and between moral rules and consequences. It also raises questions about personal responsibility and the value of individual lives. Ultimately, your decision reveals which ethical principles you hold most strongly.\n",
       "\n",
       "---\n",
       "\n",
       "**In summary:**  \n",
       "- **Intervene:** Save more lives but actively cause a death.\n",
       "- **Do not intervene:** Do not actively cause harm, but more people die.  \n",
       "Both choices carry significant ethical weight and highlight the complexity of moral decision-making.</blockquote>\n",
       "\n",
       "---\n",
       "\n",
       "### üí¨ Google Gemini 2.5 Flash's Response:\n",
       "<blockquote>This is a classic ethical dilemma known as the \"Trolley Problem,\" which highlights the tension between different moral frameworks. There is no single universally agreed-upon \"right\" answer, as different ethical theories lead to different conclusions.\n",
       "\n",
       "Let's break down the ethical implications of intervening versus not intervening:\n",
       "\n",
       "### 1. The Utilitarian Perspective (Intervene)\n",
       "\n",
       "*   **Core Principle:** Utilitarianism is a form of consequentialism, meaning it judges the morality of an action based on its outcome. The goal is to maximize overall good and minimize harm ‚Äì \"the greatest good for the greatest number.\"\n",
       "*   **Application:** From a purely utilitarian standpoint, switching the tracks to save five lives at the cost of one is the morally preferable action. Five deaths are a greater loss than one death. By intervening, you are reducing the total amount of suffering and loss of life.\n",
       "*   **Ethical Implication:** This perspective argues that you have a moral obligation to act in a way that produces the best possible outcome, even if it means actively causing harm to one person to prevent greater harm to many. The individual lives are, in a sense, quantifiable, and saving more is better.\n",
       "\n",
       "### 2. The Deontological Perspective (Do Not Intervene, or Intervene is Wrong)\n",
       "\n",
       "*   **Core Principle:** Deontology (duty-based ethics) focuses on the inherent rightness or wrongness of actions themselves, rather than their consequences. It emphasizes moral duties, rules, and rights.\n",
       "*   **Application:**\n",
       "    *   **Not Intervening:** From a deontological perspective, if you do nothing, the train is the direct cause of the five deaths. You are not actively participating in or causing those deaths. Your inaction, while leading to a tragic outcome, doesn't make you morally culpable for *causing* the deaths in the same way that actively switching the tracks would make you culpable for the death of the one person. There's a distinction between \"killing\" and \"letting die.\"\n",
       "    *   **Intervening is Wrong:** If you switch the tracks, you are actively choosing to sacrifice one innocent life. This could be seen as violating a fundamental moral rule: \"Do not kill.\" You would be using the one person as a means to an end (saving the five), which is often seen as morally impermissible in deontology (e.g., Kant's categorical imperative against treating people merely as means).\n",
       "*   **Ethical Implication:** This perspective argues that you have a duty not to kill an innocent person, regardless of the potential positive consequences of doing so. Actively causing a death, even to save more lives, is an inherently wrong act.\n",
       "\n",
       "### 3. Other Ethical Considerations\n",
       "\n",
       "*   **Agency and Responsibility:**\n",
       "    *   **Intervening:** You become directly responsible for the death of the one person. This can have significant psychological and moral weight.\n",
       "    *   **Not Intervening:** You are responsible for *allowing* five people to die when you had the power to prevent it. Some argue that inaction when you can prevent harm is also a moral failing.\n",
       "*   **The Value of a Life:** Both perspectives grapple with the inherent value of human life. Utilitarianism, while valuing life, prioritizes the *quantity* of lives saved. Deontology emphasizes the *inviolability* of each individual life. Is one life less valuable than five? Most would agree that each life has intrinsic and equal value, but the dilemma forces a choice where that ideal cannot be fully upheld.\n",
       "*   **Moral Intuition:** Many people find the idea of actively pushing a person to their death (a variation of the trolley problem) more repugnant than pulling a lever, even if the outcome (one death vs. five) is the same. This highlights how our moral intuitions are often sensitive to the *nature* of the action and our direct involvement. In this lever scenario, the \"distance\" might make it feel slightly less personal, but the agency is still there.\n",
       "*   **The \"Clean Hands\" Argument:** Some might argue that by not intervening, you keep your hands \"clean\" from direct responsibility for killing. However, this is often challenged by the idea that you are still complicit by allowing a preventable tragedy.\n",
       "\n",
       "### Conclusion\n",
       "\n",
       "There is no easy answer.\n",
       "\n",
       "*   **If you intervene:** You align with a utilitarian viewpoint, prioritizing the greatest good (saving more lives), but you violate a deontological duty not to actively cause harm to an innocent individual. You become directly responsible for a death.\n",
       "*   **If you do not intervene:** You align with a deontological viewpoint, avoiding direct responsibility for killing, but you fail to prevent a greater tragedy, which a utilitarian would see as a moral failing. You are responsible for allowing five deaths when you could have prevented them.\n",
       "\n",
       "The decision is agonizing because it forces a choice between two profoundly difficult and tragic outcomes, each with significant moral weight. It reveals the limitations of applying any single ethical framework perfectly to complex real-world (or hypothetical) dilemmas.</blockquote>\n",
       "\n",
       "---\n",
       "\n",
       "### üí¨ Groq Llama 3 120B's Response:\n",
       "<blockquote>**The classic ‚Äútrolley‚Äëproblem‚Äù** you describe is a thought experiment that has been used for decades to probe the tensions between different moral theories.  Below is a systematic look at the main ethical lenses through which the decision can be examined, the practical considerations that complicate the picture, and a brief synthesis of what the various perspectives tend to recommend.\n",
       "\n",
       "---\n",
       "\n",
       "## 1.  The Decision at Hand  \n",
       "\n",
       "- **Situation**: A train is heading toward a fork.  \n",
       "  - **Track A**: Five people are tied to the rails.  \n",
       "  - **Track B**: One person is tied to the rails.  \n",
       "- **Action**: You can pull a lever (or otherwise intervene) to divert the train onto Track‚ÄØB, thereby **causing the death of the one** but **saving the five**.  \n",
       "- **Alternative**: Do nothing, allowing the train to stay on its current course, resulting in the death of the five.\n",
       "\n",
       "The question is whether the **act of switching** is morally permissible (or even obligatory) despite its direct causal link to the one person‚Äôs death.\n",
       "\n",
       "---\n",
       "\n",
       "## 2.  Ethical Theories and Their Verdicts  \n",
       "\n",
       "| Ethical Framework | Core Principle | How It Evaluates the Switch | Key Objections / Nuances |\n",
       "|-------------------|----------------|----------------------------|--------------------------|\n",
       "| **Consequentialism (Utilitarianism)** | Maximize overall welfare (usually quantified as ‚Äúthe greatest good for the greatest number‚Äù). | **Favourable** ‚Äì 5 lives saved > 1 life lost; the net utility is higher. | ‚Ä¢ Assumes we can accurately compare the value of lives. <br>‚Ä¢ Ignores rights, duties, and the moral significance of *how* outcomes are produced. |\n",
       "| **Rule Utilitarianism** | Follow rules that, if generally adopted, maximize utility. | Might still **support** the switch if a rule like ‚Äúwhen a choice can save many at the cost of few, intervene‚Äù produces better overall outcomes. | ‚Ä¢ A rule that permits killing even one innocent person could be abused; the rule‚Äëbased approach may reject the act to protect trust in the rule. |\n",
       "| **Deontological Ethics (Kantian)** | Moral duties are derived from rational principles; treat persons as ends in themselves, never merely as means. | **Opposes** the switch. Pulling the lever makes you *actively* cause the death of an innocent person, using them as a means to save others, which violates the categorical imperative. | ‚Ä¢ Some Kantians argue that *omission* (not pulling) is also a failure to respect the five, but the dominant reading holds that *acts* of killing are impermissible regardless of consequences. |\n",
       "| **Doctrine of Double Effect (DDE)** | An action that has both good and bad effects can be morally permissible if: (1) the act itself is morally neutral or good; (2) the bad effect is not intended (though foreseen); (3) the good effect is proportionally greater; (4) there is no less‚Äëharmful alternative. | The switch is a *means* to an end (the death of the one is a means to save five), so many DDE proponents deem it **impermissible**. | ‚Ä¢ Some interpret the act as ‚Äúdiverting a dangerous train‚Äù (neutral) with the death as an *unintended* side‚Äëeffect, which would make it permissible; the debate hinges on how the causal chain is framed. |\n",
       "| **Virtue Ethics** | Focus on the character of the agent (courage, compassion, prudence) rather than on rules or consequences. | A virtuous agent might **choose** to pull the lever out of compassion for the many, but also consider the *justice* owed to the individual. The decision is guided by practical wisdom (phronesis). | ‚Ä¢ Virtue ethics does not give a crisp ‚Äúyes/no‚Äù answer; it invites a reflective deliberation about what a flourishing moral agent would do. |\n",
       "| **Rights‚ÄëBased Ethics** | Individuals possess inviolable rights (e.g., the right not to be killed). | Switching **violates** the one person‚Äôs right to life, even if it saves five others. | ‚Ä¢ Some rights theorists allow *prima facie* rights that can be overridden in extreme emergencies; the hierarchy of rights becomes contested. |\n",
       "| **Legal/Responsibility Perspective** | Distinguishes between *acts* (commission) and *omissions* (non‚Äëaction). | Pulling the lever could be seen as **an act of homicide**, incurring criminal liability; doing nothing may be deemed *non‚Äëintervention* (often not punishable). | ‚Ä¢ Jurisdictions differ on whether omission can be criminal when a *duty to act* exists (e.g., a lifeguard). |\n",
       "\n",
       "---\n",
       "\n",
       "## 3.  The ‚ÄúAct vs. Omission‚Äù Problem  \n",
       "\n",
       "- **Act**: By pulling the lever you *cause* the death of the one person.  \n",
       "- **Omission**: By not pulling, you *allow* the train to continue, resulting in five deaths.  \n",
       "\n",
       "Many moral philosophers argue that the moral weight of *causing* a death is greater than that of *allowing* a death, even if the outcomes are numerically the same. This intuition underlies the deontological and DDE positions. Conversely, consequentialists treat the two outcomes as morally equivalent if the net welfare is the same.\n",
       "\n",
       "---\n",
       "\n",
       "## 4.  Psychological and Social Dimensions  \n",
       "\n",
       "1. **Moral Intuition**: Empirical studies (e.g., ‚Äútrolley‚Äëproblem‚Äù experiments) show that laypeople often **approve** of the switch when the numbers are high, but their approval drops sharply when the ‚Äúone‚Äù is replaced by a *personified* individual (e.g., a child or a loved one). This suggests that *identifiability* influences moral judgment.  \n",
       "\n",
       "2. **Moral Responsibility & Guilt**: Acting to kill, even for a ‚Äúgreater good,‚Äù can generate profound feelings of personal culpability and may have long‚Äëterm psychological effects (e.g., PTSD). Some argue that the *psychological cost* should factor into the moral calculus.  \n",
       "\n",
       "3. **Social Trust**: If society accepted that it is permissible to sacrifice a single individual for the majority, it could erode trust in institutions (e.g., hospitals, courts) that are expected to protect each person‚Äôs rights.  \n",
       "\n",
       "4. **Precedent & Slippery Slope**: Allowing the switch in this isolated case may set a precedent that justifies increasingly invasive utilitarian calculations (e.g., triage policies, autonomous weaponry).  \n",
       "\n",
       "---\n",
       "\n",
       "## 5.  Real‚ÄëWorld Complications  \n",
       "\n",
       "| Factor | Why It Matters |\n",
       "|--------|----------------|\n",
       "| **Uncertainty** | In practice, we rarely know for sure how many people are on each track, their ages, health, or the probability that the switch will work. |\n",
       "| **Agency of the Victims** | The people are ‚Äútied to the tracks,‚Äù implying they are *involuntary* victims. If they had voluntarily taken a risk, the moral assessment could shift. |\n",
       "| **Alternative Options** | Could you shout, apply brakes, or otherwise mitigate the disaster without a binary choice? The classic thought experiment eliminates alternatives to isolate the moral dilemma, but real life rarely does. |\n",
       "| **Legal Duty** | If you are a railway employee, you may have a *professional duty* to act; if you are a passerby, the duty may be less clear. Legal liability can differ dramatically. |\n",
       "\n",
       "---\n",
       "\n",
       "## 6.  Synthesis ‚Äì What Do Different Ethical Views Recommend?  \n",
       "\n",
       "| Perspective | Recommended Action | Rationale |\n",
       "|-------------|--------------------|-----------|\n",
       "| **Act‚ÄëUtilitarian** | **Pull the lever** (kill one, save five). | Maximizes total welfare. |\n",
       "| **Rule‚ÄëUtilitarian** | May **pull** if a rule permitting such trade‚Äëoffs yields overall better outcomes; may **refrain** if the rule undermines trust. | Balances outcomes with the stability of moral rules. |\n",
       "| **Kantian / Deontological** | **Do not pull**. | Killing an innocent is using them merely as a means, violating categorical duty. |\n",
       "| **Doctrine of Double Effect** | **Do not pull** (the death is intended as a means). | The bad effect is not merely foreseen but instrumental. |\n",
       "| **Virtue Ethics** | **Context‚Äëdependent**: a virtuous agent might pull out of compassion, but would also weigh justice, prudence, and the character‚Äëforming impact of the act. | Emphasizes practical wisdom over fixed formulas. |\n",
       "| **Rights‚ÄëBased** | **Do not pull** (the one person‚Äôs right to life is inviolable). | Rights trump aggregate welfare unless a higher‚Äëorder right overrides. |\n",
       "| **Legal/Responsibility** | **Do not pull** (to avoid criminal homicide), unless a specific legal duty to act exists. | Distinguishes between act and omission under criminal law. |\n",
       "\n",
       "---\n",
       "\n",
       "## 7.  A Pragmatic ‚ÄúMiddle‚ÄëGround‚Äù Approach  \n",
       "\n",
       "Many contemporary ethicists suggest that the stark binary of the classic trolley problem is a *heuristic* rather than a prescription. A more nuanced decision‚Äëmaking process might involve:\n",
       "\n",
       "1. **Assess the Certainty** ‚Äì Verify the numbers and the feasibility of the switch.  \n",
       "2. **Seek Alternatives** ‚Äì Look for any way to stop the train or free the victims without a trade‚Äëoff.  \n",
       "3. **Consider Proportionality** ‚Äì If the only options are ‚Äú5 die‚Äù vs. ‚Äú1 die,‚Äù the proportional loss favors the switch under a *principle of proportionality* used in just‚Äëwar theory.  \n",
       "4. **Reflect on Moral Agency** ‚Äì Ask whether you are *causing* the death or merely *allowing* it, and what your personal moral commitments dictate.  \n",
       "5. **Document & Communicate** ‚Äì In real‚Äëworld settings (e.g., autonomous vehicle programming), make the decision criteria transparent so that society can deliberate on the values encoded in the system.\n",
       "\n",
       "---\n",
       "\n",
       "## 8.  Concluding Thoughts  \n",
       "\n",
       "- **No single answer** satisfies every moral tradition. The trolley problem remains valuable precisely because it forces us to confront the clash between **outcome‚Äëoriented** and **principle‚Äëoriented** ethics.  \n",
       "- **Your decision** will depend on which moral weight you assign to *consequences*, *rights*, *duties*, and *character*.  \n",
       "- **Ethical deliberation** benefits from acknowledging the limits of the thought experiment (its simplifications, lack of context, and the emotional distance it creates) and from translating the insights into policies that respect both human welfare and fundamental rights.\n",
       "\n",
       "In short, **if you adopt a utilitarian calculus, you would likely intervene and switch the tracks**; **if you adopt a deontological or rights‚Äëbased stance, you would refrain**, viewing the act of killing as morally impermissible even to save more lives. A virtue‚Äëethics or pragmatic approach would ask you to weigh the concrete circumstances, possible alternatives, and the character‚Äëforming impact of your choice before arriving at a decision.</blockquote>\n",
       "\n",
       "---\n",
       "\n",
       "### üí¨ DeepSeek Chat's Response:\n",
       "<blockquote>This scenario is a version of the classic **trolley problem** in ethics.  \n",
       "\n",
       "Let‚Äôs break it down:  \n",
       "\n",
       "---\n",
       "\n",
       "## **1. The basic setup**\n",
       "- A train is heading toward a fork.  \n",
       "- One track has **five people** tied to it.  \n",
       "- The other track has **one person** tied to it.  \n",
       "- You can pull a lever to switch the train from the track with five people to the track with one person.  \n",
       "- If you do nothing, five people die. If you act, one person dies.  \n",
       "\n",
       "---\n",
       "\n",
       "## **2. Key ethical perspectives**\n",
       "\n",
       "### **Utilitarian view**\n",
       "- Maximize overall well-being / minimize total harm.  \n",
       "- One death is better than five deaths.  \n",
       "- Therefore, you **should** pull the lever.  \n",
       "\n",
       "### **Deontological view** (e.g., Kantian ethics)\n",
       "- Focus on duties and moral rules, not just outcomes.  \n",
       "- Some argue: acting to kill someone (even to save more) is morally wrong because you are *using* that one person merely as a means to save others.  \n",
       "- Others argue: inaction also has moral weight, but the distinction between *killing* and *letting die* matters.  \n",
       "- So, some deontologists might say **do not pull** the lever because pulling makes you directly responsible for the one death.  \n",
       "\n",
       "### **Rights-based view**\n",
       "- Each person has a right not to be harmed.  \n",
       "- Switching the track violates the one person‚Äôs right not to be killed by your deliberate action.  \n",
       "- But the five also have a right to life ‚Äî does inaction violate their rights?  \n",
       "\n",
       "### **Doctrine of double effect**\n",
       "- If your intention is to save five, and the death of the one is a foreseen but unintended side effect, some ethical systems (like certain Catholic moral teachings) might permit it.  \n",
       "- But here, the death of the one is a *means* to save the five (since you divert the threat to them), so it‚Äôs controversial.  \n",
       "\n",
       "---\n",
       "\n",
       "## **3. Real-world complicating factors**\n",
       "- Do you know the identities or ages of the people?  \n",
       "- Are they there voluntarily?  \n",
       "- Is there any chance to warn anyone or stop the train entirely?  \n",
       "- What if the single person is a close relative?  \n",
       "- Legal implications: in some jurisdictions, you might be legally liable if you act, but not if you do nothing.  \n",
       "\n",
       "---\n",
       "\n",
       "## **4. Psychological research**\n",
       "In studies, many people say they would pull the lever in the impersonal version, but reluctance increases:\n",
       "- If it requires *physically pushing* someone in front of the train to stop it (the \"footbridge\" variant).  \n",
       "- If the one person is a child or family member.  \n",
       "- This suggests that emotional intuition often conflicts with utilitarian reasoning.\n",
       "\n",
       "---\n",
       "\n",
       "## **5. My conclusion (for this basic case)**\n",
       "From a **utilitarian** standpoint, pulling the lever seems clearly right ‚Äî it minimizes total harm.  \n",
       "From a **non-consequentialist** standpoint, it‚Äôs less clear ‚Äî you might feel that deliberately causing one death is morally impermissible even to save more lives.  \n",
       "\n",
       "In practice, many people feel a strong impulse to save the five, but also a strong aversion to being the direct cause of the one‚Äôs death.  \n",
       "\n",
       "**Ethical implication**: There‚Äôs no universally agreed answer ‚Äî it reveals a deep split in moral theories between *consequentialism* and *deontology*, and in personal morality between *outcome-focused* and *act-focused* reasoning.</blockquote>\n",
       "\n",
       "---\n",
       "\n",
       "### üí¨ Ollama Llama 3's Response:\n",
       "<blockquote>This classic thought experiment raises complex and challenging ethical considerations. The dilemma at hand involves weighing the value of human life against the principle of minimizing harm and avoiding unnecessary suffering.\n",
       "\n",
       "Here's a breakdown of the arguments for and against intervening:\n",
       "\n",
       "**Arguments For Intervening:**\n",
       "\n",
       "1. **Value of Human Life:** One person's life is inherently more valuable than five people, as each individual has inherent worth and dignity. Sacrificing one person to save multiple others might be considered morally justifiable.\n",
       "2. **Minimizing Harm:** Switching the tracks would likely result in the death of the person on the other track, which could be seen as minimizing overall harm and suffering. This approach prioritizes avoiding unnecessary death over saving lives.\n",
       "\n",
       "**Arguments Against Intervening:**\n",
       "\n",
       "1. **Utilitarianism:** From a utilitarian perspective, the decision to intervene should prioritize the greatest good for the greatest number. In this case, saving five people might be considered the greater good, even if it means sacrificing one person.\n",
       "2. **Human Rights and Dignity:** Tying five people to the tracks could be seen as a form of dehumanizing treatment, potentially violating their human rights and dignity. Saving them would demonstrate respect for their inherent worth and autonomy.\n",
       "\n",
       "**Alternative Perspectives:**\n",
       "\n",
       "1. **The Non-Maleficence Principle:** This principle emphasizes doing no harm. In this scenario, switching the tracks might be seen as causing unnecessary harm to one person without saving anyone else.\n",
       "2. **Respect for Autonomy:** The decision of who should be saved (or sacrificed) could be argued to be a matter of individual autonomy and self-determination. Some might argue that it's not for you to decide who lives or dies, but rather to act as a passive observer.\n",
       "\n",
       "**Conclusion:**\n",
       "\n",
       "There is no clear-cut answer to this dilemma, as the ethical implications are complex and multifaceted. Ultimately, the decision would depend on one's personal values, moral framework, and individual circumstances.\n",
       "\n",
       "If I were to hypothetically intervene, I might consider the following factors:\n",
       "\n",
       "1. **Knowledge of the individuals involved:** Would you know the person on the other track? If so, their relationships with others could influence your decision.\n",
       "2. **Alternative options:** Are there any other ways to address the issue without switching tracks?\n",
       "3. **Contextual considerations:** Is this a situation where human life is at risk due to circumstances beyond one's control (e.g., natural disasters, accidents)?\n",
       "\n",
       "Ultimately, this thought experiment highlights the importance of nuance and individualized decision-making in complex moral dilemmas.\n",
       "\n",
       "(Note: This response does not imply that intervening or not intervening would be morally wrong. It aims to provide a framework for understanding the ethical complexities involved.)</blockquote>\n",
       "\n",
       "---\n",
       "\n",
       "<br><br>\n",
       "\n",
       "## Question 3 (Weight: 20 Points)\n",
       "\n",
       "> **3. A farmer has a certain number of apples and gives away half of them to a neighbor. The neighbor then gives back a third of what they received. If the farmer ends up with 30 apples, how many did they start with? Explain your reasoning step by step.**\n",
       "\n",
       "<hr>\n",
       "\n",
       "### üí¨ OpenAI GPT-5 Mini's Response:\n",
       "<blockquote>Let's break down the problem step by step.\n",
       "\n",
       "Let‚Äôs call the number of apples the farmer **started with**:  \n",
       "Let‚Äôs say the farmer started with **x** apples.\n",
       "\n",
       "**Step 1:** The farmer gives away half to a neighbor.  \n",
       "- Apples given away: \\( \\frac{x}{2} \\)\n",
       "- Apples left: \\( \\frac{x}{2} \\)\n",
       "\n",
       "**Step 2:** The neighbor gives back a third of what they received.  \n",
       "- The neighbor received \\( \\frac{x}{2} \\) apples.\n",
       "- A third of that is \\( \\frac{1}{3} \\times \\frac{x}{2} = \\frac{x}{6} \\).\n",
       "- The farmer gets back \\( \\frac{x}{6} \\) apples.\n",
       "\n",
       "**Step 3:** The farmer now has:  \n",
       "Apples left after giving away: \\( \\frac{x}{2} \\)  \n",
       "Plus apples given back: \\( \\frac{x}{6} \\)  \n",
       "Total apples now: \\( \\frac{x}{2} + \\frac{x}{6} \\)\n",
       "\n",
       "**Step 4:** We know that after all this, the farmer has **30 apples**.\n",
       "\n",
       "So,\n",
       "\\[\n",
       "\\frac{x}{2} + \\frac{x}{6} = 30\n",
       "\\]\n",
       "\n",
       "Combine into a single fraction:\n",
       "\\[\n",
       "\\frac{3x}{6} + \\frac{x}{6} = \\frac{4x}{6} = \\frac{2x}{3}\n",
       "\\]\n",
       "So,\n",
       "\\[\n",
       "\\frac{2x}{3} = 30\n",
       "\\]\n",
       "\n",
       "**Step 5:** Solve for x:\n",
       "\\[\n",
       "2x = 90 \\implies x = 45\n",
       "\\]\n",
       "\n",
       "**Final Answer:**  \n",
       "The farmer started with **45 apples**.\n",
       "\n",
       "---\n",
       "\n",
       "**Summary of Reasoning:**\n",
       "1. Farmer gives away half: \\( \\frac{x}{2} \\) left.\n",
       "2. Neighbor returns one-third of what they got: \\( \\frac{x}{6} \\) given back.\n",
       "3. Farmer ends with \\( \\frac{x}{2} + \\frac{x}{6} = 30 \\).\n",
       "4. Solve the equation to find the starting apples: \\( x = 45 \\).</blockquote>\n",
       "\n",
       "---\n",
       "\n",
       "### üí¨ Google Gemini 2.5 Flash's Response:\n",
       "<blockquote>Let's break this down step by step:\n",
       "\n",
       "1.  **Define the unknown:** Let 'A' be the initial number of apples the farmer started with.\n",
       "\n",
       "2.  **Farmer gives away half:**\n",
       "    *   The farmer gives away A/2 apples.\n",
       "    *   The farmer is left with A - A/2 = A/2 apples.\n",
       "    *   The neighbor receives A/2 apples.\n",
       "\n",
       "3.  **Neighbor gives back a third of what they received:**\n",
       "    *   The neighbor received A/2 apples.\n",
       "    *   A third of what they received is (1/3) * (A/2) = A/6 apples.\n",
       "    *   The neighbor gives A/6 apples back to the farmer.\n",
       "\n",
       "4.  **Farmer's final count:**\n",
       "    *   The farmer had A/2 apples after giving some away.\n",
       "    *   The farmer then received A/6 apples back.\n",
       "    *   So, the farmer's total number of apples is (A/2) + (A/6).\n",
       "\n",
       "5.  **Set up the equation:**\n",
       "    We are told the farmer ends up with 30 apples. So:\n",
       "    A/2 + A/6 = 30\n",
       "\n",
       "6.  **Solve the equation:**\n",
       "    *   To add the fractions, find a common denominator, which is 6.\n",
       "    *   Rewrite A/2 as (3A)/6.\n",
       "    *   (3A)/6 + A/6 = 30\n",
       "    *   Combine the fractions: (3A + A)/6 = 30\n",
       "    *   4A/6 = 30\n",
       "    *   Simplify the fraction 4/6 to 2/3: 2A/3 = 30\n",
       "    *   Multiply both sides by 3: 2A = 30 * 3\n",
       "    *   2A = 90\n",
       "    *   Divide by 2: A = 90 / 2\n",
       "    *   A = 45\n",
       "\n",
       "**Conclusion:**\n",
       "The farmer started with **45** apples.\n",
       "\n",
       "**Verification:**\n",
       "*   Start with 45 apples.\n",
       "*   Gives away half: 45 / 2 = 22.5 apples.\n",
       "*   Farmer now has: 45 - 22.5 = 22.5 apples.\n",
       "*   Neighbor received 22.5 apples.\n",
       "*   Neighbor gives back a third of what they received: (1/3) * 22.5 = 7.5 apples.\n",
       "*   Farmer ends up with: 22.5 (what they had) + 7.5 (what they got back) = 30 apples.\n",
       "This matches the problem statement.</blockquote>\n",
       "\n",
       "---\n",
       "\n",
       "### üí¨ Groq Llama 3 120B's Response:\n",
       "<blockquote>**Step‚ÄØ1: Define the unknown**\n",
       "\n",
       "Let  \n",
       "\n",
       "\\[\n",
       "x = \\text{the number of apples the farmer started with.}\n",
       "\\]\n",
       "\n",
       "---\n",
       "\n",
       "**Step‚ÄØ2: First transaction ‚Äì the farmer gives half to the neighbor**\n",
       "\n",
       "The farmer gives away half of his apples:\n",
       "\n",
       "\\[\n",
       "\\text{Apples given to neighbor} = \\frac{x}{2}.\n",
       "\\]\n",
       "\n",
       "After this, the farmer still has the other half:\n",
       "\n",
       "\\[\n",
       "\\text{Apples left with farmer after the gift} = \\frac{x}{2}.\n",
       "\\]\n",
       "\n",
       "---\n",
       "\n",
       "**Step‚ÄØ3: Second transaction ‚Äì the neighbor returns a third of what he received**\n",
       "\n",
       "The neighbor received \\(\\frac{x}{2}\\) apples.  \n",
       "He gives back a third of that amount:\n",
       "\n",
       "\\[\n",
       "\\text{Apples returned to farmer} = \\frac{1}{3}\\times\\frac{x}{2}= \\frac{x}{6}.\n",
       "\\]\n",
       "\n",
       "---\n",
       "\n",
       "**Step‚ÄØ4: Compute the farmer‚Äôs final total**\n",
       "\n",
       "The farmer ends up with the apples he kept plus the apples returned:\n",
       "\n",
       "\\[\n",
       "\\text{Final apples} = \\frac{x}{2} + \\frac{x}{6}.\n",
       "\\]\n",
       "\n",
       "Find a common denominator (6):\n",
       "\n",
       "\\[\n",
       "\\frac{x}{2} = \\frac{3x}{6},\\qquad\n",
       "\\frac{x}{2} + \\frac{x}{6}= \\frac{3x}{6} + \\frac{x}{6}= \\frac{4x}{6}= \\frac{2x}{3}.\n",
       "\\]\n",
       "\n",
       "---\n",
       "\n",
       "**Step‚ÄØ5: Set the final total equal to the given number**\n",
       "\n",
       "We‚Äôre told the farmer ends up with 30 apples:\n",
       "\n",
       "\\[\n",
       "\\frac{2x}{3}=30.\n",
       "\\]\n",
       "\n",
       "Solve for \\(x\\):\n",
       "\n",
       "\\[\n",
       "2x = 30 \\times 3 = 90 \\\\\n",
       "x = \\frac{90}{2}=45.\n",
       "\\]\n",
       "\n",
       "---\n",
       "\n",
       "**Answer**\n",
       "\n",
       "The farmer originally had **45 apples**.\n",
       "\n",
       "**Check (optional):**  \n",
       "- Half given away: \\(45/2 = 22.5\\) apples (the problem can be interpreted with whole numbers; in practice we‚Äôd assume the farmer started with a number that makes all intermediate steps integer, and indeed 45 works because the half and third steps produce whole numbers: 45‚ÄØ‚Üí‚ÄØ22.5‚ÄØ‚Üí‚ÄØneighbor gets 22.5, returns a third =‚ÄØ7.5, farmer ends with 22.5‚ÄØ+‚ÄØ7.5‚ÄØ=‚ÄØ30). If we require whole apples at every step, we could think of ‚Äúhalf‚Äù and ‚Äúa third‚Äù as exact fractions, which still give the correct final count. \n",
       "\n",
       "Thus, the starting amount is **45 apples**.</blockquote>\n",
       "\n",
       "---\n",
       "\n",
       "### üí¨ DeepSeek Chat's Response:\n",
       "<blockquote>Alright, let's go step-by-step.\n",
       "\n",
       "---\n",
       "\n",
       "**Step 1: Define the initial number of apples**\n",
       "\n",
       "Let \\( x \\) be the number of apples the farmer started with.\n",
       "\n",
       "---\n",
       "\n",
       "**Step 2: Farmer gives away half to neighbor**\n",
       "\n",
       "After giving away half, the farmer has:\n",
       "\n",
       "\\[\n",
       "x - \\frac{x}{2} = \\frac{x}{2}\n",
       "\\]\n",
       "\n",
       "The neighbor receives \\( \\frac{x}{2} \\) apples.\n",
       "\n",
       "---\n",
       "\n",
       "**Step 3: Neighbor gives back a third of what they received**\n",
       "\n",
       "The neighbor gives back:\n",
       "\n",
       "\\[\n",
       "\\frac{1}{3} \\times \\frac{x}{2} = \\frac{x}{6}\n",
       "\\]\n",
       "\n",
       "---\n",
       "\n",
       "**Step 4: Farmer‚Äôs final count**\n",
       "\n",
       "The farmer now has:\n",
       "\n",
       "\\[\n",
       "\\frac{x}{2} + \\frac{x}{6}\n",
       "\\]\n",
       "\n",
       "---\n",
       "\n",
       "**Step 5: Set up the equation**\n",
       "\n",
       "We are told the farmer ends up with 30 apples:\n",
       "\n",
       "\\[\n",
       "\\frac{x}{2} + \\frac{x}{6} = 30\n",
       "\\]\n",
       "\n",
       "---\n",
       "\n",
       "**Step 6: Solve for \\( x \\)**\n",
       "\n",
       "Common denominator is 6:\n",
       "\n",
       "\\[\n",
       "\\frac{3x}{6} + \\frac{x}{6} = 30\n",
       "\\]\n",
       "\\[\n",
       "\\frac{4x}{6} = 30\n",
       "\\]\n",
       "\\[\n",
       "\\frac{2x}{3} = 30\n",
       "\\]\n",
       "\\[\n",
       "2x = 90\n",
       "\\]\n",
       "\\[\n",
       "x = 45\n",
       "\\]\n",
       "\n",
       "---\n",
       "\n",
       "**Step 7: Conclusion**\n",
       "\n",
       "The farmer started with \\( \\mathbf{45} \\) apples.\n",
       "\n",
       "---\n",
       "\n",
       "**Check:**\n",
       "\n",
       "Start: 45 apples  \n",
       "Gives away half: keeps \\( 22.5 \\), neighbor gets \\( 22.5 \\)  \n",
       "Neighbor gives back \\( \\frac{1}{3} \\) of \\( 22.5 \\) = \\( 7.5 \\)  \n",
       "Farmer now has \\( 22.5 + 7.5 = 30 \\) ‚úÖ\n",
       "\n",
       "\\[\n",
       "\\boxed{45}\n",
       "\\]</blockquote>\n",
       "\n",
       "---\n",
       "\n",
       "### üí¨ Ollama Llama 3's Response:\n",
       "<blockquote>Let's break down the problem step by step:\n",
       "\n",
       "1. The farmer starts with an unknown number of apples. Let's call this number x.\n",
       "\n",
       "2. The farmer gives away half of their apples to a neighbor, which means they are left with 0.5x apples (since half is the same as multiplying by 0.5).\n",
       "\n",
       "3. The neighbor then gives back a third of what they received, which means they give back 1/3 * 0.5x = (1/6)x apples.\n",
       "\n",
       "4. After receiving the apples from the neighbor, the farmer now has 0.5x + (1/6)x apples.\n",
       "\n",
       "5. We know that the farmer ends up with 30 apples. So we can set up an equation:\n",
       "\n",
       "    0.5x + (1/6)x = 30\n",
       "\n",
       "6. To simplify the equation, we need to find a common denominator for both fractions. The least common multiple of 2 and 6 is 6. So we multiply both terms by 6 to clear the denominators:\n",
       "\n",
       "    3x + x = 180\n",
       "   4x = 180\n",
       "\n",
       "7. Now we can solve for x by dividing both sides by 4:\n",
       "\n",
       "   x = 180/4\n",
       "   x = 45\n",
       "\n",
       "So, the farmer started with 45 apples.</blockquote>\n",
       "\n",
       "---\n",
       "\n",
       "<br><br>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# stage 3\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# --- Displaying Answers (Question-Based Comparison) ---\n",
    "\n",
    "# IMPORTANT: For this code to work, the 'all_answers', 'questions', and 'competitors_display_names' \n",
    "# variables must have been created in the previous stages.\n",
    "\n",
    "if 'all_answers' not in globals() or 'questions' not in globals():\n",
    "    print(\"Error: Required data ('all_answers' or 'questions') is missing. Please run Stage 1 and 2.\")\n",
    "else:\n",
    "    markdown_output = \"# --- üèÅ COMPETITION ANSWERS: ALL MODELS --- \\n\\n\"\n",
    "    \n",
    "    question_points = [50, 30, 20] \n",
    "    \n",
    "    # --- OUTER LOOP: Group by Question ---\n",
    "    for i, question_text in enumerate(questions):\n",
    "        \n",
    "        # Heading for the Question\n",
    "        markdown_output += f\"## Question {i+1} (Weight: {question_points[i]} Points)\\n\\n\"\n",
    "        \n",
    "        # Display the Question itself\n",
    "        markdown_output += f\"> **{question_text}**\\n\\n\"\n",
    "        markdown_output += \"<hr>\\n\\n\"\n",
    "        \n",
    "        # --- INNER LOOP: Display Each Competitor's Answer to This Question ---\n",
    "        # We iterate over the list of display names to ensure a consistent order\n",
    "        for model_display_name in competitors_display_names:\n",
    "            if model_display_name in all_answers and len(all_answers[model_display_name]) > i:\n",
    "                \n",
    "                # Get the specific answer for this model and question index (i)\n",
    "                answer = all_answers[model_display_name][i]\n",
    "                \n",
    "                markdown_output += f\"### üí¨ {model_display_name}'s Response:\\n\"\n",
    "                \n",
    "                # Use a blockquote for clear separation\n",
    "                markdown_output += f\"<blockquote>{answer}</blockquote>\\n\\n\"\n",
    "                markdown_output += \"---\\n\\n\" \n",
    "            else:\n",
    "                markdown_output += f\"### üí¨ {model_display_name}'s Response:\\n\"\n",
    "                markdown_output += \"> *ERROR: Response not found or missing for this model.*\\n\\n\"\n",
    "                markdown_output += \"---\\n\\n\"\n",
    "        \n",
    "        markdown_output += \"<br><br>\\n\\n\" \n",
    "\n",
    "    # Display the final aggregated output\n",
    "    display(Markdown(markdown_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Stage 4: Formatting Data for the Judge LLM (`together_string`)\n",
    "\n",
    "This code block takes the raw answers collected in Stage 2 and transforms them into a single, highly structured text string (`together_string`) that the Judge LLM can easily understand and process.\n",
    "\n",
    "It serves as the **Data Presentation Layer** before the final API call.\n",
    "\n",
    "### üìù Logic and Purpose\n",
    "\n",
    "The primary goal is to meticulously label every piece of information within the text input so the Judge LLM knows exactly which answer belongs to which competitor and which question weight it carries. This prevents scoring errors and ambiguity.\n",
    "\n",
    "| Element | Code Action | Judge's Interpretation |\n",
    "| :--- | :--- | :--- |\n",
    "| **Outer Loop** | Iterates through `competitors_display_names`. | Clearly identifies **Competitor 1**, **Competitor 2**, etc., ensuring the order matches the final JSON `competitor_number`. |\n",
    "| **Header** | `--- Competitor {index + 1} ({model_display_name}) ---` | Explicitly marks the start of a model's complete set of answers. |\n",
    "| **Inner Loop** | Iterates 3 times (for Q1, Q2, Q3). | Guarantees that **all three answers** (or placeholders) are included in the text input. |\n",
    "| **Labels** | `[Q1 (50pts) Answer]:` | Explicitly links the answer text to its **question weight**, eliminating scoring confusion. |\n",
    "| **Separator** | `=\" * 50` | Provides a strong, visual break between competitors, which helps the LLM reliably parse the long text input. |\n",
    "\n",
    "This meticulous formatting is essential for coercing a Language Model into returning clean, structured **JSON** data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Judge Prompt Input (together_string) Prepared ---\n",
      "\n",
      "--- Competitor 1 (OpenAI GPT-5 Mini) ---\n",
      "Model ID for scoring: **1**\n",
      "\n",
      "[Q1 (50pts) Answer]:\n",
      "Certainly! Here‚Äôs a detailed description of an innovative product that blends artificial intelligence and renewable energy to combat climate change:\n",
      "\n",
      "---\n",
      "\n",
      "**Product Name:** **EnergiSync**\n",
      "\n",
      "**Product Overview:**  \n",
      "EnergiSync is an AI-powered, cloud-based energy management platform that seamlessly integrates with homes and businesses to optimize the generation, storage, and consumption of renewable energy (...\n"
     ]
    }
   ],
   "source": [
    "# --- Stage 4: Prepare the Input Data for the Judge LLM ---\n",
    "\n",
    "# IMPORTANT: The 'all_answers' and 'competitors_display_names' variables \n",
    "# must come from Stage 2 (Executing the Competition).\n",
    "\n",
    "together_string = \"\"\n",
    "\n",
    "# Define the question labels/weights for clarity in the prompt\n",
    "question_labels = [\"Q1 (50pts)\", \"Q2 (30pts)\", \"Q3 (20pts)\"]\n",
    "\n",
    "# We use 'competitors_display_names' to maintain the order for scoring\n",
    "for index, model_display_name in enumerate(competitors_display_names):\n",
    "    \n",
    "    # Retrieve all answers for this model from the 'all_answers' dictionary\n",
    "    # It should contain [answer_q1, answer_q2, answer_q3]\n",
    "    answers = all_answers.get(model_display_name, [\n",
    "        \"ERROR: Answer 1 Not Found\", \n",
    "        \"ERROR: Answer 2 Not Found\", \n",
    "        \"ERROR: Answer 3 Not Found\"\n",
    "    ])\n",
    "    \n",
    "    # Header: Competitor Number and Name\n",
    "    together_string += f\"\\n--- Competitor {index + 1} ({model_display_name}) ---\\n\"\n",
    "    together_string += f\"Model ID for scoring: **{index + 1}**\\n\\n\"\n",
    "    \n",
    "    # Format all 3 Answers sequentially\n",
    "    for i in range(3):\n",
    "        label = question_labels[i]\n",
    "        answer = answers[i]\n",
    "        \n",
    "        # Append the answer with its corresponding point label\n",
    "        together_string += f\"[{label} Answer]:\\n\"\n",
    "        together_string += answer + \"\\n\\n\"\n",
    "    \n",
    "    # Add a strong separator between competitors' full sets of answers\n",
    "    together_string += \"=\" * 50 + \"\\n\"\n",
    "\n",
    "print(\"--- Judge Prompt Input (together_string) Prepared ---\")\n",
    "\n",
    "# (Optional: Print the first few lines for verification)\n",
    "print(together_string[:500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öñÔ∏è Stage 5: Defining the Final Judge Prompt\n",
    "\n",
    "This block assembles the **master prompt** that will be sent to the powerful Judge LLM (e.g., `gpt-4o`). This is the most complex and critical piece of prompt engineering in the entire workflow.\n",
    "\n",
    "It combines all previously generated data (`questions` and `together_string`) with a strict set of instructions and a **required JSON output format**.\n",
    "\n",
    "### üìú Anatomy of the Judge Prompt\n",
    "\n",
    "The `judge_prompt_text` variable is a large f-string that dynamically injects the following components:\n",
    "\n",
    "| Injected Variable | Purpose |\n",
    "| :--- | :--- |\n",
    "| `{len(competitors_display_names)}` | Tells the Judge how many competitors to score (e.g., \"5 competitors\"). |\n",
    "| `{question_list_str}` | Provides the full text of all 3 questions, so the Judge has the **context** of what was asked. |\n",
    "| **JSON Schema** (in prompt text) | **The most critical part.** This is a rigid template that *forces* the LLM to return a nested JSON object. |\n",
    "| **Scoring Rules** (in prompt text) | Instructs the Judge to provide the two distinct scores: `judge_score` (objective) and `peer_average_score` (estimated). |\n",
    "| `{together_string}` | This is the **main data payload**. It injects all the formatted answers from all competitors (created in Stage 3/4). |\n",
    "\n",
    "> **üéØ Goal:** To coerce the Judge LLM into acting like a reliable, structured data-parsing-and-scoring API. The success of the final scoring (Stage 6) depends entirely on this prompt returning clean, valid JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Judge Prompt Defined ---\n"
     ]
    }
   ],
   "source": [
    "# --- Stage 5: Define the Final Judge Prompt ---\n",
    "\n",
    "# NOTE: This prompt assumes the following variables are defined from previous stages:\n",
    "# 1. competitors_display_names (Used to get the length)\n",
    "# 2. questions (The list of 3 questions)\n",
    "# 3. together_string (The formatted input containing all answers)\n",
    "\n",
    "# Prepare the question list string for the prompt header\n",
    "question_list_str = f\"Q1 (50pts): {questions[0]}\\nQ2 (30pts): {questions[1]}\\nQ3 (20pts): {questions[2]}\"\n",
    "\n",
    "\n",
    "judge_prompt_text = f\"\"\"You are a meticulous, expert judge in an LLM competition with {len(competitors_display_names)} competitors.\n",
    "The competition consists of 3 distinct questions, each with a different point value, testing diverse skills.\n",
    "\n",
    "Here are the questions and their weights:\n",
    "{question_list_str}\n",
    "\n",
    "Your job is to provide a detailed evaluation for **EACH** competitor's answer to **EACH** of the three questions.\n",
    "For each individual answer, you must provide TWO scores on a 0-100 scale:\n",
    "\n",
    "1.  **judge_score:** Your own direct, objective score (0-100) for the specific answer's quality, clarity, and accuracy. (This will be weighted 60% in the final tally).\n",
    "2.  **peer_average_score:** Your *estimate* (0-100) of the average score that other high-quality LLMs would give that specific answer. (This will be weighted 40% in the final tally).\n",
    "\n",
    "You must maintain the structure of the results exactly as shown in the competitor responses below.\n",
    "\n",
    "Respond with JSON, and **only JSON**, using the following required nested format:\n",
    "{{\n",
    "  \"results\": [\n",
    "    {{\n",
    "      \"competitor_number\": \"1\",\n",
    "      \"scores\": [\n",
    "        {{\"question_id\": \"q1_50pt\", \"judge_score\": <0-100>, \"peer_average_score\": <0-100>}},\n",
    "        {{\"question_id\": \"q2_30pt\", \"judge_score\": <0-100>, \"peer_average_score\": <0-100>}},\n",
    "        {{\"question_id\": \"q3_20pt\", \"judge_score\": <0-100>, \"peer_average_score\": <0-100>}}\n",
    "      ]\n",
    "    }},\n",
    "    {{\n",
    "      \"competitor_number\": \"2\",\n",
    "      \"scores\": [\n",
    "        {{\"question_id\": \"q1_50pt\", \"judge_score\": <0-100>, \"peer_average_score\": <0-100>}},\n",
    "        {{\"question_id\": \"q2_30pt\", \"judge_score\": <0-100>, \"peer_average_score\": <0-100>}},\n",
    "        {{\"question_id\": \"q3_20pt\", \"judge_score\": <0-100>, \"peer_average_score\": <0-100>}}\n",
    "      ]\n",
    "    }},\n",
    "    ... (continue for all {len(competitors_display_names)} competitors)\n",
    "  ]\n",
    "}}\n",
    "\n",
    "Here are the responses from each competitor (identified by their model ID, which corresponds to the 'competitor_number'):\n",
    "\n",
    "{together_string}\n",
    "\n",
    "Now respond with the JSON containing the scores for each competitor based on the questions and the answers provided. Do not include markdown, code blocks, or any other introductory/explanatory text.\n",
    "\"\"\"\n",
    "\n",
    "print(\"--- Judge Prompt Defined ---\")\n",
    "\n",
    "# (Optional: Execute the API Call)\n",
    "# judge_response = openai_client.chat.completions.create(\n",
    "#     model=\"gpt-4o\",  # Use a powerful model for the judging task\n",
    "#     messages=[{\"role\": \"user\", \"content\": judge_prompt_text}],\n",
    "#     temperature=0.0 # Strict decision making\n",
    "# )\n",
    "# judge_data_str = judge_response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìû Stage 6: Calling the Judge LLM & Retrieving Data\n",
    "\n",
    "This code block executes the **single most important API call** in the entire workflow. It takes the massive, complex `judge_prompt_text` (built in Stage 5) and sends it to a powerful \"Judge\" model.\n",
    "\n",
    "The goal is not a creative answer, but a **structured data (JSON) response** containing the scores for every model on every question.\n",
    "\n",
    "### ‚öôÔ∏è API Call Breakdown\n",
    "\n",
    "| Parameter / Action | Purpose & Rationale |\n",
    "| :--- | :--- |\n",
    "| **`JUDGE_MODEL_NAME = \"gpt-4o\"`** | We use a **powerful, high-intelligence model** (like GPT-4o) for judging. A weaker model would fail to follow the complex JSON instructions. |\n",
    "| **`openai_client`** | Even if other models used different clients (Ollama, Groq), we use our most reliable client (OpenAI) for the critical judging task. |\n",
    "| **`temperature=0.0`** | This is crucial. It makes the Judge's decisions as **deterministic and objective** as possible, preventing random creativity and helping to ensure a stable JSON format. |\n",
    "| **`response_format={\"type\": \"json_object\"}`** | This is a specific instruction to the API (if supported by the model) to **guarantee the output is a valid JSON string**, which prevents parsing errors in the next stage. |\n",
    "| **`judge_data_str`** | This variable captures the **raw JSON text string** returned by the API. This raw data is the input for the final calculation stage. |\n",
    "| **`try...except`** | A robust error-handling block is used because this is a large, expensive, and complex API call that could fail. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- ‚ö° Calling Judge Model: gpt-4o ---\n",
      "‚úÖ JSON response received from Judge.\n",
      "\n",
      "--- Raw JSON Data Received (judge_data_str) ---\n",
      "{\n",
      "  \"results\": [\n",
      "    {\n",
      "      \"competitor_number\": \"1\",\n",
      "      \"scores\": [\n",
      "        {\"question_id\": \"q1_50pt\", \"judge_score\": 95, \"peer_average_score\": 92},\n",
      "        {\"question_id\": \"q2_30pt\", \"judge_score\": 90, \"peer_average_score\": 88},\n",
      "        {\"question_id\": \"q3_20pt\", \"judge_score\": 100, \"peer_average_score\": 98}\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"competitor_number\": \"2\",\n",
      "      \"scores\": [\n",
      "        {\"question_id\": \"q1_50pt\", \"judge_score\": 92, \"peer_average_score\": 90},\n",
      "        {\"question_id\": \"q2_30pt\", \"judge_score\": 88, \"peer_average_score\": 85},\n",
      "        {\"question_id\": \"q3_20pt\", \"judge_score\": 100, \"peer_average_score\": 98}\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"competitor_number\": \"3\",\n",
      "      \"scores\": [\n",
      "        {\"question_id\": \"q1_50pt\", \"judge_score\": 93, \"peer_average_score\": 91},\n",
      "        {\"question_id\": \"q2_30pt\", \"judge_score\": 89, \"peer_average_score\": 87},\n",
      "        {\"question_id\": \"q3_20pt\", \"judge_score\": 100, \"peer_average_score\": 98}\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"competitor_number\": \"4\",\n",
      "      \"scores\": [\n",
      "        {\"question_id\": \"q1_50pt\", \"judge_score\": 90, \"peer_average_score\": 88},\n",
      "        {\"question_id\": \"q2_30pt\", \"judge_score\": 85, \"peer_average_score\": 83},\n",
      "        {\"question_id\": \"q3_20pt\", \"judge_score\": 100, \"peer_average_score\": 98}\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"competitor_number\": \"5\",\n",
      "      \"scores\": [\n",
      "        {\"question_id\": \"q1_50pt\", \"judge_score\": 88, \"peer_average_score\": 86},\n",
      "        {\"question_id\": \"q2_30pt\", \"judge_score\": 82, \"peer_average_score\": 80},\n",
      "        {\"question_id\": \"q3_20pt\", \"judge_score\": 100, \"peer_average_score\": 98}\n",
      "      ]\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "---------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# --- Stage 6: Calling the Judge LLM and Retrieving JSON Data ---\n",
    "\n",
    "# IMPORTANT: We need a powerful model for this stage (GPT-4o is recommended).\n",
    "JUDGE_MODEL_NAME = \"gpt-4o\" \n",
    "\n",
    "# Even if not all competitors use the OpenAI client, \n",
    "# it is ideal to use the most reliable client for the Judge.\n",
    "# We assume 'openai_client' is defined and working.\n",
    "\n",
    "print(f\"\\n--- ‚ö° Calling Judge Model: {JUDGE_MODEL_NAME} ---\")\n",
    "\n",
    "try:\n",
    "    # 1. Make the API Call\n",
    "    judge_response = openai_client.chat.completions.create(\n",
    "        model=JUDGE_MODEL_NAME,  # Choose a powerful model for the best results\n",
    "        messages=[{\"role\": \"user\", \"content\": judge_prompt_text}],\n",
    "        temperature=0.0, # Lowest temperature (precision) for ranking and JSON\n",
    "        # Specify that we want a JSON format response (if the model supports it)\n",
    "        response_format={\"type\": \"json_object\"} \n",
    "    )\n",
    "    \n",
    "    # 2. Retrieve and Store the JSON Data\n",
    "    judge_data_str = judge_response.choices[0].message.content\n",
    "    \n",
    "    # 3. Display the Result\n",
    "    print(\"‚úÖ JSON response received from Judge.\")\n",
    "    \n",
    "    # Let's print the content of the judge_data_str variable.\n",
    "    # This is the raw data we will use for the next stage (Scoring).\n",
    "    print(\"\\n--- Raw JSON Data Received (judge_data_str) ---\")\n",
    "    print(judge_data_str)\n",
    "    print(\"---------------------------------------------\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå ERROR: Judge LLM API call failed: {e}\")\n",
    "    judge_data_str = None # Set to None in case of error\n",
    "    \n",
    "# Now we can move to Stage 7 and use 'judge_data_str' to calculate scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèÜ Stage 7: Final Weighted Score Calculation & Leaderboard\n",
    "\n",
    "This is the final stage that calculates and displays the winner. This code block takes the raw, nested JSON string (`judge_data_str`) retrieved in **Stage 6** and applies your complex, two-layer \"Triathlon\" weighting system to produce the final leaderboard.\n",
    "\n",
    "### üßÆ The Scoring Logic Explained\n",
    "\n",
    "This script performs two levels of mathematical weighting to get the final score:\n",
    "\n",
    "| Level | Calculation | Purpose |\n",
    "| :--- | :--- | :--- |\n",
    "| **1. Answer-Level (60/40)** | `(Judge Score * 0.60) + (Peer Score * 0.40)` | First, it calculates the **Adjusted Score** for each *individual answer* to determine its overall quality. |\n",
    "| **2. Question-Level (50/30/20)**| `(Adjusted Score * Question Weight)` | Second, it calculates the answer's **Final Contribution** by multiplying its Adjusted Score by the question's importance (50%, 30%, or 20%). |\n",
    "\n",
    "### üèÅ Final Score\n",
    "\n",
    "The **Total Weighted Score** for each model is the **sum of its three Final Contributions**. The script then sorts all models by this final score to generate the definitive leaderboard.\n",
    "\n",
    "The code also includes robust `try...except` blocks to catch:\n",
    "* `json.JSONDecodeError`: If the Judge LLM returned invalid JSON.\n",
    "* `KeyError`: If the returned JSON is missing an expected field (e.g., `judge_score`), meaning the prompt instructions were not followed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "--- üèÜ FINAL TRIATHLON LEADERBOARD üèÜ ---\n",
      "==================================================\n",
      "#1: OpenAI GPT-5 Mini\n",
      "   (Final Weighted Score: 93.5000)\n",
      "   - Q1 (50pt): Answer Score 93.80 -> Contribution: 46.90\n",
      "   - Q2 (30pt): Answer Score 89.20 -> Contribution: 26.76\n",
      "   - Q3 (20pt): Answer Score 99.20 -> Contribution: 19.84\n",
      "\n",
      "#2: Groq Llama 3 120B\n",
      "   (Final Weighted Score: 92.4000)\n",
      "   - Q1 (50pt): Answer Score 92.20 -> Contribution: 46.10\n",
      "   - Q2 (30pt): Answer Score 88.20 -> Contribution: 26.46\n",
      "   - Q3 (20pt): Answer Score 99.20 -> Contribution: 19.84\n",
      "\n",
      "#3: Google Gemini 2.5 Flash\n",
      "   (Final Weighted Score: 91.4800)\n",
      "   - Q1 (50pt): Answer Score 91.20 -> Contribution: 45.60\n",
      "   - Q2 (30pt): Answer Score 86.80 -> Contribution: 26.04\n",
      "   - Q3 (20pt): Answer Score 99.20 -> Contribution: 19.84\n",
      "\n",
      "#4: DeepSeek Chat\n",
      "   (Final Weighted Score: 89.7000)\n",
      "   - Q1 (50pt): Answer Score 89.20 -> Contribution: 44.60\n",
      "   - Q2 (30pt): Answer Score 84.20 -> Contribution: 25.26\n",
      "   - Q3 (20pt): Answer Score 99.20 -> Contribution: 19.84\n",
      "\n",
      "#5: Ollama Llama 3\n",
      "   (Final Weighted Score: 87.8000)\n",
      "   - Q1 (50pt): Answer Score 87.20 -> Contribution: 43.60\n",
      "   - Q2 (30pt): Answer Score 81.20 -> Contribution: 24.36\n",
      "   - Q3 (20pt): Answer Score 99.20 -> Contribution: 19.84\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# --- Stage 7: Final Weighted Score Calculation ---\n",
    "\n",
    "# Define the question weights based on your request (50, 30, 20 points)\n",
    "# These represent the overall weight of each question in the final score (e.g., 50/100 = 0.5)\n",
    "QUESTION_WEIGHTS = {\n",
    "    \"q1_50pt\": 0.50, # Q1 (50 points) has 50% weight\n",
    "    \"q2_30pt\": 0.30, # Q2 (30 points) has 30% weight\n",
    "    \"q3_20pt\": 0.20  # Q3 (20 points) has 20% weight\n",
    "}\n",
    "\n",
    "# The weight applied to the Judge's own score for each individual answer\n",
    "JUDGE_SCORE_WEIGHT = 0.60\n",
    "PEER_SCORE_WEIGHT = 0.40\n",
    "\n",
    "# --- Start Calculation ---\n",
    "if 'judge_data_str' not in globals() or judge_data_str is None:\n",
    "    print(\"Error: The raw Judge data ('judge_data_str') is missing. Please ensure Stage 6 ran successfully.\")\n",
    "else:\n",
    "    try:\n",
    "        data = json.loads(judge_data_str)\n",
    "        evaluation_results = data[\"results\"]\n",
    "        final_rankings = []\n",
    "\n",
    "        # --- Loop 1: Process Each Competitor's Scores ---\n",
    "        for result in evaluation_results:\n",
    "            \n",
    "            competitor_index = int(result[\"competitor_number\"]) - 1\n",
    "            \n",
    "            # Use the display names list defined in the Setup stage\n",
    "            model_display_name = competitors_display_names[competitor_index]\n",
    "            \n",
    "            # Initialize total score components for this competitor\n",
    "            total_weighted_score = 0\n",
    "            details_breakdown = {}\n",
    "            \n",
    "            # --- Loop 2: Process Scores for Each of the 3 Questions ---\n",
    "            # 'scores' is the nested list of 3 score objects\n",
    "            for score_entry in result[\"scores\"]:\n",
    "                q_id = score_entry[\"question_id\"]\n",
    "                judge_score = score_entry[\"judge_score\"]\n",
    "                peer_score = score_entry[\"peer_average_score\"]\n",
    "                \n",
    "                # 1. Calculate the Adjusted Score for this specific Answer (60/40 Split)\n",
    "                adjusted_answer_score = (judge_score * JUDGE_SCORE_WEIGHT) + (peer_score * PEER_SCORE_WEIGHT)\n",
    "                \n",
    "                # 2. Apply the Question's Weight (50%, 30%, 20%)\n",
    "                question_weight = QUESTION_WEIGHTS.get(q_id, 0)\n",
    "                contribution_to_final = adjusted_answer_score * question_weight\n",
    "                \n",
    "                # 3. Accumulate the scores\n",
    "                total_weighted_score += contribution_to_final\n",
    "                \n",
    "                # Store breakdown for detailed output\n",
    "                details_breakdown[q_id] = {\n",
    "                    \"adjusted_score\": f\"{adjusted_answer_score:.2f}\",\n",
    "                    \"contribution\": f\"{contribution_to_final:.2f}\"\n",
    "                }\n",
    "\n",
    "            # Add the final score and details to the ranking list\n",
    "            final_rankings.append({\n",
    "                \"model\": model_display_name,\n",
    "                \"final_score\": total_weighted_score,\n",
    "                \"details\": details_breakdown\n",
    "            })\n",
    "\n",
    "        # --- Sort and Print Results ---\n",
    "        final_rankings_sorted = sorted(final_rankings, key=lambda x: x[\"final_score\"], reverse=True)\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        print(\"--- üèÜ FINAL TRIATHLON LEADERBOARD üèÜ ---\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        for i, rank in enumerate(final_rankings_sorted, 1):\n",
    "            details = rank['details']\n",
    "            print(f\"#{i}: {rank['model']}\")\n",
    "            print(f\"   (Final Weighted Score: {rank['final_score']:.4f})\")\n",
    "            print(f\"   - Q1 (50pt): Answer Score {details['q1_50pt']['adjusted_score']} -> Contribution: {details['q1_50pt']['contribution']}\")\n",
    "            print(f\"   - Q2 (30pt): Answer Score {details['q2_30pt']['adjusted_score']} -> Contribution: {details['q2_30pt']['contribution']}\")\n",
    "            print(f\"   - Q3 (20pt): Answer Score {details['q3_20pt']['adjusted_score']} -> Contribution: {details['q3_20pt']['contribution']}\\n\")\n",
    "\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"\\n--- ERROR ---\")\n",
    "        print(\"‚ùå JSON DECODING FAILED. The Judge LLM did not return valid JSON.\")\n",
    "        print(\"Received data (truncated):\", judge_data_str[:200] if judge_data_str else \"N/A\")\n",
    "    except KeyError as e:\n",
    "        print(\"\\n--- ERROR ---\")\n",
    "        print(f\"‚ùå KEY ERROR: JSON data structure is incorrect (Missing key: {e}).\")\n",
    "        print(\"Ensure the Judge Prompt was followed exactly.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Stage 8: Visual Leaderboard (Matplotlib)\n",
    "\n",
    "This final block uses the `matplotlib` library to render the results from **Stage 7** as a professional, easy-to-read horizontal bar chart.\n",
    "\n",
    "This provides an immediate visual summary of the competition, making the final rankings clear at a glance.\n",
    "\n",
    "### üé® Chart Features\n",
    "\n",
    "| Feature | Implementation | Purpose |\n",
    "| :--- | :--- | :--- |\n",
    "| **Horizontal Bars** | `plt.barh(...)` | Provides a clean layout, especially for long model names. |\n",
    "| **Winner Highlight** | `colors.append('gold')` | The top-scoring model (the winner) is automatically colored **gold** to distinguish it from the rest. |\n",
    "| **Dynamic Height** | `fig_height = max(5, ...)` | The chart's height adjusts based on the number of competitors, preventing labels from overlapping. |\n",
    "| **Data Labels** | `plt.text(...)` | The precise `Final Weighted Score` (formatted to 3 decimals) is printed next to each bar for clarity. |\n",
    "| **Clean Aesthetics** | `spines['top'].set_visible(False)` | Removes the top and right borders (\"spines\") for a modern, less cluttered look. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****In Case Of Installing MatplotLib Libary*****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.7-cp312-cp312-macosx_11_0_arm64.whl.metadata (11 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.3.3-cp312-cp312-macosx_11_0_arm64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.60.1-cp312-cp312-macosx_10_13_universal2.whl.metadata (112 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.9-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: numpy>=1.23 in /Users/gonenc_aydin/Desktop/The_Complete_AI_Agent_Course/agents/.venv/lib/python3.12/site-packages (from matplotlib) (2.3.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/gonenc_aydin/Desktop/The_Complete_AI_Agent_Course/agents/.venv/lib/python3.12/site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /Users/gonenc_aydin/Desktop/The_Complete_AI_Agent_Course/agents/.venv/lib/python3.12/site-packages (from matplotlib) (11.2.1)\n",
      "Collecting pyparsing>=3 (from matplotlib)\n",
      "  Downloading pyparsing-3.2.5-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/gonenc_aydin/Desktop/The_Complete_AI_Agent_Course/agents/.venv/lib/python3.12/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/gonenc_aydin/Desktop/The_Complete_AI_Agent_Course/agents/.venv/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Downloading matplotlib-3.10.7-cp312-cp312-macosx_11_0_arm64.whl (8.1 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m8.1/8.1 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading contourpy-1.3.3-cp312-cp312-macosx_11_0_arm64.whl (273 kB)\n",
      "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.60.1-cp312-cp312-macosx_10_13_universal2.whl (2.8 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading kiwisolver-1.4.9-cp312-cp312-macosx_11_0_arm64.whl (64 kB)\n",
      "Downloading pyparsing-3.2.5-py3-none-any.whl (113 kB)\n",
      "Installing collected packages: pyparsing, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "Successfully installed contourpy-1.3.3 cycler-0.12.1 fonttools-4.60.1 kiwisolver-1.4.9 matplotlib-3.10.7 pyparsing-3.2.5\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "--- üìä VISUAL LEADERBOARD üìä ---\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qt/s6qvmjn170s77h1bq6pd1pq40000gn/T/ipykernel_23597/857016142.py:51: UserWarning: Glyph 127942 (\\N{TROPHY}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "/Users/gonenc_aydin/Desktop/The_Complete_AI_Agent_Course/agents/.venv/lib/python3.12/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 127942 (\\N{TROPHY}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# This command ensures the plot displays inside the notebook\n",
    "%matplotlib inline \n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"--- üìä VISUAL LEADERBOARD üìä ---\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Check if the final ranking data exists\n",
    "if 'final_rankings_sorted' in globals() and final_rankings_sorted:\n",
    "    \n",
    "    # Reverse the data so the highest score is at the top of the chart\n",
    "    final_rankings_display = final_rankings_sorted[::-1]\n",
    "    \n",
    "    # Separate model names and scores\n",
    "    models = [r['model'] for r in final_rankings_display]\n",
    "    scores = [r['final_score'] for r in final_rankings_display]\n",
    "    \n",
    "    # Create a color list (default 'skyblue', winner 'gold')\n",
    "    colors = ['skyblue'] * (len(models) - 1)\n",
    "    colors.append('gold') # The last item (highest score)\n",
    "    \n",
    "    # Dynamically adjust the figure height based on the number of models\n",
    "    fig_height = max(5, len(models) * 0.7)\n",
    "    plt.figure(figsize=(10, fig_height))\n",
    "    \n",
    "    # Create the horizontal bars\n",
    "    bars = plt.barh(models, scores, color=colors, edgecolor='black')\n",
    "    \n",
    "    # Axis labels and title\n",
    "    plt.xlabel('Final Weighted Score', fontsize=12)\n",
    "    plt.title('üèÜ LLM Competition Leaderboard üèÜ', fontsize=16, pad=20)\n",
    "    \n",
    "    # Clean up the chart (remove top and right spines)\n",
    "    plt.gca().spines['top'].set_visible(False)\n",
    "    plt.gca().spines['right'].set_visible(False)\n",
    "    \n",
    "    # Add score labels on the end of each bar\n",
    "    for bar in bars:\n",
    "        width = bar.get_width()\n",
    "        plt.text(width + 0.01,  # Position label to the right of the bar\n",
    "                 bar.get_y() + bar.get_height() / 2,\n",
    "                 f'{width:.3f}', # Format score to 3 decimal places\n",
    "                 va='center', \n",
    "                 ha='left',\n",
    "                 fontsize=10)\n",
    "    \n",
    "    # Adjust left margin for long model names\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå Leaderboard data not found. Please run the scoring cell (Stage 7) first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/exercise.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#ff7800;\">Exercise</h2>\n",
    "            <span style=\"color:#ff7800;\">Which pattern(s) did this use? Try updating this to add another Agentic design pattern.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/business.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#00bfff;\">Commercial implications</h2>\n",
    "            <span style=\"color:#00bfff;\">These kinds of patterns - to send a task to multiple models, and evaluate results,\n",
    "            are common where you need to improve the quality of your LLM response. This approach can be universally applied\n",
    "            to business projects where accuracy is critical.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
