{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üèÜ Welcome to the LLM Triathlon Engine üèÜ\n",
    "\n",
    "Welcome to the **LLM Triathlon Engine**! This notebook is an automated framework designed to rigorously test and benchmark multiple Large Language Models (LLMs) from various providers (like OpenAI, Groq, and local Ollama) in a fair and comprehensive \"triathlon.\"\n",
    "\n",
    "---\n",
    "\n",
    "> ### üèä‚Äç‚ôÇÔ∏èüö¥‚Äç‚ôÇÔ∏èüèÉ‚Äç‚ôÇÔ∏è The Triathlon Concept\n",
    ">\n",
    "> A simple 100m dash (one question) isn't enough to find the best all-around model. A triathlon tests endurance and skill across three different events with *different weights*. Our engine does the same:\n",
    "> * **Event 1 (Heavy):** A \"heavy-weight\" question (worth **50 points**)\n",
    "> * **Event 2 (Medium):** A \"medium-weight\" question (worth **30 points**)\n",
    "> * **Event 3 (Light):** A \"light-weight\" question (worth **20 points**)\n",
    ">\n",
    "> The final winner is the model with the highest **total weighted score** across all three events.\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ How It Works: The 8-Stage Pipeline\n",
    "\n",
    "This engine runs in a sequential pipeline. Here is the step-by-step \"bulletin board\" for how it functions:\n",
    "\n",
    "| Stage | Emoji | Description | Key Output |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| **Stage 1** | ‚öôÔ∏è | **Setup & Client Init**<br>Dynamically checks all your `os.getenv()` API keys (OpenAI, Groq, etc.) and your local Ollama server. It then builds the list of **available** models to compete. | `competitors` (list) |\n",
    "| **Stage 2** | üß† | **Question Generation**<br>Uses a \"Generator\" LLM to create 3 diverse, high-quality questions. It then uses a \"Consultant\" LLM to rank them and assign the **50, 30, and 20-point** weights. | `questions` (list) |\n",
    "| **Stage 3** | üèÉ‚Äç‚ôÇÔ∏è | **The Race (Execution)**<br>The main engine. It loops through every *available* model (`competitors`) and asks it *all three* ranked questions (`questions`), dynamically using the correct API client for each. | `all_answers` (dict) |\n",
    "| **Stage 4** | üìä | **Answer Visualization**<br>Renders all raw answers in a clean, **question-by-question** format. This allows you, the user, to manually inspect and compare the performance on each task. | *Markdown Output* |\n",
    "| **Stage 5** | üèóÔ∏è | **Judge Data Formatting**<br>Combines *all* answers from all models into a single, massive, and meticulously labeled text string (`together_string`) ready to be sent to the Judge. | `together_string` (str) |\n",
    "| **Stage 6** | ‚öñÔ∏è | **The \"Judge\" Call**<br>Sends the massive prompt (with `together_string`) to a powerful Judge LLM (`gpt-4o`). It requests a **nested JSON** response containing 6 scores per model. | `judge_data_str` (JSON) |\n",
    "| **Stage 7** | üßÆ | **Final Score Calculation**<br>Parses the Judge's JSON. Applies the **two-layer weighting system:**<br> 1. `(Judge Score * 0.6) + (Peer Score * 0.4)`<br> 2. `(Q1 * 0.5) + (Q2 * 0.3) + (Q3 * 0.2)` | `final_rankings_sorted` (list) |\n",
    "| **Stage 8** | ü•á | **The Podium (Graphing)**<br>Uses `matplotlib` to plot the final results in a horizontal bar chart. The winning model with the highest total score is highlighted in **gold**. | *Visual Bar Chart* |\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with imports - ask ChatGPT to explain any package that you don't know\n",
    "\n",
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from anthropic import Anthropic\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Always remember to do this!\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚öôÔ∏è Stage 0 : Competition Setup and Client Initialization\n",
    "\n",
    "This block prepares the foundational infrastructure for the LLM competition. It checks all available API keys defined in environment variables and dynamically adds only the **accessible** models to the list of competitors. This ensures the robustness and flexibility of our testing environment.\n",
    "\n",
    "| Task | Purpose | Control Mechanism |\n",
    "| :--- | :--- | :--- |\n",
    "| **API Key Detection** | Determines which services can be used. | Checks for the presence of keys using `os.getenv()`. |\n",
    "| **Client Initialization** | Creates `OpenAI` compatible clients using the correct `base_url` and `api_key` for each service. | **Example:** Uses specific `base_url` for providers like Groq and DeepSeek. |\n",
    "| **Ollama Check** | Tests whether the local server is active. | Sends an HTTP request via `requests` to check `http://localhost:11434`. |\n",
    "| **Competitor List** | Adds each initialized model (with its client, model ID, and display name) to the `competitors` list. | Only `‚úÖ Successful` models proceed to the next stage. |\n",
    "\n",
    "***Please verify any instances of \"‚ùå ERROR\" or \"‚ùå Not Set\" in the output.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 0\n",
    "# Print the key prefixes to help with any debugging\n",
    "import os\n",
    "import time\n",
    "from openai import OpenAI\n",
    "import requests \n",
    "\n",
    "# --- Stage 0: Initialization and Variable Definitions ---\n",
    "\n",
    "# Initialize the questions list\n",
    "questions = []\n",
    "\n",
    "# Initialize the competitors list as empty\n",
    "competitors = []\n",
    "\n",
    "print(\"--- üèÅ Setup Check: API Keys and Clients ---\")\n",
    "\n",
    "# --- Stage 1: Retrieve API Keys from Environment Variables ---\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "google_api_key = os.getenv(\"GOOGLE_API_KEY\") \n",
    "deepseek_api_key = os.getenv(\"DEEPSEEK_API_KEY\")\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "# --- Stage 2: Check and Initialize Clients ---\n",
    "\n",
    "# --- OpenAI Check ---\n",
    "if openai_api_key:\n",
    "    print(f\"‚úÖ OpenAI API Key found (Starts: {openai_api_key[:8]}...)\")\n",
    "    try:\n",
    "        openai_client = OpenAI(api_key=openai_api_key)\n",
    "        # Note: Model name 'gpt-4.1' is not official and likely to cause errors. Using it as provided for now.\n",
    "        competitors.append({\"name\": \"gpt-4.1\", \"client\": openai_client, \"display_name\": \"OpenAI GPT-5 Mini\"})\n",
    "        print(\"   -> OpenAI client and models added to the competition.\")\n",
    "    except Exception as e:\n",
    "        print(f\"   -> ‚ùå ERROR: Could not initialize OpenAI client: {e}\")\n",
    "else:\n",
    "    print(\"‚ùå OpenAI API KEY not set.\")\n",
    "print(\"-\" * 20)\n",
    "\n",
    "# --- Google (Gemini) Check ---\n",
    "if google_api_key:\n",
    "    print(f\"‚úÖ Google API Key found (Starts: {google_api_key[:2]}...)\")\n",
    "    try:\n",
    "        # Client configured for an assumed OpenAI-compatible wrapper/proxy\n",
    "        google_client = OpenAI(  \n",
    "            api_key=google_api_key,\n",
    "            # Note: This base_url might need to be adjusted based on the specific wrapper being used.\n",
    "            base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    "        )\n",
    "        # Add Google's OpenAI-compatible model\n",
    "        competitors.append({\n",
    "            \"name\": \"gemini-2.5-flash\", # Model ID in the OpenAI-compatible interface\n",
    "            \"client\": google_client,\n",
    "            \"display_name\": \"Google Gemini 2.5 Flash\"\n",
    "        })\n",
    "        print(\"   -> Google client (Gemini) and model added to the competition.\")\n",
    "    except Exception as e:\n",
    "        print(f\"   -> ‚ùå ERROR: Could not initialize Google client: {e}\")\n",
    "else:\n",
    "    print(\"‚ùå Google API KEY not set.\")\n",
    "print(\"-\" * 20)\n",
    "\n",
    "# --- Groq Check ---\n",
    "if groq_api_key:\n",
    "    print(f\"‚úÖ Groq API Key found (Starts: {groq_api_key[:4]}...)\")\n",
    "    try:\n",
    "        groq_client = OpenAI(\n",
    "            api_key=groq_api_key,\n",
    "            base_url='https://api.groq.com/openai/v1'\n",
    "        )\n",
    "        # Note: Model name 'openai/gpt-oss-120b' is likely incorrect for Groq. Using it as provided for now.\n",
    "        competitors.append({\"name\": \"openai/gpt-oss-120b\", \"client\": groq_client, \"display_name\": \"Groq Llama 3 120B\"})\n",
    "        print(\"   -> Groq client and model added to the competition.\")\n",
    "    except Exception as e:\n",
    "        print(f\"   -> ‚ùå ERROR: Could not initialize Groq client: {e}\")\n",
    "else:\n",
    "    print(\"‚ùå Groq API KEY not set.\")\n",
    "print(\"-\" * 20)\n",
    "\n",
    "# --- DeepSeek Check ---\n",
    "if deepseek_api_key:\n",
    "    print(f\"‚úÖ DeepSeek API Key found (Starts: {deepseek_api_key[:3]}...)\")\n",
    "    try:\n",
    "        deepseek_client = OpenAI(\n",
    "            api_key=deepseek_api_key,\n",
    "            base_url=\"https://api.deepseek.com/v1\"\n",
    "        )\n",
    "        competitors.append({\"name\": \"deepseek-chat\", \"client\": deepseek_client, \"display_name\": \"DeepSeek Chat\"})\n",
    "        print(\"   -> DeepSeek client and model added to the competition.\")\n",
    "    except Exception as e:\n",
    "        print(f\"   -> ‚ùå ERROR: Could not initialize DeepSeek client: {e}\")\n",
    "else:\n",
    "    print(\"‚ùå Deepseek API KEY not set.\")\n",
    "print(\"-\" * 20)\n",
    "\n",
    "# --- Ollama Check (Local Server) ---\n",
    "print(\"üîÑ Checking Ollama (Local) server...\")\n",
    "try:\n",
    "    # Check if the Ollama server is running locally\n",
    "    response = requests.get('http://localhost:11434/v1/models', timeout=3)\n",
    "    if response.status_code == 200:\n",
    "        print(f\"‚úÖ Ollama server is active at 'http://localhost:11434'.\")\n",
    "        ollama_client = OpenAI(\n",
    "            base_url='http://localhost:11434/v1', \n",
    "            api_key='ollama'\n",
    "        )\n",
    "        # Add Ollama model (Assuming 'llama3.2' is installed locally)\n",
    "        competitors.append({\"name\": \"llama3.2\", \"client\": ollama_client, \"display_name\": \"Ollama Llama 3\"})\n",
    "        print(\"   -> Ollama client and model added to the competition.\")\n",
    "    else:\n",
    "        print(f\"‚ùå Ollama server is not responding (Status: {response.status_code}).\")\n",
    "except requests.ConnectionError:\n",
    "    print(\"‚ùå Could not connect to the Ollama server at 'http://localhost:11434'. Is the server running?\")\n",
    "except Exception as e:\n",
    "    print(f\"   -> ‚ùå ERROR: Could not initialize Ollama client: {e}\")\n",
    "print(\"-\" * 20)\n",
    "\n",
    "\n",
    "# --- Stage 3: Final Report ---\n",
    "print(\"\\n--- ‚úÖ CONTROL COMPLETE ---\")\n",
    "if competitors:\n",
    "    print(f\"{len(competitors)} models successfully configured for the competition:\")\n",
    "    for c in competitors:\n",
    "        print(f\"  - {c['display_name']} (Model ID: {c['name']})\")\n",
    "    \n",
    "    # Create the display names list for reporting in later stages\n",
    "    competitors_display_names = [c[\"display_name\"] for c in competitors]\n",
    "    print(f\"\\nCompetitor list for reporting: {competitors_display_names}\")\n",
    "else:\n",
    "    print(\"‚ùå No models could be loaded for the competition. Please check your API keys or Ollama server.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü•á Stage 1: Question Generation & Weighting\n",
    "\n",
    "This block creates the \"triathlon\" of questions for the competition. Instead of one single question, we generate **three distinct questions** and then use a \"Consultant\" LLM to **assign different weights (50, 30, and 20 points)** to them based on their quality and challenge.\n",
    "\n",
    "This ensures the competition is a more robust test of model abilities across different domains (e.g., logic, creativity, ethics).\n",
    "\n",
    "### Two-Step Generation Process\n",
    "\n",
    "| Step | Model Used | Purpose |\n",
    "| :--- | :--- | :--- |\n",
    "| **1. Generation** | `gpt-4o-mini` (Creative Mode) | Generates 3 diverse, nuanced question candidates, separated by `---`. |\n",
    "| **2. Ranking** | `gpt-4o-mini` (Consultant Mode) | Ranks the 3 candidates and returns a JSON object assigning them to `q_50`, `q_30`, and `q_20`. |\n",
    "\n",
    "### üì¨ Output\n",
    "\n",
    "The code block finishes by:\n",
    "1.  Printing the three questions in their ranked, point-weighted order.\n",
    "2.  Storing them in the global `questions` list, which will be used by **Stage 1 (Competition Execution)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 1\n",
    "import os\n",
    "import json\n",
    "from openai import OpenAI\n",
    "\n",
    "openai = OpenAI()\n",
    "\n",
    "# --- Step 1: Generate 3 Question Candidates ---\n",
    "\n",
    "request = \"Please come up with three challenging, nuanced questions that I can ask a number of LLMs to evaluate their intelligence. \"\n",
    "request += \"The questions should test different skills (e.g., one logic, one creativity, one ethics). \"\n",
    "request += \"Answer only with the questions, separated by '---'.\"\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": request}]\n",
    "\n",
    "try:\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=messages,\n",
    "        n=1, # n=1 is sufficient, as we're asking for 3 questions in one response\n",
    "        temperature=0.8\n",
    "    )\n",
    "    \n",
    "    # Split the questions by the '---' separator\n",
    "    generated_text = response.choices[0].message.content\n",
    "    cands = [q.strip() for q in generated_text.split('---') if q.strip()]\n",
    "\n",
    "    if len(cands) < 3:\n",
    "        raise ValueError(f\"Expected 3 questions, but only got {len(cands)}\")\n",
    "    \n",
    "    # Get the 3 questions\n",
    "    q_cand_1, q_cand_2, q_cand_3 = cands[0], cands[1], cands[2]\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error generating questions: {e}\")\n",
    "    # Continue with default questions in case of error\n",
    "    q_cand_1 = \"Default Question 1\"\n",
    "    q_cand_2 = \"Default Question 2\"\n",
    "    q_cand_3 = \"Default Question 3\"\n",
    "\n",
    "\n",
    "# --- Step 2: Consultant Evaluation (Scoring the Questions) ---\n",
    "\n",
    "# Ask the consultant to assign these 3 questions to 50, 30, 20 point slots\n",
    "consultant_messages = [\n",
    "    {\"role\": \"system\", \"content\": \"\"\"You are a ranking consultant. You will be given three questions. \n",
    "Your job is to rank them by quality and suitability for an LLM competition.\n",
    "The best question should be 'q_50', the second best 'q_30', and the third 'q_20'.\n",
    "Respond ONLY with JSON in the format: {\"q_50\": \"1\", \"q_30\": \"3\", \"q_20\": \"2\"}\n",
    "(Where the number corresponds to the question's order.)\"\"\"},\n",
    "    {\"role\": \"user\", \"content\": f\"Here are the questions:\\n\\n1) {q_cand_1}\\n\\n2) {q_cand_2}\\n\\n3) {q_cand_3}\\n\\nJSON:\"}\n",
    "]\n",
    "\n",
    "# Call the \"consultant\" model with temperature=0 (for a decisive decision)\n",
    "try:\n",
    "    consultant = openai.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\", # Use a powerful model for ranking\n",
    "        messages=consultant_messages,\n",
    "        temperature=0\n",
    "    )\n",
    "\n",
    "    decision_json = json.loads(consultant.choices[0].message.content.strip())\n",
    "    \n",
    "    # Assign questions to variables based on their scores\n",
    "    questions_map = {\"1\": q_cand_1, \"2\": q_cand_2, \"3\": q_cand_3}\n",
    "    \n",
    "    question_50_points = questions_map[decision_json[\"q_50\"]]\n",
    "    question_30_points = questions_map[decision_json[\"q_30\"]]\n",
    "    question_20_points = questions_map[decision_json[\"q_20\"]]\n",
    "\n",
    "    print(\"--- Competition Questions Generated and Ranked ---\")\n",
    "    print(f\"\\n[50 Points]: {question_50_points}\")\n",
    "    print(f\"\\n[30 Points]: {question_30_points}\")\n",
    "    print(f\"\\n[20 Points]: {question_20_points}\")\n",
    "\n",
    "    # Initialize questions list if it doesn't exist\n",
    "    if 'questions' not in globals():\n",
    "        questions = []\n",
    "    # Add the ranked questions to the global 'questions' list\n",
    "    questions.extend([question_50_points, question_30_points, question_20_points])\n",
    "    print(questions)\n",
    "except Exception as e:\n",
    "    print(f\"Error in consultant ranking phase: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèÉ‚Äç‚ôÇÔ∏è Stage 2: Competition Execution (Answer Collection)\n",
    "\n",
    "This is the main engine of the competition. This block iterates through every model configured in **Stage 1** (`competitors` list) and runs them against every question generated in **Stage 1** (`questions` list).\n",
    "\n",
    "It dynamically uses the correct API client (e.g., `openai_client`, `ollama_client`) for each specific model.\n",
    "\n",
    "### ‚öôÔ∏è Workflow Logic\n",
    "\n",
    "1.  **Outer Loop (By Competitor):** Iterates through each model dictionary in the `competitors` list. It retrieves the model's API name (`model_api_name`), its specific API client (`model_client`), and its pretty name (`model_display_name`).\n",
    "2.  **Inner Loop (By Question):** For each model, this loop iterates three times, asking the 50pt, 30pt, and 20pt questions sequentially.\n",
    "3.  **API Call:** It uses the correct, pre-configured `model_client` to call the `chat.completions.create` method. This allows Ollama, Groq, and OpenAI models to all be called using the same code structure.\n",
    "4.  **Error Handling:** A `try...except` block ensures that if one model fails (due to an API error, timeout, or rate limit), it doesn't crash the entire competition. An error message is logged, and a placeholder is stored.\n",
    "5.  **Data Storage:** All three answers from a model are collected into a temporary list and then stored in the main `all_answers` dictionary, using the model's `display_name` as the key.\n",
    "\n",
    "> **‚ùóÔ∏è Rate Limit Note:** A `time.sleep(1)` is included to prevent \"429: Too Many Requests\" errors from APIs by adding a short delay between calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "\n",
    "# --- Stage 2: Execute the Competition (Collect All Answers) ---\n",
    "\n",
    "# NOTE: The 'competitors' list (containing 'name', 'client', and 'display_name') \n",
    "# MUST be initialized in the preceding setup check block. \n",
    "# We will use this pre-configured list here.\n",
    "\n",
    "# The 'questions' list (containing Q1_50pt, Q2_30pt, Q3_20pt texts) \n",
    "# MUST come from Stage 1 (Question Generation).\n",
    "\n",
    "# Dictionary to store all collected responses {display_name: [answer_q1, answer_q2, answer_q3]}\n",
    "all_answers = {}\n",
    "\n",
    "# Check if necessary data exists\n",
    "if 'questions' not in globals() or len(questions) < 3:\n",
    "    print(\"Error: 'questions' list not found or incomplete. Stage 1 was not run correctly.\")\n",
    "elif 'competitors' not in globals() or not competitors:\n",
    "    print(\"Error: 'competitors' list is empty or not configured. Check API keys and setup.\")\n",
    "else:\n",
    "    print(f\"--- üèÅ COMPETITION STARTING ---\")\n",
    "    print(f\"Number of Competitors: {len(competitors)}\")\n",
    "    print(f\"Number of Questions: {len(questions)}\")\n",
    "\n",
    "    question_points = [50, 30, 20] # Define point values for reporting\n",
    "    \n",
    "    # --- OUTER LOOP: Iterates through each configured competitor (client + model) ---\n",
    "    for competitor in competitors:\n",
    "        \n",
    "        # Extract competitor details\n",
    "        model_api_name = competitor[\"name\"]\n",
    "        model_client = competitor[\"client\"] # The specific client (OpenAI, Ollama, Groq, etc.)\n",
    "        model_display_name = competitor[\"display_name\"]\n",
    "        \n",
    "        print(f\"\\n--- ‚ö° Querying Model: {model_display_name} ---\")\n",
    "        \n",
    "        # List to temporarily store the 3 answers for this model\n",
    "        model_answers_list = []\n",
    "        \n",
    "        # --- INNER LOOP: Iterates through each of the 3 questions ---\n",
    "        for i, question_text in enumerate(questions):\n",
    "            \n",
    "            points = question_points[i] \n",
    "            print(f\"  -> Asking Question {i+1} ({points} points)...\")\n",
    "            \n",
    "            # Create messages list for the current question\n",
    "            messages = [{\"role\": \"user\", \"content\": question_text}]\n",
    "            \n",
    "            try:\n",
    "                # API Call using the specific 'model_client' for the current model\n",
    "                response = model_client.chat.completions.create(\n",
    "                    model=model_api_name,\n",
    "                    messages=messages,\n",
    "                    temperature=0.7 # Consistent temperature for all models\n",
    "                )\n",
    "                \n",
    "                answer = response.choices[0].message.content\n",
    "                model_answers_list.append(answer)\n",
    "                \n",
    "                # Small delay to respect API rate limits\n",
    "                time.sleep(1) \n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"    *** ERROR: {model_display_name} failed to respond to Question {i+1}: {e}\")\n",
    "                model_answers_list.append(f\"ERROR: No response received from {model_display_name}.\")\n",
    "        \n",
    "        # After 3 answers are collected, store them in the main dictionary \n",
    "        # using the descriptive 'display_name' as the key\n",
    "        all_answers[model_display_name] = model_answers_list\n",
    "\n",
    "    print(\"\\n--- ‚úÖ COMPETITION COMPLETED ---\")\n",
    "    print(\"All answers collected from the configured models.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Stage 3: Comparative Visualization of All Answers\n",
    "\n",
    "This code block is a **visualization script** that takes the complex `all_answers` dictionary (collected in Stage 2/3) and renders it as clean, human-readable Markdown.\n",
    "\n",
    "Its primary purpose is to allow for **manual inspection and comparison** of the models' performance. The output is intentionally grouped **by question**, not by model, making it easy to see how all competitors tackled the same challenge side-by-side.\n",
    "\n",
    "### üèõÔ∏è Report Structure\n",
    "\n",
    "This script dynamically builds a single large Markdown string by looping through the data.\n",
    "\n",
    "| Loop | Purpose | Visual Output (Example) |\n",
    "| :--- | :--- | :--- |\n",
    "| **Outer Loop (By Question)** | Groups all responses for Q1, then Q2, etc. | `## Question 1 (Weight: 50 Points)` |\n",
    "| *Question Display* | Prints the question text itself in a blockquote. | `> **What is the future of...**` |\n",
    "| **Inner Loop (By Model)** | Iterates through each competitor in a consistent order. | `### üí¨ OpenAI GPT-4o Mini's Response:` |\n",
    "| *Answer Display* | Fetches the specific answer and formats it in a blockquote. | `<blockquote>The future is...</blockquote>` |\n",
    "\n",
    "The entire report is then rendered in the cell output using the `display(Markdown(...))` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stage 3\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# --- Displaying Answers (Question-Based Comparison) ---\n",
    "\n",
    "# IMPORTANT: For this code to work, the 'all_answers', 'questions', and 'competitors_display_names' \n",
    "# variables must have been created in the previous stages.\n",
    "\n",
    "if 'all_answers' not in globals() or 'questions' not in globals():\n",
    "    print(\"Error: Required data ('all_answers' or 'questions') is missing. Please run Stage 1 and 2.\")\n",
    "else:\n",
    "    markdown_output = \"# --- üèÅ COMPETITION ANSWERS: ALL MODELS --- \\n\\n\"\n",
    "    \n",
    "    question_points = [50, 30, 20] \n",
    "    \n",
    "    # --- OUTER LOOP: Group by Question ---\n",
    "    for i, question_text in enumerate(questions):\n",
    "        \n",
    "        # Heading for the Question\n",
    "        markdown_output += f\"## Question {i+1} (Weight: {question_points[i]} Points)\\n\\n\"\n",
    "        \n",
    "        # Display the Question itself\n",
    "        markdown_output += f\"> **{question_text}**\\n\\n\"\n",
    "        markdown_output += \"<hr>\\n\\n\"\n",
    "        \n",
    "        # --- INNER LOOP: Display Each Competitor's Answer to This Question ---\n",
    "        # We iterate over the list of display names to ensure a consistent order\n",
    "        for model_display_name in competitors_display_names:\n",
    "            if model_display_name in all_answers and len(all_answers[model_display_name]) > i:\n",
    "                \n",
    "                # Get the specific answer for this model and question index (i)\n",
    "                answer = all_answers[model_display_name][i]\n",
    "                \n",
    "                markdown_output += f\"### üí¨ {model_display_name}'s Response:\\n\"\n",
    "                \n",
    "                # Use a blockquote for clear separation\n",
    "                markdown_output += f\"<blockquote>{answer}</blockquote>\\n\\n\"\n",
    "                markdown_output += \"---\\n\\n\" \n",
    "            else:\n",
    "                markdown_output += f\"### üí¨ {model_display_name}'s Response:\\n\"\n",
    "                markdown_output += \"> *ERROR: Response not found or missing for this model.*\\n\\n\"\n",
    "                markdown_output += \"---\\n\\n\"\n",
    "        \n",
    "        markdown_output += \"<br><br>\\n\\n\" \n",
    "\n",
    "    # Display the final aggregated output\n",
    "    display(Markdown(markdown_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Stage 4: Formatting Data for the Judge LLM (`together_string`)\n",
    "\n",
    "This code block takes the raw answers collected in Stage 2 and transforms them into a single, highly structured text string (`together_string`) that the Judge LLM can easily understand and process.\n",
    "\n",
    "It serves as the **Data Presentation Layer** before the final API call.\n",
    "\n",
    "### üìù Logic and Purpose\n",
    "\n",
    "The primary goal is to meticulously label every piece of information within the text input so the Judge LLM knows exactly which answer belongs to which competitor and which question weight it carries. This prevents scoring errors and ambiguity.\n",
    "\n",
    "| Element | Code Action | Judge's Interpretation |\n",
    "| :--- | :--- | :--- |\n",
    "| **Outer Loop** | Iterates through `competitors_display_names`. | Clearly identifies **Competitor 1**, **Competitor 2**, etc., ensuring the order matches the final JSON `competitor_number`. |\n",
    "| **Header** | `--- Competitor {index + 1} ({model_display_name}) ---` | Explicitly marks the start of a model's complete set of answers. |\n",
    "| **Inner Loop** | Iterates 3 times (for Q1, Q2, Q3). | Guarantees that **all three answers** (or placeholders) are included in the text input. |\n",
    "| **Labels** | `[Q1 (50pts) Answer]:` | Explicitly links the answer text to its **question weight**, eliminating scoring confusion. |\n",
    "| **Separator** | `=\" * 50` | Provides a strong, visual break between competitors, which helps the LLM reliably parse the long text input. |\n",
    "\n",
    "This meticulous formatting is essential for coercing a Language Model into returning clean, structured **JSON** data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Stage 4: Prepare the Input Data for the Judge LLM ---\n",
    "\n",
    "# IMPORTANT: The 'all_answers' and 'competitors_display_names' variables \n",
    "# must come from Stage 2 (Executing the Competition).\n",
    "\n",
    "together_string = \"\"\n",
    "\n",
    "# Define the question labels/weights for clarity in the prompt\n",
    "question_labels = [\"Q1 (50pts)\", \"Q2 (30pts)\", \"Q3 (20pts)\"]\n",
    "\n",
    "# We use 'competitors_display_names' to maintain the order for scoring\n",
    "for index, model_display_name in enumerate(competitors_display_names):\n",
    "    \n",
    "    # Retrieve all answers for this model from the 'all_answers' dictionary\n",
    "    # It should contain [answer_q1, answer_q2, answer_q3]\n",
    "    answers = all_answers.get(model_display_name, [\n",
    "        \"ERROR: Answer 1 Not Found\", \n",
    "        \"ERROR: Answer 2 Not Found\", \n",
    "        \"ERROR: Answer 3 Not Found\"\n",
    "    ])\n",
    "    \n",
    "    # Header: Competitor Number and Name\n",
    "    together_string += f\"\\n--- Competitor {index + 1} ({model_display_name}) ---\\n\"\n",
    "    together_string += f\"Model ID for scoring: **{index + 1}**\\n\\n\"\n",
    "    \n",
    "    # Format all 3 Answers sequentially\n",
    "    for i in range(3):\n",
    "        label = question_labels[i]\n",
    "        answer = answers[i]\n",
    "        \n",
    "        # Append the answer with its corresponding point label\n",
    "        together_string += f\"[{label} Answer]:\\n\"\n",
    "        together_string += answer + \"\\n\\n\"\n",
    "    \n",
    "    # Add a strong separator between competitors' full sets of answers\n",
    "    together_string += \"=\" * 50 + \"\\n\"\n",
    "\n",
    "print(\"--- Judge Prompt Input (together_string) Prepared ---\")\n",
    "\n",
    "# (Optional: Print the first few lines for verification)\n",
    "print(together_string[:500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öñÔ∏è Stage 5: Defining the Final Judge Prompt\n",
    "\n",
    "This block assembles the **master prompt** that will be sent to the powerful Judge LLM (e.g., `gpt-4o`). This is the most complex and critical piece of prompt engineering in the entire workflow.\n",
    "\n",
    "It combines all previously generated data (`questions` and `together_string`) with a strict set of instructions and a **required JSON output format**.\n",
    "\n",
    "### üìú Anatomy of the Judge Prompt\n",
    "\n",
    "The `judge_prompt_text` variable is a large f-string that dynamically injects the following components:\n",
    "\n",
    "| Injected Variable | Purpose |\n",
    "| :--- | :--- |\n",
    "| `{len(competitors_display_names)}` | Tells the Judge how many competitors to score (e.g., \"5 competitors\"). |\n",
    "| `{question_list_str}` | Provides the full text of all 3 questions, so the Judge has the **context** of what was asked. |\n",
    "| **JSON Schema** (in prompt text) | **The most critical part.** This is a rigid template that *forces* the LLM to return a nested JSON object. |\n",
    "| **Scoring Rules** (in prompt text) | Instructs the Judge to provide the two distinct scores: `judge_score` (objective) and `peer_average_score` (estimated). |\n",
    "| `{together_string}` | This is the **main data payload**. It injects all the formatted answers from all competitors (created in Stage 3/4). |\n",
    "\n",
    "> **üéØ Goal:** To coerce the Judge LLM into acting like a reliable, structured data-parsing-and-scoring API. The success of the final scoring (Stage 6) depends entirely on this prompt returning clean, valid JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Stage 5: Define the Final Judge Prompt ---\n",
    "\n",
    "# NOTE: This prompt assumes the following variables are defined from previous stages:\n",
    "# 1. competitors_display_names (Used to get the length)\n",
    "# 2. questions (The list of 3 questions)\n",
    "# 3. together_string (The formatted input containing all answers)\n",
    "\n",
    "# Prepare the question list string for the prompt header\n",
    "question_list_str = f\"Q1 (50pts): {questions[0]}\\nQ2 (30pts): {questions[1]}\\nQ3 (20pts): {questions[2]}\"\n",
    "\n",
    "\n",
    "judge_prompt_text = f\"\"\"You are a meticulous, expert judge in an LLM competition with {len(competitors_display_names)} competitors.\n",
    "The competition consists of 3 distinct questions, each with a different point value, testing diverse skills.\n",
    "\n",
    "Here are the questions and their weights:\n",
    "{question_list_str}\n",
    "\n",
    "Your job is to provide a detailed evaluation for **EACH** competitor's answer to **EACH** of the three questions.\n",
    "For each individual answer, you must provide TWO scores on a 0-100 scale:\n",
    "\n",
    "1.  **judge_score:** Your own direct, objective score (0-100) for the specific answer's quality, clarity, and accuracy. (This will be weighted 60% in the final tally).\n",
    "2.  **peer_average_score:** Your *estimate* (0-100) of the average score that other high-quality LLMs would give that specific answer. (This will be weighted 40% in the final tally).\n",
    "\n",
    "You must maintain the structure of the results exactly as shown in the competitor responses below.\n",
    "\n",
    "Respond with JSON, and **only JSON**, using the following required nested format:\n",
    "{{\n",
    "  \"results\": [\n",
    "    {{\n",
    "      \"competitor_number\": \"1\",\n",
    "      \"scores\": [\n",
    "        {{\"question_id\": \"q1_50pt\", \"judge_score\": <0-100>, \"peer_average_score\": <0-100>}},\n",
    "        {{\"question_id\": \"q2_30pt\", \"judge_score\": <0-100>, \"peer_average_score\": <0-100>}},\n",
    "        {{\"question_id\": \"q3_20pt\", \"judge_score\": <0-100>, \"peer_average_score\": <0-100>}}\n",
    "      ]\n",
    "    }},\n",
    "    {{\n",
    "      \"competitor_number\": \"2\",\n",
    "      \"scores\": [\n",
    "        {{\"question_id\": \"q1_50pt\", \"judge_score\": <0-100>, \"peer_average_score\": <0-100>}},\n",
    "        {{\"question_id\": \"q2_30pt\", \"judge_score\": <0-100>, \"peer_average_score\": <0-100>}},\n",
    "        {{\"question_id\": \"q3_20pt\", \"judge_score\": <0-100>, \"peer_average_score\": <0-100>}}\n",
    "      ]\n",
    "    }},\n",
    "    ... (continue for all {len(competitors_display_names)} competitors)\n",
    "  ]\n",
    "}}\n",
    "\n",
    "Here are the responses from each competitor (identified by their model ID, which corresponds to the 'competitor_number'):\n",
    "\n",
    "{together_string}\n",
    "\n",
    "Now respond with the JSON containing the scores for each competitor based on the questions and the answers provided. Do not include markdown, code blocks, or any other introductory/explanatory text.\n",
    "\"\"\"\n",
    "\n",
    "print(\"--- Judge Prompt Defined ---\")\n",
    "\n",
    "# (Optional: Execute the API Call)\n",
    "# judge_response = openai_client.chat.completions.create(\n",
    "#     model=\"gpt-4o\",  # Use a powerful model for the judging task\n",
    "#     messages=[{\"role\": \"user\", \"content\": judge_prompt_text}],\n",
    "#     temperature=0.0 # Strict decision making\n",
    "# )\n",
    "# judge_data_str = judge_response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìû Stage 6: Calling the Judge LLM & Retrieving Data\n",
    "\n",
    "This code block executes the **single most important API call** in the entire workflow. It takes the massive, complex `judge_prompt_text` (built in Stage 5) and sends it to a powerful \"Judge\" model.\n",
    "\n",
    "The goal is not a creative answer, but a **structured data (JSON) response** containing the scores for every model on every question.\n",
    "\n",
    "### ‚öôÔ∏è API Call Breakdown\n",
    "\n",
    "| Parameter / Action | Purpose & Rationale |\n",
    "| :--- | :--- |\n",
    "| **`JUDGE_MODEL_NAME = \"gpt-4o\"`** | We use a **powerful, high-intelligence model** (like GPT-4o) for judging. A weaker model would fail to follow the complex JSON instructions. |\n",
    "| **`openai_client`** | Even if other models used different clients (Ollama, Groq), we use our most reliable client (OpenAI) for the critical judging task. |\n",
    "| **`temperature=0.0`** | This is crucial. It makes the Judge's decisions as **deterministic and objective** as possible, preventing random creativity and helping to ensure a stable JSON format. |\n",
    "| **`response_format={\"type\": \"json_object\"}`** | This is a specific instruction to the API (if supported by the model) to **guarantee the output is a valid JSON string**, which prevents parsing errors in the next stage. |\n",
    "| **`judge_data_str`** | This variable captures the **raw JSON text string** returned by the API. This raw data is the input for the final calculation stage. |\n",
    "| **`try...except`** | A robust error-handling block is used because this is a large, expensive, and complex API call that could fail. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Stage 6: Calling the Judge LLM and Retrieving JSON Data ---\n",
    "\n",
    "# IMPORTANT: We need a powerful model for this stage (GPT-4o is recommended).\n",
    "JUDGE_MODEL_NAME = \"gpt-4o\" \n",
    "\n",
    "# Even if not all competitors use the OpenAI client, \n",
    "# it is ideal to use the most reliable client for the Judge.\n",
    "# We assume 'openai_client' is defined and working.\n",
    "\n",
    "print(f\"\\n--- ‚ö° Calling Judge Model: {JUDGE_MODEL_NAME} ---\")\n",
    "\n",
    "try:\n",
    "    # 1. Make the API Call\n",
    "    judge_response = openai_client.chat.completions.create(\n",
    "        model=JUDGE_MODEL_NAME,  # Choose a powerful model for the best results\n",
    "        messages=[{\"role\": \"user\", \"content\": judge_prompt_text}],\n",
    "        temperature=0.0, # Lowest temperature (precision) for ranking and JSON\n",
    "        # Specify that we want a JSON format response (if the model supports it)\n",
    "        response_format={\"type\": \"json_object\"} \n",
    "    )\n",
    "    \n",
    "    # 2. Retrieve and Store the JSON Data\n",
    "    judge_data_str = judge_response.choices[0].message.content\n",
    "    \n",
    "    # 3. Display the Result\n",
    "    print(\"‚úÖ JSON response received from Judge.\")\n",
    "    \n",
    "    # Let's print the content of the judge_data_str variable.\n",
    "    # This is the raw data we will use for the next stage (Scoring).\n",
    "    print(\"\\n--- Raw JSON Data Received (judge_data_str) ---\")\n",
    "    print(judge_data_str)\n",
    "    print(\"---------------------------------------------\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå ERROR: Judge LLM API call failed: {e}\")\n",
    "    judge_data_str = None # Set to None in case of error\n",
    "    \n",
    "# Now we can move to Stage 7 and use 'judge_data_str' to calculate scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèÜ Stage 7: Final Weighted Score Calculation & Leaderboard\n",
    "\n",
    "This is the final stage that calculates and displays the winner. This code block takes the raw, nested JSON string (`judge_data_str`) retrieved in **Stage 6** and applies your complex, two-layer \"Triathlon\" weighting system to produce the final leaderboard.\n",
    "\n",
    "### üßÆ The Scoring Logic Explained\n",
    "\n",
    "This script performs two levels of mathematical weighting to get the final score:\n",
    "\n",
    "| Level | Calculation | Purpose |\n",
    "| :--- | :--- | :--- |\n",
    "| **1. Answer-Level (60/40)** | `(Judge Score * 0.60) + (Peer Score * 0.40)` | First, it calculates the **Adjusted Score** for each *individual answer* to determine its overall quality. |\n",
    "| **2. Question-Level (50/30/20)**| `(Adjusted Score * Question Weight)` | Second, it calculates the answer's **Final Contribution** by multiplying its Adjusted Score by the question's importance (50%, 30%, or 20%). |\n",
    "\n",
    "### üèÅ Final Score\n",
    "\n",
    "The **Total Weighted Score** for each model is the **sum of its three Final Contributions**. The script then sorts all models by this final score to generate the definitive leaderboard.\n",
    "\n",
    "The code also includes robust `try...except` blocks to catch:\n",
    "* `json.JSONDecodeError`: If the Judge LLM returned invalid JSON.\n",
    "* `KeyError`: If the returned JSON is missing an expected field (e.g., `judge_score`), meaning the prompt instructions were not followed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# --- Stage 7: Final Weighted Score Calculation ---\n",
    "\n",
    "# Define the question weights based on your request (50, 30, 20 points)\n",
    "# These represent the overall weight of each question in the final score (e.g., 50/100 = 0.5)\n",
    "QUESTION_WEIGHTS = {\n",
    "    \"q1_50pt\": 0.50, # Q1 (50 points) has 50% weight\n",
    "    \"q2_30pt\": 0.30, # Q2 (30 points) has 30% weight\n",
    "    \"q3_20pt\": 0.20  # Q3 (20 points) has 20% weight\n",
    "}\n",
    "\n",
    "# The weight applied to the Judge's own score for each individual answer\n",
    "JUDGE_SCORE_WEIGHT = 0.60\n",
    "PEER_SCORE_WEIGHT = 0.40\n",
    "\n",
    "# --- Start Calculation ---\n",
    "if 'judge_data_str' not in globals() or judge_data_str is None:\n",
    "    print(\"Error: The raw Judge data ('judge_data_str') is missing. Please ensure Stage 6 ran successfully.\")\n",
    "else:\n",
    "    try:\n",
    "        data = json.loads(judge_data_str)\n",
    "        evaluation_results = data[\"results\"]\n",
    "        final_rankings = []\n",
    "\n",
    "        # --- Loop 1: Process Each Competitor's Scores ---\n",
    "        for result in evaluation_results:\n",
    "            \n",
    "            competitor_index = int(result[\"competitor_number\"]) - 1\n",
    "            \n",
    "            # Use the display names list defined in the Setup stage\n",
    "            model_display_name = competitors_display_names[competitor_index]\n",
    "            \n",
    "            # Initialize total score components for this competitor\n",
    "            total_weighted_score = 0\n",
    "            details_breakdown = {}\n",
    "            \n",
    "            # --- Loop 2: Process Scores for Each of the 3 Questions ---\n",
    "            # 'scores' is the nested list of 3 score objects\n",
    "            for score_entry in result[\"scores\"]:\n",
    "                q_id = score_entry[\"question_id\"]\n",
    "                judge_score = score_entry[\"judge_score\"]\n",
    "                peer_score = score_entry[\"peer_average_score\"]\n",
    "                \n",
    "                # 1. Calculate the Adjusted Score for this specific Answer (60/40 Split)\n",
    "                adjusted_answer_score = (judge_score * JUDGE_SCORE_WEIGHT) + (peer_score * PEER_SCORE_WEIGHT)\n",
    "                \n",
    "                # 2. Apply the Question's Weight (50%, 30%, 20%)\n",
    "                question_weight = QUESTION_WEIGHTS.get(q_id, 0)\n",
    "                contribution_to_final = adjusted_answer_score * question_weight\n",
    "                \n",
    "                # 3. Accumulate the scores\n",
    "                total_weighted_score += contribution_to_final\n",
    "                \n",
    "                # Store breakdown for detailed output\n",
    "                details_breakdown[q_id] = {\n",
    "                    \"adjusted_score\": f\"{adjusted_answer_score:.2f}\",\n",
    "                    \"contribution\": f\"{contribution_to_final:.2f}\"\n",
    "                }\n",
    "\n",
    "            # Add the final score and details to the ranking list\n",
    "            final_rankings.append({\n",
    "                \"model\": model_display_name,\n",
    "                \"final_score\": total_weighted_score,\n",
    "                \"details\": details_breakdown\n",
    "            })\n",
    "\n",
    "        # --- Sort and Print Results ---\n",
    "        final_rankings_sorted = sorted(final_rankings, key=lambda x: x[\"final_score\"], reverse=True)\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        print(\"--- üèÜ FINAL TRIATHLON LEADERBOARD üèÜ ---\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        for i, rank in enumerate(final_rankings_sorted, 1):\n",
    "            details = rank['details']\n",
    "            print(f\"#{i}: {rank['model']}\")\n",
    "            print(f\"   (Final Weighted Score: {rank['final_score']:.4f})\")\n",
    "            print(f\"   - Q1 (50pt): Answer Score {details['q1_50pt']['adjusted_score']} -> Contribution: {details['q1_50pt']['contribution']}\")\n",
    "            print(f\"   - Q2 (30pt): Answer Score {details['q2_30pt']['adjusted_score']} -> Contribution: {details['q2_30pt']['contribution']}\")\n",
    "            print(f\"   - Q3 (20pt): Answer Score {details['q3_20pt']['adjusted_score']} -> Contribution: {details['q3_20pt']['contribution']}\\n\")\n",
    "\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"\\n--- ERROR ---\")\n",
    "        print(\"‚ùå JSON DECODING FAILED. The Judge LLM did not return valid JSON.\")\n",
    "        print(\"Received data (truncated):\", judge_data_str[:200] if judge_data_str else \"N/A\")\n",
    "    except KeyError as e:\n",
    "        print(\"\\n--- ERROR ---\")\n",
    "        print(f\"‚ùå KEY ERROR: JSON data structure is incorrect (Missing key: {e}).\")\n",
    "        print(\"Ensure the Judge Prompt was followed exactly.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Stage 8: Visual Leaderboard (Matplotlib)\n",
    "\n",
    "This final block uses the `matplotlib` library to render the results from **Stage 7** as a professional, easy-to-read horizontal bar chart.\n",
    "\n",
    "This provides an immediate visual summary of the competition, making the final rankings clear at a glance.\n",
    "\n",
    "### üé® Chart Features\n",
    "\n",
    "| Feature | Implementation | Purpose |\n",
    "| :--- | :--- | :--- |\n",
    "| **Horizontal Bars** | `plt.barh(...)` | Provides a clean layout, especially for long model names. |\n",
    "| **Winner Highlight** | `colors.append('gold')` | The top-scoring model (the winner) is automatically colored **gold** to distinguish it from the rest. |\n",
    "| **Dynamic Height** | `fig_height = max(5, ...)` | The chart's height adjusts based on the number of competitors, preventing labels from overlapping. |\n",
    "| **Data Labels** | `plt.text(...)` | The precise `Final Weighted Score` (formatted to 3 decimals) is printed next to each bar for clarity. |\n",
    "| **Clean Aesthetics** | `spines['top'].set_visible(False)` | Removes the top and right borders (\"spines\") for a modern, less cluttered look. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****In Case Of Installing MatplotLib Libary*****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# This command ensures the plot displays inside the notebook\n",
    "%matplotlib inline \n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"--- üìä VISUAL LEADERBOARD üìä ---\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Check if the final ranking data exists\n",
    "if 'final_rankings_sorted' in globals() and final_rankings_sorted:\n",
    "    \n",
    "    # Reverse the data so the highest score is at the top of the chart\n",
    "    final_rankings_display = final_rankings_sorted[::-1]\n",
    "    \n",
    "    # Separate model names and scores\n",
    "    models = [r['model'] for r in final_rankings_display]\n",
    "    scores = [r['final_score'] for r in final_rankings_display]\n",
    "    \n",
    "    # Create a color list (default 'skyblue', winner 'gold')\n",
    "    colors = ['skyblue'] * (len(models) - 1)\n",
    "    colors.append('gold') # The last item (highest score)\n",
    "    \n",
    "    # Dynamically adjust the figure height based on the number of models\n",
    "    fig_height = max(5, len(models) * 0.7)\n",
    "    plt.figure(figsize=(10, fig_height))\n",
    "    \n",
    "    # Create the horizontal bars\n",
    "    bars = plt.barh(models, scores, color=colors, edgecolor='black')\n",
    "    \n",
    "    # Axis labels and title\n",
    "    plt.xlabel('Final Weighted Score', fontsize=12)\n",
    "    plt.title('üèÜ LLM Competition Leaderboard üèÜ', fontsize=16, pad=20)\n",
    "    \n",
    "    # Clean up the chart (remove top and right spines)\n",
    "    plt.gca().spines['top'].set_visible(False)\n",
    "    plt.gca().spines['right'].set_visible(False)\n",
    "    \n",
    "    # Add score labels on the end of each bar\n",
    "    for bar in bars:\n",
    "        width = bar.get_width()\n",
    "        plt.text(width + 0.01,  # Position label to the right of the bar\n",
    "                 bar.get_y() + bar.get_height() / 2,\n",
    "                 f'{width:.3f}', # Format score to 3 decimal places\n",
    "                 va='center', \n",
    "                 ha='left',\n",
    "                 fontsize=10)\n",
    "    \n",
    "    # Adjust left margin for long model names\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå Leaderboard data not found. Please run the scoring cell (Stage 7) first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/exercise.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#ff7800;\">Exercise</h2>\n",
    "            <span style=\"color:#ff7800;\">Which pattern(s) did this use? Try updating this to add another Agentic design pattern.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/business.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#00bfff;\">Commercial implications</h2>\n",
    "            <span style=\"color:#00bfff;\">These kinds of patterns - to send a task to multiple models, and evaluate results,\n",
    "            are common where you need to improve the quality of your LLM response. This approach can be universally applied\n",
    "            to business projects where accuracy is critical.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
