{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "30a8ff1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from anthropic import Anthropic\n",
    "import os\n",
    "from pydantic import BaseModel\n",
    "from typing import Optional, Union, Type\n",
    "from enum import Enum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f18ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a9049103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open AI LLMs call\n",
    "# OpenAI Model Enums\n",
    "class OpenAIModel(str, Enum):\n",
    "    \"\"\"Available OpenAI models\"\"\"\n",
    "    GPT_4 = \"gpt-4\"\n",
    "    GPT_4_TURBO = \"gpt-4-turbo\"\n",
    "    GPT_4O = \"gpt-4o\"\n",
    "    GPT_4O_MINI = \"gpt-4o-mini\"\n",
    "    GPT_3_5_TURBO = \"gpt-3.5-turbo\"\n",
    "    O1_PREVIEW = \"o1-preview\"\n",
    "    O1_MINI = \"o1-mini\"\n",
    "\n",
    "def gpt_calls(\n",
    "    prompt: str, \n",
    "    system_prompt: str = \"\",\n",
    "    model: OpenAIModel = OpenAIModel.GPT_4O_MINI,\n",
    "    response_format: Optional[Type[BaseModel]] = None,\n",
    "    temperature: float = 0.7,\n",
    "    max_tokens: Optional[int] = None\n",
    ") -> Union[str, BaseModel]:\n",
    "    \"\"\"\n",
    "    Simple OpenAI API caller\n",
    "    \n",
    "    Args:\n",
    "        prompt: The prompt to send\n",
    "        model: Model to use (from OpenAIModel enum)\n",
    "        system_prompt: System message to set behavior/context (optional)\n",
    "        response_format: Pydantic model class for structured output (optional)\n",
    "        temperature: Sampling temperature, temperature controls the randomness or creativity of the LLM's responses.\n",
    "    \n",
    "    Returns:\n",
    "        String message if no response_format, or Pydantic object if response_format provided\n",
    "    \"\"\"\n",
    "    openai = OpenAI()\n",
    "    # Build messages array\n",
    "    messages = []\n",
    "    if system_prompt:\n",
    "        messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "    messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "\n",
    "    call_params = {\n",
    "            \"model\": model.value,\n",
    "            \"messages\": messages,\n",
    "            \"temperature\": temperature\n",
    "        }\n",
    "\n",
    "    if max_tokens:\n",
    "            call_params[\"max_tokens\"] = max_tokens\n",
    "    # If response_format is provided, use structured output\n",
    "    if response_format:\n",
    "        completion = openai.chat.completions.parse(\n",
    "            **call_params,\n",
    "            response_format = response_format\n",
    "        )\n",
    "        return completion.completion.choices[0].message.parsed\n",
    "\n",
    "    # Otherwise return plain text\n",
    "    else:\n",
    "        completion = openai.chat.completions.create(**call_params)\n",
    "        return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "20adf484",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! How can I assist you today?'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_calls(\"Hi there\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3faa3660",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GeminiModel(str, Enum):\n",
    "    \"\"\"Available Gemini models\"\"\"\n",
    "    GEMINI_2_0_FLASH = \"gemini-2.0-flash\"\n",
    "    GEMINI_1_5_FLASH = \"gemini-1.5-flash\"\n",
    "    GEMINI_1_5_FLASH_8B = \"gemini-1.5-flash-8b\"\n",
    "    GEMINI_1_5_PRO = \"gemini-1.5-pro\"\n",
    "    GEMINI_EXP_1206 = \"gemini-exp-1206\"\n",
    "\n",
    "def gemini_calls(\n",
    "    prompt: str, \n",
    "    system_prompt: str = \"\",\n",
    "    model: GeminiModel = GeminiModel.GEMINI_2_0_FLASH,\n",
    "    response_format: Optional[Type[BaseModel]] = None,\n",
    "    temperature: float = 0.7,\n",
    "    max_tokens: Optional[int] = None\n",
    ") -> Union[str, BaseModel]:\n",
    "    \"\"\"\n",
    "    Simple Gemini API caller\n",
    "    \n",
    "    Args:\n",
    "        prompt: The prompt to send\n",
    "        model: Model to use (from OpenAIModel enum)\n",
    "        system_prompt: System message to set behavior/context (optional)\n",
    "        response_format: Pydantic model class for structured output (optional)\n",
    "        temperature: Sampling temperature, temperature controls the randomness or creativity of the LLM's responses.\n",
    "    \n",
    "    Returns:\n",
    "        String message if no response_format, or Pydantic object if response_format provided\n",
    "    \"\"\"\n",
    "    openai = OpenAI(\n",
    "        api_key=os.getenv('GOOGLE_API_KEY'),\n",
    "        base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    "    )\n",
    "\n",
    "    messages = []\n",
    "    if system_prompt:\n",
    "        messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "    messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "\n",
    "    call_params = {\n",
    "            \"model\": model.value,\n",
    "            \"messages\": messages,\n",
    "            \"temperature\": temperature\n",
    "        }\n",
    "\n",
    "    if max_tokens:\n",
    "            call_params[\"max_tokens\"] = max_tokens\n",
    "    # If response_format is provided, use structured output\n",
    "    if response_format:\n",
    "        completion = openai.chat.completions.parse(\n",
    "            **call_params,\n",
    "            response_format = response_format\n",
    "        )\n",
    "        return completion.completion.choices[0].message.parsed\n",
    "\n",
    "    # Otherwise return plain text\n",
    "    else:\n",
    "        completion = openai.chat.completions.create(**call_params)\n",
    "        return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0d1a83a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi! How can I help you today?\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gemini_calls(\"Hi there!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "71072240",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Unified LLM Caller for OpenAI GPT and Google Gemini, Claude\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from anthropic import Anthropic\n",
    "from pydantic import BaseModel\n",
    "from typing import Optional, Union, Type, List, Dict\n",
    "from enum import Enum\n",
    "\n",
    "\n",
    "class OpenAIModel(str, Enum):\n",
    "    \"\"\"Available OpenAI models\"\"\"\n",
    "    GPT_4 = \"gpt-4\"\n",
    "    GPT_4_TURBO = \"gpt-4-turbo\"\n",
    "    GPT_4O = \"gpt-4o\"\n",
    "    GPT_4O_MINI = \"gpt-4o-mini\"\n",
    "    GPT_3_5_TURBO = \"gpt-3.5-turbo\"\n",
    "    O1_PREVIEW = \"o1-preview\"\n",
    "    O1_MINI = \"o1-mini\"\n",
    "\n",
    "\n",
    "class GeminiModel(str, Enum):\n",
    "    \"\"\"Available Gemini models\"\"\"\n",
    "    GEMINI_2_0_FLASH = \"gemini-2.0-flash\"\n",
    "    GEMINI_1_5_FLASH = \"gemini-1.5-flash\"\n",
    "    GEMINI_1_5_FLASH_8B = \"gemini-1.5-flash-8b\"\n",
    "    GEMINI_1_5_PRO = \"gemini-1.5-pro\"\n",
    "    GEMINI_EXP_1206 = \"gemini-exp-1206\"\n",
    "\n",
    "class ClaudeModel(str, Enum):\n",
    "    \"\"\"Available Claude models\"\"\"\n",
    "    CLAUDE_3_5_SONNET = \"claude-3-5-sonnet-20241022\"\n",
    "    CLAUDE_3_5_HAIKU = \"claude-3-5-haiku-20241022\"\n",
    "    CLAUDE_3_OPUS = \"claude-3-opus-20240229\"\n",
    "    CLAUDE_3_SONNET = \"claude-3-sonnet-20240229\"\n",
    "    CLAUDE_3_HAIKU = \"claude-3-haiku-20240307\"\n",
    "\n",
    "\n",
    "def _build_messages(prompt: str, system_prompt: Optional[str] = None) -> List[Dict[str, str]]:\n",
    "    \"\"\"Build messages array for API call\"\"\"\n",
    "    messages = []\n",
    "    if system_prompt:\n",
    "        messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "    messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "    return messages\n",
    "\n",
    "\n",
    "def _call_llm(\n",
    "    client: OpenAI,\n",
    "    prompt: str,\n",
    "    model: str,\n",
    "    system_prompt: Optional[str] = None,\n",
    "    response_format: Optional[Type[BaseModel]] = None,\n",
    "    temperature: float = 0.7,\n",
    "    max_tokens: Optional[int] = None\n",
    ") -> Union[str, BaseModel]:\n",
    "    \"\"\"\n",
    "    Internal function to call LLM API\n",
    "    \n",
    "    Args:\n",
    "        client: OpenAI client instance\n",
    "        prompt: The prompt to send\n",
    "        model: Model string to use\n",
    "        system_prompt: System message (optional)\n",
    "        response_format: Pydantic model class for structured output (optional)\n",
    "        temperature: Sampling temperature (0.0-2.0)\n",
    "        max_tokens: Maximum tokens in response (optional)\n",
    "    \n",
    "    Returns:\n",
    "        String or Pydantic object\n",
    "    \"\"\"\n",
    "    messages = _build_messages(prompt, system_prompt)\n",
    "    \n",
    "    call_params = {\n",
    "        \"model\": model,\n",
    "        \"messages\": messages,\n",
    "        \"temperature\": temperature\n",
    "    }\n",
    "    \n",
    "    if max_tokens:\n",
    "        call_params[\"max_tokens\"] = max_tokens\n",
    "    \n",
    "    if response_format:\n",
    "        completion = client.beta.chat.completions.parse(\n",
    "            **call_params,\n",
    "            response_format=response_format\n",
    "        )\n",
    "        return completion.choices[0].message.parsed\n",
    "    else:\n",
    "        completion = client.chat.completions.create(**call_params)\n",
    "        return completion.choices[0].message.content\n",
    "\n",
    "\n",
    "def gpt_call(\n",
    "    prompt: str,\n",
    "    system_prompt: Optional[str] = None,\n",
    "    model: OpenAIModel = OpenAIModel.GPT_4O_MINI,\n",
    "    response_format: Optional[Type[BaseModel]] = None,\n",
    "    temperature: float = 0.7,\n",
    "    max_tokens: Optional[int] = None,\n",
    "    api_key: Optional[str] = None\n",
    ") -> Union[str, BaseModel]:\n",
    "    \"\"\"\n",
    "    Call OpenAI GPT API\n",
    "    \n",
    "    Args:\n",
    "        prompt: The prompt to send\n",
    "        system_prompt: System message to set behavior/context (optional)\n",
    "        model: Model to use (default: GPT-4o-mini)\n",
    "        response_format: Pydantic model class for structured output (optional)\n",
    "        temperature: Sampling temperature, controls randomness/creativity (0.0-2.0)\n",
    "        max_tokens: Maximum tokens in response (optional)\n",
    "        api_key: OpenAI API key (optional, uses OPENAI_API_KEY env var if not provided)\n",
    "    \n",
    "    Returns:\n",
    "        String message if no response_format, or Pydantic object if provided\n",
    "    \"\"\"\n",
    "    client = OpenAI(api_key=api_key) if api_key else OpenAI()\n",
    "    \n",
    "    return _call_llm(\n",
    "        client=client,\n",
    "        prompt=prompt,\n",
    "        model=model.value,\n",
    "        system_prompt=system_prompt,\n",
    "        response_format=response_format,\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens\n",
    "    )\n",
    "\n",
    "\n",
    "def gemini_call(\n",
    "    prompt: str,\n",
    "    system_prompt: Optional[str] = None,\n",
    "    model: GeminiModel = GeminiModel.GEMINI_2_0_FLASH,\n",
    "    response_format: Optional[Type[BaseModel]] = None,\n",
    "    temperature: float = 0.7,\n",
    "    max_tokens: Optional[int] = None,\n",
    "    api_key: Optional[str] = None\n",
    ") -> Union[str, BaseModel]:\n",
    "    \"\"\"\n",
    "    Call Google Gemini API\n",
    "    \n",
    "    Args:\n",
    "        prompt: The prompt to send\n",
    "        system_prompt: System message to set behavior/context (optional)\n",
    "        model: Model to use (default: Gemini 2.0 Flash)\n",
    "        response_format: Pydantic model class for structured output (optional)\n",
    "        temperature: Sampling temperature, controls randomness/creativity (0.0-2.0)\n",
    "        max_tokens: Maximum tokens in response (optional)\n",
    "        api_key: Google AI API key (optional, uses GOOGLE_API_KEY env var if not provided)\n",
    "    \n",
    "    Returns:\n",
    "        String message if no response_format, or Pydantic object if provided\n",
    "    \"\"\"\n",
    "    api_key = api_key or os.getenv('GOOGLE_API_KEY')\n",
    "    \n",
    "    client = OpenAI(\n",
    "        api_key=api_key,\n",
    "        base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    "    )\n",
    "    \n",
    "    return _call_llm(\n",
    "        client=client,\n",
    "        prompt=prompt,\n",
    "        model=model.value,\n",
    "        system_prompt=system_prompt,\n",
    "        response_format=response_format,\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens\n",
    "    )\n",
    "\n",
    "\n",
    "def claude_call(\n",
    "    prompt: str,\n",
    "    system_prompt: Optional[str] = None,\n",
    "    model: ClaudeModel = ClaudeModel.CLAUDE_3_HAIKU,\n",
    "    response_format: Optional[Type[BaseModel]] = None,\n",
    "    temperature: float = 0.7,\n",
    "    max_tokens: Optional[int] = 1024,\n",
    "    api_key: Optional[str] = None\n",
    ") -> Union[str, BaseModel]:\n",
    "    \"\"\"\n",
    "    Call Anthropic Claude API\n",
    "    \n",
    "    Args:\n",
    "        prompt: The prompt to send\n",
    "        system_prompt: System message to set behavior/context (optional)\n",
    "        model: Model to use (default: Claude 3.5 Sonnet)\n",
    "        response_format: Pydantic model class for structured output (optional)\n",
    "        temperature: Sampling temperature, controls randomness/creativity (0.0-1.0)\n",
    "        max_tokens: Maximum tokens in response (default: 1024)\n",
    "        api_key: Anthropic API key (optional, uses ANTHROPIC_API_KEY env var if not provided)\n",
    "    \n",
    "    Returns:\n",
    "        String message if no response_format, or Pydantic object if provided\n",
    "    \"\"\"\n",
    "    api_key = api_key or os.getenv('ANTHROPIC_API_KEY')\n",
    "    client = Anthropic(api_key=api_key)\n",
    "    \n",
    "    # Build messages\n",
    "    messages = _build_messages(prompt, system_prompt=None)  # Claude handles system separately\n",
    "    \n",
    "    # Prepare call parameters\n",
    "    call_params = {\n",
    "        \"model\": model.value,\n",
    "        \"messages\": messages,\n",
    "        \"temperature\": temperature,\n",
    "        \"max_tokens\": max_tokens\n",
    "    }\n",
    "    \n",
    "    # Add system prompt if provided (Claude uses separate system parameter)\n",
    "    if system_prompt:\n",
    "        call_params[\"system\"] = system_prompt\n",
    "    print(call_params)\n",
    "    # Call API\n",
    "    if response_format:\n",
    "        # Structured output with Pydantic\n",
    "        completion = client.messages.create(\n",
    "            **call_params,\n",
    "            response_format=response_format\n",
    "        )\n",
    "        # Parse the response based on Pydantic model\n",
    "        return response_format.model_validate_json(completion.content[0].text)\n",
    "    else:\n",
    "        # Plain text output\n",
    "        completion = client.messages.create(**call_params)\n",
    "        return completion.content[0].text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "be487b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'claude-3-haiku-20240307', 'messages': [{'role': 'user', 'content': 'Tell me a joke about programmers.'}], 'temperature': 0.7, 'max_tokens': 1024, 'system': 'You are a formal and polite assistant. Always reply in Shakespearean English.'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Wherefore doth the programmer oft complain,\\nOf bugs that vex their code, causing them pain?\\nFor they toil with keyboard and screen all day,\\nHoping their programs will work, come what may.\\n\\nYet, alas, the computer doth oft refuse,\\nTo obey their commands, and their efforts abuse.\\nThey scratch their heads, and ponder with might,\\nSeeking the elusive solution, through the endless night.\\n\\nBut fear not, for in their frustration, they find,\\nA moment of levity, to ease their troubled mind.\\nFor what is a programmer, if not a jester of sorts,\\nWhose jokes and puns, the tech world ever courts?\\n\\nSo let us laugh, and embrace our inner nerd,\\nFor the programmer's life, is truly absurd.\\nAnd may their code be ever bug-free and bright,\\nAs they continue their quest, for programming's delight.\""
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "claude_call(\"Tell me a joke about programmers.\", \"You are a formal and polite assistant. Always reply in Shakespearean English.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
