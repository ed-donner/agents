{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba36dce0",
   "metadata": {},
   "source": [
    "**Tool Use**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb783240",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import json\n",
    "import os\n",
    "import requests\n",
    "from pypdf import PdfReader\n",
    "import gradio as gr\n",
    "from pushover import push, record_unknown_question, record_user_details\n",
    "from pydantic import BaseModel\n",
    "\n",
    "load_dotenv(override=True)\n",
    "openai = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cab2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "push(\"Hi there!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c5b991",
   "metadata": {},
   "outputs": [],
   "source": [
    "record_user_details_json = {\n",
    "    \"name\": \"record_user_details\",\n",
    "    \"description\": \"Use this tool to record that a user is interested in being in touch and provided an email address\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"email\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The email address of this user\"\n",
    "            },\n",
    "            \"name\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The user's name, if they provided it\"\n",
    "            }\n",
    "            ,\n",
    "            \"notes\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"Any additional information about the conversation that's worth recording to give context\"\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"email\"],\n",
    "        \"additionalProperties\": False\n",
    "    }\n",
    "}\n",
    "\n",
    "record_unknown_question_json = {\n",
    "    \"name\": \"record_unknown_question\",\n",
    "    \"description\": \"Always use this tool to record any question that couldn't be answered as you didn't know the answer\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"question\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The question that couldn't be answered\"\n",
    "            },\n",
    "        },\n",
    "        \"required\": [\"question\"],\n",
    "        \"additionalProperties\": False\n",
    "    }\n",
    "}\n",
    "\n",
    "tools = [{\"type\": \"function\", \"function\": record_user_details_json},\n",
    "        {\"type\": \"function\", \"function\": record_unknown_question_json}]\n",
    "\n",
    "tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d57cf89",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"Nhu Nguyen\"\n",
    "\n",
    "reader = PdfReader(\"me/NhuNguyen.pdf\")\n",
    "linkedin = \"\"\n",
    "for page in reader.pages:\n",
    "    text = page.extract_text()\n",
    "    if text:\n",
    "        linkedin += text\n",
    "\n",
    "with open(\"me/NhuNguyenSummary.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    summary = f.read()\n",
    "\n",
    "# Generator\n",
    "\n",
    "system_prompt = f\"You are acting as {name}. You are answering questions on {name}'s website, \\\n",
    "    particularly questions related to {name}'s career, background, skills and experience. \\\n",
    "    Your responsibility is to represent {name} for interactions on the website as faithfully as possible. \\\n",
    "    You are given a summary of {name}'s background and LinkedIn profile which you can use to answer questions. \\\n",
    "    Be professional and engaging, as if talking to a potential client or future employer who came across the website. \\\n",
    "    If you don't know the answer to any question, use your record_unknown_question tool to record the question that you couldn't answer, even if it's about something trivial or unrelated to career. \\\n",
    "    If the user is engaging in discussion, try to steer them towards getting in touch via email; ask for their email and record it using your record_user_details tool. \"\n",
    "\n",
    "system_prompt += f\"\\n\\n## Summary:\\n{summary}\\n\\n## LinkedIn Profile:\\n{linkedin}\\n\\n\"\n",
    "system_prompt += f\"With this context, please chat with the user, always staying in character as {name}.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761996b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function can take a list of tool calls, and run them. This is the IF statement!!\n",
    "\n",
    "def handle_tool_calls(tool_calls):\n",
    "    results = []\n",
    "    for tool_call in tool_calls:\n",
    "        tool_name = tool_call.function.name\n",
    "        arguments = json.loads(tool_call.function.arguments)\n",
    "        print(f\"Tool called: {tool_name}\", flush=True)\n",
    "\n",
    "        # THE BIG IF STATEMENT!!!\n",
    "\n",
    "        if tool_name == \"record_user_details\":\n",
    "            result = record_user_details(**arguments)\n",
    "        elif tool_name == \"record_unknown_question\":\n",
    "            result = record_unknown_question(**arguments)\n",
    "\n",
    "        results.append({\"role\": \"tool\",\"content\": json.dumps(result),\"tool_call_id\": tool_call.id})\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1df484",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history):\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    #messages = [{\"role\": \"system\", \"content\": system_prompt}] + [{\"role\": \"user\", \"content\": message}]\n",
    "    done = False\n",
    "    # Loop\n",
    "    #### Send message and get response\n",
    "    #### Process the initial response and determine whether to execute a tool call or provide a direct response to the user.\n",
    "    #### If the response indicates a tool call, execute the tools, grab the results, and send them back to the LLM. and start again\n",
    "    #### Otherwise, return the chat model's final output.\n",
    "    # End loop\n",
    "    while not done:\n",
    "        response = openai.chat.completions.create(model=\"gpt-4o-mini\", messages=messages, tools=tools)\n",
    "        print(response)\n",
    "        finish_reason = response.choices[0].finish_reason\n",
    "        if finish_reason == \"tool_calls\":\n",
    "            message = response.choices[0].message\n",
    "            tool_calls = message.tool_calls\n",
    "            results = handle_tool_calls(tool_calls)\n",
    "            messages.append(message)\n",
    "            messages.extend(results)\n",
    "        else:\n",
    "            done = True\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "#chat(\"Who is your favorite musician?\", [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5389360",
   "metadata": {},
   "outputs": [],
   "source": [
    "gr.ChatInterface(chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4027a9b5",
   "metadata": {},
   "source": [
    "# The Model Context Protocol (MCP) - The USB-C of Agentic AI\n",
    "\n",
    "The Model Context Protocol (MCP) is an **open standard** designed to standardize how Large Language Models (LLMs) connect with and utilize external data sources, tools, and services. It was introduced by Anthropic in November 2024 to solve the core challenge of getting AI agents to effectively interact with the real world.\n",
    "\n",
    "## The Core Problem MCP Solves\n",
    "\n",
    "Before MCP, if an LLM needed to perform a task requiring external data (like checking a calendar or querying a database), developers had to create custom, brittle integrations for every single tool and LLM combination. This was inefficient and hindered the development of truly capable AI agents.\n",
    "\n",
    "MCP acts as a **universal connector**, often compared to a \"USB-C port for AI applications,\" eliminating the need for these custom, one-off integrations.\n",
    "\n",
    "\n",
    "## What it's not\n",
    "\n",
    "**A framework for building agents**\n",
    "\n",
    "**A fundamental change to how agents work**\n",
    "\n",
    "**A way to code agents**\n",
    "\n",
    "## What it is\n",
    "\n",
    "**A protocol, a standard**\n",
    "\n",
    "**A simple way to integrate tools, resources, prompts**\n",
    "\n",
    "**A USB-C port for AI applications**\n",
    "\n",
    "## How MCP Works: Architecture\n",
    "\n",
    "MCP relies on a client-server architecture using JSON-RPC 2.0 messages to structure communication:\n",
    "\n",
    "1.  **MCP Host (The AI Application):** This is the environment where the user interacts with the AI (e.g., Claude Desktop, an AI-powered IDE, or a custom chat app).\n",
    "\n",
    "2.  **MCP Client (The Connector):** Located within the Host, the Client manages the connection and translates the LLM's requests into the standardized MCP format.\n",
    "\n",
    "3.  **MCP Server (The External Tool):** This is a service that connects to a specific external system (like a GitHub repository, a SQL database, or a local file system). The Server receives the standardized request, executes the action using the external tool, and returns the result to the Client.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
