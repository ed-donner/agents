{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e49b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from untils import gpt_call, gemini_call\n",
    "from pydantic import BaseModel\n",
    "\n",
    "# Response format\n",
    "\n",
    "class GeneratorResponse(BaseModel):\n",
    "    thoughts: list[str]\n",
    "    response: str\n",
    "\n",
    "\n",
    "\n",
    "def generate(task: str, context: str = \"\") -> GeneratorResponse:\n",
    "    \"\"\"Generate and improve solution base on feedback\"\"\"\n",
    "    # Generator prompt\n",
    "    generator_system_prompt = \"\"\"\n",
    "    Your goal is to complete the task based on <user input>. If there are feedbacks provided in the <context> from your previous generations, \n",
    "    you should reflect on them to improve your solution.\n",
    "\n",
    "    Output your answer concisely in the following format:\n",
    "\n",
    "    <thoughts>\n",
    "    [Your understanding of the task and feedback and how you plan to improve]\n",
    "    </thoughts>\n",
    "\n",
    "    <response>\n",
    "    [Your code implementation here]\n",
    "    </response>\n",
    "    \"\"\"\n",
    "    if context:\n",
    "        generator_system_prompt +=f\"\"\"\n",
    "        <context>\n",
    "        {context}\n",
    "        </context>\n",
    "        \"\"\"\n",
    "    \n",
    "    result = gpt_call(prompt= task, system_prompt= generator_system_prompt,response_format=GeneratorResponse)\n",
    "    print(\"\\n=== GENERATION START ===\")\n",
    "    print(f\"Thoughts:\\n{result.thoughts}\\n\")\n",
    "    print(f\"Generated:\\n{result.response}\\n\")\n",
    "    print(\"=== GENERATION END ===\\n\")\n",
    "    print(result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a813ddbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"\"\"\n",
    "<user input>\n",
    "Implement a Stack with:\n",
    "1. push(x)\n",
    "2. pop()\n",
    "3. getMin()\n",
    "All operations should be O(1).\n",
    "</user input>\n",
    "\"\"\"\n",
    "\n",
    "# generator_result = generate(task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2539bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Structured response from evaluator\n",
    "\n",
    "class EvaluatorResponse(BaseModel):\n",
    "    is_accepted: bool\n",
    "    feedback: str\n",
    "\n",
    "def evaluate(task: str, response_from_generator: str) -> EvaluatorResponse:\n",
    "    \"\"\"Evaluate if a solution meets requirements.\"\"\"\n",
    "    evaluator_system_prompt=\"\"\"\n",
    "    Evaluate this following code implementation for:\n",
    "    1. code correctness\n",
    "    2. time complexity\n",
    "    3. style and best practices\n",
    "\n",
    "    You should be evaluating only and not attemping to solve the task.\n",
    "    Only output \"is_accepted = true\" if all criteria are met and you have no further suggestions for improvements.\n",
    "    Output your evaluation concisely in the following format.\n",
    "\n",
    "    <is_accepted>true or false</is_accepted>\n",
    "    <feedback>\n",
    "    What needs improvement and why.\n",
    "    </feedback>\n",
    "    \"\"\"\n",
    "    evaluator_user_prompt=f\"\"\"\n",
    "    Original task:\\n\n",
    "    {task}\\n\n",
    "    Content to evaluate:\\n\n",
    "    {response_from_generator}\n",
    "    \"\"\"\n",
    "    result = gemini_call(prompt= evaluator_user_prompt, system_prompt= evaluator_system_prompt, response_format= EvaluatorResponse)\n",
    "    print(\"=== EVALUATION START ===\")\n",
    "    print(f\"Is Accepted:\\n {result.is_accepted}\\n\")\n",
    "    print(f\"Feedback:\\n {result.feedback}\\n\")\n",
    "    print(\"=== EVALUATION END ===\\n\")\n",
    "    print(result)\n",
    "\n",
    "    return result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77869c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(task, generator_result.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eead349",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loop(task: str)->tuple[str, list[dict]]:\n",
    "    \"\"\"Keep generating and evaluating until requirements are met.\"\"\"\n",
    "    memory = []\n",
    "    chain_of_thought = []\n",
    "\n",
    "    generator_result = generate(task)\n",
    "    memory.append(generator_result.response)\n",
    "    chain_of_thought.append({\"thoughts\": generator_result.thoughts, \"result\": generator_result.response})\n",
    "\n",
    "    while True:\n",
    "        evaluator_result = evaluate(task, generator_result.response)\n",
    "        if evaluator_result.is_accepted:\n",
    "            print(\"===Passed evaluation===\")\n",
    "            return generator_result.response, chain_of_thought\n",
    "        \n",
    "        print(\"===Failed evaluation===\")\n",
    "        context = \"\\n\".join([\n",
    "                \"Previous attempts:\",\n",
    "                *[f\"- {m}\" for m in memory],\n",
    "                f\"\\nFeedback: {evaluator_result.feedback}\"\n",
    "            ])\n",
    "            \n",
    "        generator_result = generate(task, context)\n",
    "        memory.append(generator_result.response)\n",
    "        chain_of_thought.append({\"thoughts\": generator_result.thoughts, \"result\": generator_result.response})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c36254",
   "metadata": {},
   "outputs": [],
   "source": [
    "loop(task)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
