{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6968efcf",
   "metadata": {},
   "source": [
    "# LLMs (Large Language Models) l√† g√¨?\n",
    "\n",
    "## ƒê·ªãnh nghƒ©a\n",
    "\n",
    "**LLMs** l√† vi·∫øt t·∫Øt c·ªßa **Large Language Models** (M√¥ h√¨nh Ng√¥n ng·ªØ L·ªõn) - nh·ªØng m√¥ h√¨nh AI ƒë∆∞·ª£c hu·∫•n luy·ªán tr√™n l∆∞·ª£ng vƒÉn b·∫£n kh·ªïng l·ªì ƒë·ªÉ hi·ªÉu v√† t·∫°o ra ng√¥n ng·ªØ t·ª± nhi√™n gi·ªëng con ng∆∞·ªùi.\n",
    "\n",
    "## C√°ch ho·∫°t ƒë·ªông ƒë∆°n gi·∫£n\n",
    "\n",
    "H√£y t∆∞·ªüng t∆∞·ª£ng LLMs nh∆∞ m·ªôt \"c·ªó m√°y d·ª± ƒëo√°n t·ª´ c·ª±c k·ª≥ th√¥ng minh\":\n",
    "\n",
    "1. **Input**: B·∫°n ƒë∆∞a m·ªôt c√¢u/ƒëo·∫°n vƒÉn\n",
    "2. **Processing**: M√¥ h√¨nh ph√¢n t√≠ch ng·ªØ c·∫£nh, m·∫´u ng√¥n ng·ªØ\n",
    "3. **Output**: D·ª± ƒëo√°n t·ª´/c√¢u ti·∫øp theo ph√π h·ª£p nh·∫•t\n",
    "\n",
    "**V√≠ d·ª•**: \n",
    "- Input: \"H√¥m nay tr·ªùi ƒë·∫πp, t√¥i mu·ªën ƒëi...\"\n",
    "- LLM d·ª± ƒëo√°n: \"d·∫°o\" (x√°c su·∫•t cao), \"ch∆°i\", \"du l·ªãch\"...\n",
    "\n",
    "### \"Large\" - L·ªõn ·ªü ƒë√¢u?\n",
    "\n",
    "- **D·ªØ li·ªáu training**: H√†ng trƒÉm t·ª∑ t·ª´ t·ª´ s√°ch, web, b√°o ch√≠\n",
    "- **Parameters**: T·ª´ v√†i t·ª∑ ƒë·∫øn h√†ng trƒÉm t·ª∑ tham s·ªë\n",
    "- **T√≠nh to√°n**: C·∫ßn h√†ng ngh√¨n GPU, h√†ng tri·ªáu USD ƒë·ªÉ train\n",
    "\n",
    "### Parameters l√† g√¨?\n",
    "\n",
    "**Parameters** (tham s·ªë) l√† nh·ªØng **con s·ªë** m√† m√¥ h√¨nh h·ªçc ƒë∆∞·ª£c trong qu√° tr√¨nh training. H√£y t∆∞·ªüng t∆∞·ª£ng parameters nh∆∞ \"b·ªô nh·ªõ\" ho·∫∑c \"kinh nghi·ªám\" c·ªßa m√¥ h√¨nh - ch√∫ng l∆∞u tr·ªØ t·∫•t c·∫£ nh·ªØng g√¨ m√¥ h√¨nh ƒë√£ h·ªçc ƒë∆∞·ª£c v·ªÅ ng√¥n ng·ªØ.\n",
    "\n",
    "### V√≠ d·ª• ƒë∆°n gi·∫£n ƒë·ªÉ hi·ªÉu Parameters: D·ª± ƒëo√°n gi√° nh√†\n",
    "\n",
    "H√£y t∆∞·ªüng t∆∞·ª£ng b·∫°n mu·ªën x√¢y d·ª±ng m·ªôt m√¥ h√¨nh AI ƒë∆°n gi·∫£n ƒë·ªÉ d·ª± ƒëo√°n gi√° nh√† d·ª±a tr√™n:\n",
    "- **Di·ªán t√≠ch s√†n** (m¬≤)\n",
    "- **S·ªë t·∫ßng**\n",
    "\n",
    "#### B∆∞·ªõc 1: C√¥ng th·ª©c d·ª± ƒëo√°n\n",
    "\n",
    "```\n",
    "Gi√° nh√† = (Di·ªán t√≠ch √ó W1) + (S·ªë t·∫ßng √ó W2) + B\n",
    "\n",
    "Trong ƒë√≥:\n",
    "- W1, W2 = Weights (tr·ªçng s·ªë) - ƒê√ÇY L√Ä PARAMETERS\n",
    "- B = Bias (ƒë·ªô l·ªách) - ƒê√ÇY C≈®NG L√Ä PARAMETER\n",
    "```\n",
    "\n",
    "**M√¥ h√¨nh n√†y c√≥ 3 parameters: W1, W2, v√† B**\n",
    "\n",
    "#### Training (H·ªçc t·ª´ d·ªØ li·ªáu), ƒëi·ªÅu ch·ªânh parameters. Sau h√†ng ngh√¨n l·∫ßn ƒëi·ªÅu ch·ªânh, parameters t·ªëi ∆∞u:\n",
    "```\n",
    "W1 = 0.045  (m·ªói m¬≤ tƒÉng gi√° ~45 tri·ªáu)\n",
    "W2 = 1.2    (m·ªói t·∫ßng tƒÉng gi√° ~1.2 t·ª∑)\n",
    "B = 0.3     (gi√° c∆° b·∫£n ~300 tri·ªáu)\n",
    "```\n",
    "### Parameters th·ª±c ch·∫•t l√† g√¨?\n",
    "\n",
    "**Weights (Tr·ªçng s·ªë) - W1, W2**\n",
    "- L√† nh·ªØng con s·ªë quy·∫øt ƒë·ªãnh \"m·ª©c ƒë·ªô quan tr·ªçng\" c·ªßa m·ªói y·∫øu t·ªë\n",
    "- W1 = 0.045 nghƒ©a l√†: 1m¬≤ l√†m tƒÉng gi√° 45 tri·ªáu\n",
    "- W2 = 1.2 nghƒ©a l√†: 1 t·∫ßng l√†m tƒÉng gi√° 1.2 t·ª∑\n",
    "\n",
    "**Bias (ƒê·ªô l·ªách) - B**\n",
    "- L√† \"gi√° c∆° b·∫£n\" khi di·ªán t√≠ch v√† s·ªë t·∫ßng = 0\n",
    "- Gi√∫p ƒëi·ªÅu ch·ªânh c√¥ng th·ª©c cho ch√≠nh x√°c h∆°n\n",
    "- B = 0.3 nghƒ©a l√† gi√° kh·ªüi ƒëi·ªÉm ~300 tri·ªáu\n",
    "\n",
    "**S·ª± kh√°c bi·ªát:**\n",
    "- M√¥ h√¨nh gi√° nh√†: 3 parameters\n",
    "- GPT-2 Small: 124 tri·ªáu parameters\n",
    "- LLaMA 2 7B: 7 t·ª∑ parameters\n",
    "- GPT-3: 175 t·ª∑ parameters\n",
    "\n",
    "C√†ng nhi·ªÅu parameters ‚Üí C√†ng h·ªçc ƒë∆∞·ª£c nhi·ªÅu m·∫´u ph·ª©c t·∫°p!\n",
    "\n",
    "### So s√°nh k√≠ch th∆∞·ªõc m√¥ h√¨nh\n",
    "\n",
    "| M√¥ h√¨nh | S·ªë Parameters | B·ªô nh·ªõ c·∫ßn (fp16) | ƒê·∫∑c ƒëi·ªÉm |\n",
    "|---------|---------------|-------------------|----------|\n",
    "| GPT-2 Small | 124 tri·ªáu | ~250 MB | H·ªçc t·∫≠p, th·ª≠ nghi·ªám |\n",
    "| Gemma 2B | 2 t·ª∑ | ~4 GB | Ch·∫°y tr√™n laptop |\n",
    "| Gemma 7B | 7 t·ª∑ | ~14 GB | Ch·∫°y tr√™n PC gaming |\n",
    "| LLaMA 2 70B | 70 t·ª∑ | ~140 GB | C·∫ßn GPU m·∫°nh |\n",
    "| GPT-3 | 175 t·ª∑ | ~350 GB | Data center |\n",
    "| GPT-4 | ~1.7 ngh√¨n t·ª∑ (∆∞·ªõc t√≠nh) | ~3.4 TB | Si√™u m√°y t√≠nh |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e31a199",
   "metadata": {},
   "source": [
    "# Understanding Tokens and Context Windows in LLMs\n",
    "\n",
    "## What are Tokens?\n",
    "\n",
    "Tokens are the fundamental units that Large Language Models (LLMs) use to process text. Think of them as the \"building blocks\" of language from the model's perspective.\n",
    "\n",
    "### Key Characteristics\n",
    "\n",
    "**Not Always Words**: A token can be:\n",
    "- A complete word (e.g., \"hello\")\n",
    "- Part of a word (e.g., \"understand\" might be split into \"under\" + \"stand\")\n",
    "- A single character or punctuation mark\n",
    "- Whitespace or special characters\n",
    "\n",
    "**Language Dependent**: Different languages tokenize differently:\n",
    "- English: ~1 token per 4 characters (roughly 0.75 words)\n",
    "- Other languages: Can require 2-3x more tokens for the same meaning\n",
    "\n",
    "## What is a Context Window?\n",
    "\n",
    "The context window is the maximum amount of information (measured in tokens) that an LLM can \"see\" and process at one time. It's like the model's working memory.\n",
    "\n",
    "### How It Works\n",
    "\n",
    "**Input + Output Combined**: The context window includes:\n",
    "- System prompts and instructions\n",
    "- Conversation history\n",
    "- Your current message\n",
    "- The model's response\n",
    "- Any retrieved documents or data\n",
    "\n",
    "**Fixed Limit**: Each model has a specific maximum:\n",
    "- GPT-3.5: 4K-16K tokens\n",
    "- GPT-4: 8K-128K tokens\n",
    "- Claude 3 Opus: 200K tokens\n",
    "- Claude 3.5 Sonnet: 200K tokens\n",
    "- Gemini 1.5 Pro: Up to 2M tokens\n",
    "\n",
    "**Sequential Processing**: Once the limit is reached, older tokens are typically removed (truncated) or the conversation needs to be summarized."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98471cc6",
   "metadata": {},
   "source": [
    "# Let's learn how to talk to AI.\n",
    "https://learn.deeplearning.ai/courses/chatgpt-prompt-eng/lesson/zi9lz/guidelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5069005",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from anthropic import Anthropic\n",
    "from IPython.display import Markdown, display\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538eef7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the OpenAI client\n",
    "openai = OpenAI()\n",
    "\n",
    "# Create a test message\n",
    "messages=[{\"role\": \"user\", \"content\": \"How much is SJC gold now?\"}]\n",
    "\n",
    "# Test the OpenAI client\n",
    "response = openai.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=messages\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be17463b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Gemini Flash gemini-2.0-flash\n",
    "\n",
    "gemini = OpenAI(api_key=os.getenv('GOOGLE_API_KEY'), base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\")\n",
    "model_name = \"gemini-2.0-flash\"\n",
    "\n",
    "response = gemini.chat.completions.create(model=model_name, messages=messages)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe20b7c",
   "metadata": {},
   "source": [
    "# ü§ñ Building Effective AI Agents\n",
    "\n",
    "*Ngu·ªìn: Anthropic Engineering*\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Agent l√† g√¨?\n",
    "\n",
    "**Workflows:** LLM v√† tools ƒë∆∞·ª£c ƒëi·ªÅu ph·ªëi qua code paths ƒë·ªãnh s·∫µn\n",
    "\n",
    "**From Anthropic Agents:** LLM t·ª± ƒë·ªông ƒëi·ªÅu khi·ªÉn quy tr√¨nh v√† s·ª≠ d·ª•ng tools c·ªßa ch√≠nh n√≥\n",
    "\n",
    "**From OpenAI:** Agents are systems that independently accomplish tasks on your behalf.\n",
    "\n",
    "\n",
    "## üß± Building Block: Augmented LLM\n",
    "\n",
    "![Augmented LLM](https://www-cdn.anthropic.com/images/4zrzovbb/website/d3083d3f40bb2b6f477901cc9a240738d3dd1371-2401x1000.png)\n",
    "\n",
    "# Retrieval, Tools, Memory\n",
    "\n",
    "## üìö Retrieval\n",
    "**Retrieval** (Truy xu·∫•t th√¥ng tin) l√† kh·∫£ nƒÉng c·ªßa LLM t√¨m ki·∫øm v√† l·∫•y th√¥ng tin t·ª´ ngu·ªìn d·ªØ li·ªáu b√™n ngo√†i ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi ch√≠nh x√°c h∆°n.\n",
    "\n",
    "### LLM thu·∫ßn t√∫y c√≥ h·∫°n ch·∫ø:\n",
    "\n",
    "‚ùå **Ki·∫øn th·ª©c c≈©**: Ch·ªâ bi·∫øt ƒë·∫øn th·ªùi ƒëi·ªÉm training cutoff (v√≠ d·ª•: th√°ng 1/2025)\n",
    "\n",
    "‚ùå **Kh√¥ng bi·∫øt th√¥ng tin ri√™ng**: Kh√¥ng bi·∫øt d·ªØ li·ªáu n·ªôi b·ªô c√¥ng ty, t√†i li·ªáu c√° nh√¢n\n",
    "\n",
    "‚ùå **Hallucination**: C√≥ th·ªÉ b·ªãa ra th√¥ng tin khi kh√¥ng ch·∫Øc ch·∫Øn\n",
    "\n",
    "‚ùå **Kh√¥ng c·∫≠p nh·∫≠t**: Kh√¥ng bi·∫øt tin t·ª©c, s·ª± ki·ªán m·ªõi\n",
    "\n",
    "\n",
    "## üõ†Ô∏è Tools\n",
    "**Tools** (C√¥ng c·ª•) l√† c√°c functions/APIs m√† LLM c√≥ th·ªÉ g·ªçi ƒë·ªÉ th·ª±c hi·ªán c√°c h√†nh ƒë·ªông c·ª• th·ªÉ ho·∫∑c l·∫•y th√¥ng tin m√† n√≥ kh√¥ng th·ªÉ t·ª± l√†m ƒë∆∞·ª£c.\n",
    "\n",
    "**Tools = \"Tay v√† ch√¢n\" c·ªßa LLM**, gi√∫p n√≥ t∆∞∆°ng t√°c v·ªõi th·∫ø gi·ªõi b√™n ngo√†i.\n",
    "\n",
    "---\n",
    "### LLM thu·∫ßn t√∫y b·ªã gi·ªõi h·∫°n:\n",
    "\n",
    "‚ùå **Ch·ªâ c√≥ th·ªÉ n√≥i chuy·ªán**: Kh√¥ng th·ªÉ th·ª±c hi·ªán h√†nh ƒë·ªông th·ª±c t·∫ø\n",
    "\n",
    "‚ùå **Kh√¥ng truy c·∫≠p d·ªØ li·ªáu real-time**: Kh√¥ng bi·∫øt gi√° c·ªï phi·∫øu, th·ªùi ti·∫øt hi·ªán t·∫°i\n",
    "\n",
    "‚ùå **Kh√¥ng t√≠nh to√°n ph·ª©c t·∫°p**: Math ƒë∆°n gi·∫£n ok, nh∆∞ng calculations l·ªõn sai\n",
    "\n",
    "‚ùå **Kh√¥ng t∆∞∆°ng t√°c h·ªá th·ªëng**: Kh√¥ng th·ªÉ g·ª≠i email, t·∫°o file, query database\n",
    "\n",
    "## üß† Memory\n",
    "**Memory** (B·ªô nh·ªõ) l√† kh·∫£ nƒÉng c·ªßa LLM ghi nh·ªõ v√† s·ª≠ d·ª•ng th√¥ng tin t·ª´ c√°c cu·ªôc h·ªôi tho·∫°i tr∆∞·ªõc ƒë√≥ ho·∫∑c t∆∞∆°ng t√°c trong qu√° kh·ª©.\n",
    "\n",
    "### LLM thu·∫ßn t√∫y kh√¥ng nh·ªõ g√¨:\n",
    "\n",
    "‚ùå **Stateless**: M·ªói request l√† ƒë·ªôc l·∫≠p, kh√¥ng bi·∫øt request tr∆∞·ªõc\n",
    "\n",
    "‚ùå **Kh√¥ng ng·ªØ c·∫£nh**: Kh√¥ng nh·ªõ ƒë√£ n√≥i g√¨ trong cu·ªôc tr√≤ chuy·ªán\n",
    "\n",
    "‚ùå **Tr·∫£i nghi·ªám k√©m**: User ph·∫£i l·∫∑p l·∫°i th√¥ng tin nhi·ªÅu l·∫ßn\n",
    "\n",
    "‚ùå **Kh√¥ng c√° nh√¢n h√≥a**: Kh√¥ng th·ªÉ t√πy ch·ªânh theo preferences c·ªßa user\n",
    "\n",
    "## Agents m·∫°nh m·∫Ω nh∆∞ng c·∫ßn c·ª±c k·ª≥ c·∫©n th·∫≠n khi tri·ªÉn khai production!\n",
    "### 5 v·∫•n ƒë·ªÅ c·ªët l√µi khi x√¢y d·ª±ng Agents:\n",
    "\n",
    "1. **Unpredictability** - Kh√¥ng bi·∫øt k·∫øt qu·∫£ s·∫Ω nh∆∞ th·∫ø n√†o\n",
    "2. **Lack of Control** - Kh√¥ng bi·∫øt ch·∫°y bao l√¢u, bao nhi√™u v√≤ng\n",
    "3. **Safety Risks** - Nguy hi·ªÉm khi autonomous\n",
    "4. **Unpredictable Cost** - Chi ph√≠ kh√¥ng ki·ªÉm so√°t\n",
    "5. **Goal Misalignment** - L√†m sai nh·ªØng g√¨ user mu·ªën\n",
    "\n",
    "---\n",
    "\n",
    "## üîÑ 5 Workflow Patterns: https://www.anthropic.com/engineering/building-effective-agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d7502b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Of course, we start with law Tech\n",
    "question = \"\"\"Identify a specific and high-impact pain point in the Australian legal industry that could be effectively addressed using an Agentic AI solution.\n",
    "Your response should be formatted in markdown, and must include:\n",
    "* The pain point (clearly stated)\n",
    "* An explanation of why it exists and why it's significant\n",
    "* One or two examples illustrating how it impacts legal professionals or firms\"\"\"\n",
    "messages = [{\"role\": \"user\", \"content\": question}]\n",
    "response = openai.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "answer = response.choices[0].message.content\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a55c360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Propose the Agentic solution\n",
    "question = f\"\"\"You are an Agentic AI expert.\n",
    "Based on the following identified problem in the Australian legal industry:\n",
    "{answer}\n",
    "Propose at least two Agentic AI-based solutions that could effectively address this issue.\n",
    "For each proposed solution, include the following details in markdown format:\n",
    "* Solution name\n",
    "* Description (how it works)\n",
    "* Key features or capabilities\n",
    "* Expected impact (how it solves the problem and benefits the legal industry)\"\"\"\n",
    "messages = [{\"role\": \"user\", \"content\": question}]\n",
    "response = openai.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=messages\n",
    ")\n",
    "answer = response.choices[0].message.content\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
