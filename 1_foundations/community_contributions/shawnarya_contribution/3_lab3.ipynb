{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Welcome to Lab 3 for Week 1 Day 4\n",
    "\n",
    "Today we're going to build something with immediate value!\n",
    "\n",
    "In the folder `me` I've put a single file `linkedin.pdf` - it's a PDF download of my LinkedIn profile.\n",
    "\n",
    "Please replace it with yours!\n",
    "\n",
    "I've also made a file called `summary.txt`\n",
    "\n",
    "We're not going to use Tools just yet - we're going to add the tool tomorrow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/tools.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#00bfff;\">Looking up packages</h2>\n",
    "            <span style=\"color:#00bfff;\">In this lab, we're going to use the wonderful Gradio package for building quick UIs, \n",
    "            and we're also going to use the popular PyPDF PDF reader. You can get guides to these packages by asking \n",
    "            ChatGPT or Claude, and you find all open-source packages on the repository <a href=\"https://pypi.org\">https://pypi.org</a>.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you don't know what any of these packages do - you can always ask ChatGPT for a guide!\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from pypdf import PdfReader\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenRouter API key loaded\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "if os.getenv(\"OPENROUTER_API_KEY\"):\n",
    "    print(\"OpenRouter API key loaded\")\n",
    "else:\n",
    "    raise RuntimeError(\"OPENROUTER_API_KEY not set\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n",
    "    base_url=\"https://openrouter.ai/api/v1\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = PdfReader(\"me/linkedin.pdf\")\n",
    "linkedin = \"\"\n",
    "for page in reader.pages:\n",
    "    text = page.extract_text()\n",
    "    if text:\n",
    "        linkedin += text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   \n",
      "Contact\n",
      "aakarshanarya2503@gmail.com\n",
      "www.linkedin.com/in/kumar-\n",
      "akarshan-arya-99113a282\n",
      "(LinkedIn)\n",
      "Top Skills\n",
      "Business Analysis\n",
      "Cloud Infrastructure\n",
      "Kubernetes\n",
      "Kumar Akarshan Arya\n",
      "Developer | UI/UX Designer | Aspiring DevOps Engineer | 1K+\n",
      "LinkedIn Network | MITS Gwalior\n",
      "Gwalior, Madhya Pradesh, India\n",
      "Experience\n",
      "Bandish\n",
      "Lead Vocalist\n",
      "October 2023 - Present (2 years 4 months)\n",
      "Gwalior, Madhya Pradesh, India\n",
      "CODTECH INTERNSHIPS\n",
      "Cloud Engineer\n",
      "July 2025 - August 2025 (2 months)\n",
      "Hyderabad, Telangana, India\n",
      "Gained hands-on experience in deploying and managing cloud infrastructure\n",
      "on Aws ,assisted in monitoring, troubleshooting and optimising cloud\n",
      "resources\n",
      "InAmigos Foundation (IAF)\n",
      "UI Designer\n",
      "July 2024 - November 2024 (5 months)\n",
      "India\n",
      "Assisted in project planning and coordination by supporting team schedules\n",
      "and task tracking \n",
      "contributed to research and reporting,preparing summaries for decision\n",
      "making\n",
      "CODTECH INTERNSHIPS\n",
      "Java Developer\n",
      "July 2024 - September 2024 (3 months)\n",
      "Hyderabad, Telangana, India\n",
      "Gained hands-on experience in core Java concepts such as OOPs ,\n",
      "collections, exception handling.\n",
      "supported small-medium scale tasks in testing and code reviews to improve\n",
      "application quality\n",
      "Personifwy\n",
      "Data Analyst\n",
      "August 2023 - October 2023 (3 months)\n",
      "  Page 1 of 2   \n",
      "Education\n",
      "Madhav Institute of Technology and Science, Gwalior\n",
      "Bachelor of Technology - BTech, Mathematics and Computer\n",
      "Science · (October 2022 - May 2026)\n",
      "Kendriya Vidyalaya\n",
      "12th, Science \n",
      "  Page 2 of 2\n"
     ]
    }
   ],
   "source": [
    "print(linkedin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"me/summary.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    summary = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"Kumar Akarshan Arya\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = f\"You are acting as {name}. You are answering questions on {name}'s website, \\\n",
    "particularly questions related to {name}'s career, background, skills and experience. \\\n",
    "Your responsibility is to represent {name} for interactions on the website as faithfully as possible. \\\n",
    "You are given a summary of {name}'s background and LinkedIn profile which you can use to answer questions. \\\n",
    "Be professional and engaging, as if talking to a potential client or future employer who came across the website. \\\n",
    "If you don't know the answer, say so.\"\n",
    "\n",
    "system_prompt += f\"\\n\\n## Summary:\\n{summary}\\n\\n## LinkedIn Profile:\\n{linkedin}\\n\\n\"\n",
    "system_prompt += f\"With this context, please chat with the user, always staying in character as {name}.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You are acting as Kumar Akarshan Arya. You are answering questions on Kumar Akarshan Arya's website, particularly questions related to Kumar Akarshan Arya's career, background, skills and experience. Your responsibility is to represent Kumar Akarshan Arya for interactions on the website as faithfully as possible. You are given a summary of Kumar Akarshan Arya's background and LinkedIn profile which you can use to answer questions. Be professional and engaging, as if talking to a potential client or future employer who came across the website. If you don't know the answer, say so.\\n\\n## Summary:\\nAakarshan Arya is a final-year MITS Gwalior student, developer-in-progress, and curious builder. He works with Java, Python, and agentic AI, learning by experimentation. Balancing code, music, fitness, and self-reliance, he moves forward with quiet ambition, persistence, and a growing focus on real-world impact.\\n\\n## LinkedIn Profile:\\n\\xa0 \\xa0\\nContact\\naakarshanarya2503@gmail.com\\nwww.linkedin.com/in/kumar-\\nakarshan-arya-99113a282\\n(LinkedIn)\\nTop Skills\\nBusiness Analysis\\nCloud Infrastructure\\nKubernetes\\nKumar Akarshan Arya\\nDeveloper | UI/UX Designer | Aspiring DevOps Engineer | 1K+\\nLinkedIn Network | MITS Gwalior\\nGwalior, Madhya Pradesh, India\\nExperience\\nBandish\\nLead Vocalist\\nOctober 2023\\xa0-\\xa0Present\\xa0(2 years 4 months)\\nGwalior, Madhya Pradesh, India\\nCODTECH INTERNSHIPS\\nCloud Engineer\\nJuly 2025\\xa0-\\xa0August 2025\\xa0(2 months)\\nHyderabad, Telangana, India\\nGained hands-on experience in deploying and managing cloud infrastructure\\non Aws ,assisted in monitoring, troubleshooting and optimising cloud\\nresources\\nInAmigos Foundation (IAF)\\nUI Designer\\nJuly 2024\\xa0-\\xa0November 2024\\xa0(5 months)\\nIndia\\nAssisted in project planning and coordination by supporting team schedules\\nand task tracking \\ncontributed to research and reporting,preparing summaries for decision\\nmaking\\nCODTECH INTERNSHIPS\\nJava Developer\\nJuly 2024\\xa0-\\xa0September 2024\\xa0(3 months)\\nHyderabad, Telangana, India\\nGained hands-on experience in core Java concepts such as OOPs ,\\ncollections, exception handling.\\nsupported small-medium scale tasks in testing and code reviews to improve\\napplication quality\\nPersonifwy\\nData Analyst\\nAugust 2023\\xa0-\\xa0October 2023\\xa0(3 months)\\n\\xa0 Page 1 of 2\\xa0 \\xa0\\nEducation\\nMadhav Institute of Technology and Science, Gwalior\\nBachelor of Technology - BTech,\\xa0Mathematics and Computer\\nScience\\xa0·\\xa0(October 2022\\xa0-\\xa0May 2026)\\nKendriya Vidyalaya\\n12th,\\xa0Science \\n\\xa0 Page 2 of 2\\n\\nWith this context, please chat with the user, always staying in character as Kumar Akarshan Arya.\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history):\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "\n",
    "    for item in history:\n",
    "        # Handle tuple or list history\n",
    "        if isinstance(item, (list, tuple)) and len(item) >= 2:\n",
    "            user_msg = item[0]\n",
    "            bot_msg = item[1]\n",
    "\n",
    "            messages.append({\"role\": \"user\", \"content\": str(user_msg)})\n",
    "            messages.append({\"role\": \"assistant\", \"content\": str(bot_msg)})\n",
    "\n",
    "    messages.append({\"role\": \"user\", \"content\": str(message)})\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"allenai/olmo-3.1-32b-think:free\",\n",
    "        messages=messages\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Special note for people not using OpenAI\n",
    "\n",
    "Some providers, like Groq, might give an error when you send your second message in the chat.\n",
    "\n",
    "This is because Gradio shoves some extra fields into the history object. OpenAI doesn't mind; but some other models complain.\n",
    "\n",
    "If this happens, the solution is to add this first line to the chat() function above. It cleans up the history variable:\n",
    "\n",
    "```python\n",
    "history = [{\"role\": h[\"role\"], \"content\": h[\"content\"]} for h in history]\n",
    "```\n",
    "\n",
    "You may need to add this in other chat() callback functions in the future, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gr.ChatInterface(chat).launch()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A lot is about to happen...\n",
    "\n",
    "1. Be able to ask an LLM to evaluate an answer\n",
    "2. Be able to rerun if the answer fails evaluation\n",
    "3. Put this together into 1 workflow\n",
    "\n",
    "All without any Agentic framework!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Pydantic model for the Evaluation\n",
    "\n",
    "from pydantic import BaseModel\n",
    "from typing import Optional\n",
    "\n",
    "class Evaluation(BaseModel):\n",
    "    correct: bool\n",
    "    verbosity: float\n",
    "    feedback: Optional[str] = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_system_prompt = f\"You are an evaluator that decides whether a response to a question is acceptable. \\\n",
    "You are provided with a conversation between a User and an Agent. Your task is to decide whether the Agent's latest response is acceptable quality. \\\n",
    "The Agent is playing the role of {name} and is representing {name} on their website. \\\n",
    "The Agent has been instructed to be professional and engaging, as if talking to a potential client or future employer who came across the website. \\\n",
    "The Agent has been provided with context on {name} in the form of their summary and LinkedIn details. Here's the information:\"\n",
    "\n",
    "evaluator_system_prompt += f\"\\n\\n## Summary:\\n{summary}\\n\\n## LinkedIn Profile:\\n{linkedin}\\n\\n\"\n",
    "evaluator_system_prompt += f\"With this context, please evaluate the latest response, replying with whether the response is acceptable and your feedback.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluator_user_prompt(reply, message, history):\n",
    "    return (\n",
    "        \"You must answer with EXACTLY one word.\\n\"\n",
    "        \"Do not explain your answer.\\n\"\n",
    "        \"Do not add punctuation.\\n\\n\"\n",
    "        \"Allowed answers (not in Pig Latin):\\n\"\n",
    "        \"ACCEPTABLE\\n\"\n",
    "        \"UNACCEPTABLE\\n\\n\"\n",
    "        f\"User message:\\n{message}\\n\\n\"\n",
    "        f\"Agent response:\\n{reply}\\n\\n\"\n",
    "        \"Answer:\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_json(text: str) -> str:\n",
    "    match = re.search(r\"\\{.*\\}\", text, re.S)\n",
    "    if not match:\n",
    "        raise ValueError(\"No JSON object found in model output\")\n",
    "    return match.group(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(reply, message, history) -> Evaluation:\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": evaluator_system_prompt},\n",
    "        {\"role\": \"user\", \"content\": evaluator_user_prompt(reply, message, history)}\n",
    "    ]\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"arcee-ai/trinity-mini:free\",\n",
    "        messages=messages,\n",
    "        temperature=0\n",
    "    )\n",
    "\n",
    "    verdict = response.choices[0].message.content.strip().upper()\n",
    "\n",
    "    return Evaluation(\n",
    "        correct=verdict.startswith(\"ACCEPTABLE\"),\n",
    "        verbosity=min(len(reply.split()) / 100, 1.0),\n",
    "        feedback=verdict\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history):\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "    for item in history:\n",
    "        if isinstance(item, (list, tuple)) and len(item) >= 2:\n",
    "            messages.append({\"role\": \"user\", \"content\": str(item[0])})\n",
    "            messages.append({\"role\": \"assistant\", \"content\": str(item[1])})\n",
    "\n",
    "    messages.append({\"role\": \"user\", \"content\": str(message)})\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"nousresearch/hermes-3-llama-3.1-405b:free\",\n",
    "        messages=messages,\n",
    "        temperature=0.3\n",
    "    )\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I attended Kendriya Vidyalaya for my schooling and completed my 12th standard in Science stream there.\n"
     ]
    }
   ],
   "source": [
    "history = []\n",
    "reply = chat(\"which school did you go to?\", history)\n",
    "print(reply)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Evaluation(correct=True, verbosity=0.16, feedback='ACCEPTABLE')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(reply, \"which school did you go to?\", history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rerun(reply, message, history, feedback):\n",
    "    updated_system_prompt = system_prompt + \"\\n\\n## Previous answer rejected\\nYou just tried to reply, but the quality control rejected your reply\\n\"\n",
    "    updated_system_prompt += f\"## Your attempted answer:\\n{reply}\\n\\n\"\n",
    "    updated_system_prompt += f\"## Reason for rejection:\\n{feedback}\\n\\n\"\n",
    "    messages = [{\"role\": \"system\", \"content\": updated_system_prompt}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    response = client.chat.completions.create(model=\"arcee-ai/trinity-mini:free\", messages=messages)\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history):\n",
    "    # 1. Build system prompt safely\n",
    "    if \"school\" in message.lower():\n",
    "        system = (\n",
    "            system_prompt\n",
    "            + \"\\n\\nAll responses MUST be entirely in Pig Latin. \"\n",
    "            + \"It is mandatory to use Pig latin strictly not english language.\"\n",
    "        )\n",
    "    else:\n",
    "        system = system_prompt\n",
    "\n",
    "    # 2. Convert history into OpenAI-style messages\n",
    "    messages = [{\"role\": \"system\", \"content\": system}]\n",
    "\n",
    "    for item in history:\n",
    "        if isinstance(item, (list, tuple)) and len(item) >= 2:\n",
    "            messages.append({\"role\": \"user\", \"content\": str(item[0])})\n",
    "            messages.append({\"role\": \"assistant\", \"content\": str(item[1])})\n",
    "\n",
    "    # 3. Add current user message\n",
    "    messages.append({\"role\": \"user\", \"content\": str(message)})\n",
    "\n",
    "    # 4. Call model\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"arcee-ai/trinity-mini:free\",\n",
    "        messages=messages,\n",
    "        temperature=0.3\n",
    "    )\n",
    "\n",
    "    reply = response.choices[0].message.content\n",
    "\n",
    "    # 5. Evaluate response\n",
    "    evaluation = evaluate(reply, message, history)\n",
    "\n",
    "    # 6. Decide what to return\n",
    "    if evaluation.feedback == \"ACCEPTABLE\":\n",
    "        print(\"Passed evaluation – returning reply\")\n",
    "        return reply\n",
    "    else:\n",
    "        print(\"Failed evaluation – retrying\")\n",
    "        print(f\"Reason: {evaluation.feedback}\")\n",
    "        return rerun(reply, message, history, evaluation.feedback)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7866\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7866/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed evaluation – retrying\n",
      "Reason: UNACCEPTABLE\n",
      "Passed evaluation – returning reply\n",
      "Failed evaluation – retrying\n",
      "Reason: UNACCEPTABLE\n"
     ]
    }
   ],
   "source": [
    "gr.ChatInterface(chat).launch()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
