{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lab 2 with DeepSeek Integration\n",
        "\n",
        "This notebook demonstrates how to use DeepSeek as an alternative to Anthropic in Lab 2. DeepSeek offers a cost-effective alternative that uses the OpenAI-compatible API format.\n",
        "\n",
        "## Author\n",
        "**Moiz M.** - *October 2025*\n",
        "\n",
        "## Key Features\n",
        "- Uses DeepSeek's chat model\n",
        "- Demonstrates OpenAI-compatible API usage\n",
        "- Shows how to integrate alternative LLM providers\n",
        "- Cost-effective alternative to other providers\n",
        "\n",
        "## Setup Requirements\n",
        "1. DeepSeek API key (get one from https://platform.deepseek.com/)\n",
        "2. Add `DEEPSEEK_API_KEY` to your `.env` file\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Start with imports\n",
        "import os\n",
        "import json\n",
        "from dotenv import load_dotenv\n",
        "from openai import OpenAI\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv(override=True)\n",
        "\n",
        "# Set up DeepSeek configuration\n",
        "DEEPSEEK_BASE_URL = \"https://api.deepseek.com/v1\"\n",
        "deepseek_api_key = os.getenv(\"DEEPSEEK_API_KEY\")\n",
        "\n",
        "# Verify API key\n",
        "if deepseek_api_key:\n",
        "    print(f\"DeepSeek API Key exists and begins with: {deepseek_api_key[:8]}\")\n",
        "else:\n",
        "    print(\"DeepSeek API Key not found in environment variables\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate a challenging question\n",
        "request = \"Please come up with a challenging, nuanced question that I can ask a number of LLMs to evaluate their intelligence. \"\n",
        "request += \"Answer only with the question, no explanation.\"\n",
        "messages = [{\"role\": \"user\", \"content\": request}]\n",
        "\n",
        "# Create DeepSeek client\n",
        "deepseek = OpenAI(base_url=DEEPSEEK_BASE_URL, api_key=deepseek_api_key)\n",
        "\n",
        "# Get the question from DeepSeek\n",
        "response = deepseek.chat.completions.create(\n",
        "    model=\"deepseek-chat\",\n",
        "    messages=messages\n",
        ")\n",
        "question = response.choices[0].message.content\n",
        "print(\"Question:\", question)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize lists to store results\n",
        "competitors = []\n",
        "answers = []\n",
        "\n",
        "# Set up the question for all competitors\n",
        "messages = [{\"role\": \"user\", \"content\": question}]\n",
        "\n",
        "# Get answers from different models\n",
        "models = {\n",
        "    \"deepseek-chat\": (deepseek, \"deepseek-chat\"),\n",
        "    \"gpt4-mini\": (OpenAI(), \"gpt-4o-mini\"),\n",
        "    \"local-llama\": (OpenAI(base_url='http://localhost:11434/v1', api_key='ollama'), \"llama3.2\")\n",
        "}\n",
        "\n",
        "for model_name, (client, model) in models.items():\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=model,\n",
        "            messages=messages\n",
        "        )\n",
        "        answer = response.choices[0].message.content\n",
        "        \n",
        "        print(f\"\\nResponse from {model_name}:\")\n",
        "        display(Markdown(answer))\n",
        "        \n",
        "        competitors.append(model_name)\n",
        "        answers.append(answer)\n",
        "    except Exception as e:\n",
        "        print(f\"Error with {model_name}: {str(e)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create judging prompt\n",
        "together = \"\"\n",
        "for index, answer in enumerate(answers):\n",
        "    together += f\"# Response from competitor {index+1}\\n\\n\"\n",
        "    together += answer + \"\\n\\n\"\n",
        "\n",
        "judge = f\"\"\"You are judging a competition between {len(competitors)} competitors.\n",
        "Each model has been given this question:\n",
        "\n",
        "{question}\n",
        "\n",
        "Your job is to evaluate each response for clarity and strength of argument, and rank them in order of best to worst.\n",
        "Respond with JSON, and only JSON, with the following format:\n",
        "{{\"results\": [\"best competitor number\", \"second best competitor number\", \"third best competitor number\", ...]}}\n",
        "\n",
        "Here are the responses from each competitor:\n",
        "\n",
        "{together}\n",
        "\n",
        "Now respond with the JSON with the ranked order of the competitors, nothing else.\"\"\"\n",
        "\n",
        "# Get judgment from DeepSeek\n",
        "judge_messages = [{\"role\": \"user\", \"content\": judge}]\n",
        "response = deepseek.chat.completions.create(\n",
        "    model=\"deepseek-chat\",\n",
        "    messages=judge_messages\n",
        ")\n",
        "results = response.choices[0].message.content\n",
        "\n",
        "# Display results\n",
        "try:\n",
        "    results_dict = json.loads(results)\n",
        "    ranks = results_dict[\"results\"]\n",
        "    print(\"\\nFinal Rankings:\")\n",
        "    for index, result in enumerate(ranks):\n",
        "        competitor = competitors[int(result)-1]\n",
        "        print(f\"Rank {index+1}: {competitor}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error parsing results: {str(e)}\")\n",
        "    print(\"Raw results:\", results)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Improvements and Differences from Original Lab 2\n",
        "\n",
        "1. **Simplified Setup**: Uses OpenAI-compatible API format for all providers\n",
        "2. **Error Handling**: Added try-catch blocks to handle potential API failures gracefully\n",
        "3. **Cost Efficiency**: Uses DeepSeek as the main model, which is generally more cost-effective\n",
        "4. **Streamlined Code**: Combined model calls into a single loop with consistent interface\n",
        "5. **Local Option**: Includes Ollama integration for those who want to run models locally\n",
        "\n",
        "## Usage Notes\n",
        "\n",
        "1. Make sure to have your DeepSeek API key in the `.env` file\n",
        "2. For the local Llama model, ensure Ollama is installed and running\n",
        "3. The code will gracefully handle missing API keys or failed API calls\n",
        "4. You can easily add more models by extending the `models` dictionary\n",
        "\n",
        "## Cost Considerations\n",
        "\n",
        "DeepSeek typically offers more competitive pricing compared to other providers:\n",
        "- Lower cost per token\n",
        "- Minimum deposit of only $2\n",
        "- Good performance for most tasks\n",
        "\n",
        "## Contributing\n",
        "\n",
        "Feel free to extend this example with:\n",
        "- Additional model providers\n",
        "- Different evaluation criteria\n",
        "- Enhanced error handling\n",
        "- UI improvements\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
